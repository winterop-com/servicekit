{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Servicekit","text":"<p>Async SQLAlchemy framework with FastAPI integration - reusable foundation for building data services.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()\n    .with_database(\"sqlite+aiosqlite:///./data.db\")\n    .build()\n)\n</code></pre> <p>Run with: <code>fastapi dev your_file.py</code></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add servicekit\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li>Repository</li> <li>Issues</li> <li>API Reference</li> <li>Chapkit - ML and data service modules built on servicekit (docs)</li> </ul>"},{"location":"#license","title":"License","text":"<p>AGPL-3.0-or-later</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for all servicekit modules, classes, and functions.</p>"},{"location":"api-reference/#core-infrastructure","title":"Core Infrastructure","text":"<p>Framework-agnostic infrastructure components.</p>"},{"location":"api-reference/#database","title":"Database","text":""},{"location":"api-reference/#servicekit.database","title":"<code>database</code>","text":"<p>Async SQLAlchemy database connection manager.</p>"},{"location":"api-reference/#servicekit.database-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.database.Database","title":"<code>Database</code>","text":"<p>Generic async SQLAlchemy database connection manager.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>class Database:\n    \"\"\"Generic async SQLAlchemy database connection manager.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        echo: bool = False,\n        alembic_dir: Path | None = None,\n        auto_migrate: bool = True,\n        pool_size: int = 5,\n        max_overflow: int = 10,\n        pool_recycle: int = 3600,\n        pool_pre_ping: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize database with connection URL and pool configuration.\"\"\"\n        self.url = url\n        self.alembic_dir = alembic_dir\n        self.auto_migrate = auto_migrate\n\n        # Build engine kwargs - skip pool params for in-memory SQLite databases\n        engine_kwargs: dict = {\"echo\": echo, \"future\": True}\n        if \":memory:\" not in url:\n            # Only add pool params for non-in-memory databases\n            engine_kwargs.update(\n                {\n                    \"pool_size\": pool_size,\n                    \"max_overflow\": max_overflow,\n                    \"pool_recycle\": pool_recycle,\n                    \"pool_pre_ping\": pool_pre_ping,\n                }\n            )\n\n        self.engine: AsyncEngine = create_async_engine(url, **engine_kwargs)\n        self._session_factory: async_sessionmaker[AsyncSession] = async_sessionmaker(\n            bind=self.engine, class_=AsyncSession, expire_on_commit=False\n        )\n\n    async def init(self) -&gt; None:\n        \"\"\"Initialize database tables using Alembic migrations or direct creation.\"\"\"\n        import asyncio\n\n        # Import Base here to avoid circular import at module level\n        from servicekit.models import Base\n\n        # For databases without migrations, use direct table creation\n        if not self.auto_migrate:\n            async with self.engine.begin() as conn:\n                await conn.run_sync(Base.metadata.create_all)\n        else:\n            # Use Alembic migrations\n            alembic_cfg = Config()\n\n            # Use custom alembic directory if provided, otherwise use bundled migrations\n            if self.alembic_dir is not None:\n                alembic_cfg.set_main_option(\"script_location\", str(self.alembic_dir))\n            else:\n                alembic_cfg.set_main_option(\"script_location\", str(Path(__file__).parent.parent.parent / \"alembic\"))\n\n            alembic_cfg.set_main_option(\"sqlalchemy.url\", self.url)\n\n            # Run upgrade in executor to avoid event loop conflicts\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(None, command.upgrade, alembic_cfg, \"head\")\n\n    @asynccontextmanager\n    async def session(self) -&gt; AsyncIterator[AsyncSession]:\n        \"\"\"Create a database session context manager.\"\"\"\n        async with self._session_factory() as s:\n            yield s\n\n    async def dispose(self) -&gt; None:\n        \"\"\"Dispose of database engine and connection pool.\"\"\"\n        await self.engine.dispose()\n</code></pre>"},{"location":"api-reference/#servicekit.database.Database-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.database.Database.__init__","title":"<code>__init__(url, *, echo=False, alembic_dir=None, auto_migrate=True, pool_size=5, max_overflow=10, pool_recycle=3600, pool_pre_ping=True)</code>","text":"<p>Initialize database with connection URL and pool configuration.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    *,\n    echo: bool = False,\n    alembic_dir: Path | None = None,\n    auto_migrate: bool = True,\n    pool_size: int = 5,\n    max_overflow: int = 10,\n    pool_recycle: int = 3600,\n    pool_pre_ping: bool = True,\n) -&gt; None:\n    \"\"\"Initialize database with connection URL and pool configuration.\"\"\"\n    self.url = url\n    self.alembic_dir = alembic_dir\n    self.auto_migrate = auto_migrate\n\n    # Build engine kwargs - skip pool params for in-memory SQLite databases\n    engine_kwargs: dict = {\"echo\": echo, \"future\": True}\n    if \":memory:\" not in url:\n        # Only add pool params for non-in-memory databases\n        engine_kwargs.update(\n            {\n                \"pool_size\": pool_size,\n                \"max_overflow\": max_overflow,\n                \"pool_recycle\": pool_recycle,\n                \"pool_pre_ping\": pool_pre_ping,\n            }\n        )\n\n    self.engine: AsyncEngine = create_async_engine(url, **engine_kwargs)\n    self._session_factory: async_sessionmaker[AsyncSession] = async_sessionmaker(\n        bind=self.engine, class_=AsyncSession, expire_on_commit=False\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.database.Database.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Initialize database tables using Alembic migrations or direct creation.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>async def init(self) -&gt; None:\n    \"\"\"Initialize database tables using Alembic migrations or direct creation.\"\"\"\n    import asyncio\n\n    # Import Base here to avoid circular import at module level\n    from servicekit.models import Base\n\n    # For databases without migrations, use direct table creation\n    if not self.auto_migrate:\n        async with self.engine.begin() as conn:\n            await conn.run_sync(Base.metadata.create_all)\n    else:\n        # Use Alembic migrations\n        alembic_cfg = Config()\n\n        # Use custom alembic directory if provided, otherwise use bundled migrations\n        if self.alembic_dir is not None:\n            alembic_cfg.set_main_option(\"script_location\", str(self.alembic_dir))\n        else:\n            alembic_cfg.set_main_option(\"script_location\", str(Path(__file__).parent.parent.parent / \"alembic\"))\n\n        alembic_cfg.set_main_option(\"sqlalchemy.url\", self.url)\n\n        # Run upgrade in executor to avoid event loop conflicts\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, command.upgrade, alembic_cfg, \"head\")\n</code></pre>"},{"location":"api-reference/#servicekit.database.Database.session","title":"<code>session()</code>  <code>async</code>","text":"<p>Create a database session context manager.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>@asynccontextmanager\nasync def session(self) -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Create a database session context manager.\"\"\"\n    async with self._session_factory() as s:\n        yield s\n</code></pre>"},{"location":"api-reference/#servicekit.database.Database.dispose","title":"<code>dispose()</code>  <code>async</code>","text":"<p>Dispose of database engine and connection pool.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>async def dispose(self) -&gt; None:\n    \"\"\"Dispose of database engine and connection pool.\"\"\"\n    await self.engine.dispose()\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabase","title":"<code>SqliteDatabase</code>","text":"<p>               Bases: <code>Database</code></p> <p>SQLite-specific database implementation with optimizations.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>class SqliteDatabase(Database):\n    \"\"\"SQLite-specific database implementation with optimizations.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        echo: bool = False,\n        alembic_dir: Path | None = None,\n        auto_migrate: bool = True,\n        pool_size: int = 5,\n        max_overflow: int = 10,\n        pool_recycle: int = 3600,\n        pool_pre_ping: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize SQLite database with connection URL and pool configuration.\"\"\"\n        self.url = url\n        self.alembic_dir = alembic_dir\n        self.auto_migrate = auto_migrate\n\n        # Build engine kwargs - pool params only for non-in-memory databases\n        engine_kwargs: dict = {\"echo\": echo, \"future\": True}\n        if not self._is_in_memory_url(url):\n            # File-based databases can use pool configuration\n            engine_kwargs.update(\n                {\n                    \"pool_size\": pool_size,\n                    \"max_overflow\": max_overflow,\n                    \"pool_recycle\": pool_recycle,\n                    \"pool_pre_ping\": pool_pre_ping,\n                }\n            )\n\n        self.engine: AsyncEngine = create_async_engine(url, **engine_kwargs)\n        _install_sqlite_connect_pragmas(self.engine)\n        self._session_factory: async_sessionmaker[AsyncSession] = async_sessionmaker(\n            bind=self.engine, class_=AsyncSession, expire_on_commit=False\n        )\n\n    @staticmethod\n    def _is_in_memory_url(url: str) -&gt; bool:\n        \"\"\"Check if URL represents an in-memory database.\"\"\"\n        return \":memory:\" in url\n\n    def is_in_memory(self) -&gt; bool:\n        \"\"\"Check if this is an in-memory database.\"\"\"\n        return self._is_in_memory_url(self.url)\n\n    async def init(self) -&gt; None:\n        \"\"\"Initialize database tables and configure SQLite using Alembic migrations.\"\"\"\n        # Import Base here to avoid circular import at module level\n        from servicekit.models import Base\n\n        # Set WAL mode first (if not in-memory)\n        if not self.is_in_memory():\n            async with self.engine.begin() as conn:\n                await conn.exec_driver_sql(\"PRAGMA journal_mode=WAL;\")\n\n        # For in-memory databases or when migrations are disabled, use direct table creation\n        if self.is_in_memory() or not self.auto_migrate:\n            async with self.engine.begin() as conn:\n                await conn.run_sync(Base.metadata.create_all)\n        else:\n            # For file-based databases, use Alembic migrations\n            await super().init()\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabase-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.database.SqliteDatabase.__init__","title":"<code>__init__(url, *, echo=False, alembic_dir=None, auto_migrate=True, pool_size=5, max_overflow=10, pool_recycle=3600, pool_pre_ping=True)</code>","text":"<p>Initialize SQLite database with connection URL and pool configuration.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    *,\n    echo: bool = False,\n    alembic_dir: Path | None = None,\n    auto_migrate: bool = True,\n    pool_size: int = 5,\n    max_overflow: int = 10,\n    pool_recycle: int = 3600,\n    pool_pre_ping: bool = True,\n) -&gt; None:\n    \"\"\"Initialize SQLite database with connection URL and pool configuration.\"\"\"\n    self.url = url\n    self.alembic_dir = alembic_dir\n    self.auto_migrate = auto_migrate\n\n    # Build engine kwargs - pool params only for non-in-memory databases\n    engine_kwargs: dict = {\"echo\": echo, \"future\": True}\n    if not self._is_in_memory_url(url):\n        # File-based databases can use pool configuration\n        engine_kwargs.update(\n            {\n                \"pool_size\": pool_size,\n                \"max_overflow\": max_overflow,\n                \"pool_recycle\": pool_recycle,\n                \"pool_pre_ping\": pool_pre_ping,\n            }\n        )\n\n    self.engine: AsyncEngine = create_async_engine(url, **engine_kwargs)\n    _install_sqlite_connect_pragmas(self.engine)\n    self._session_factory: async_sessionmaker[AsyncSession] = async_sessionmaker(\n        bind=self.engine, class_=AsyncSession, expire_on_commit=False\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabase.is_in_memory","title":"<code>is_in_memory()</code>","text":"<p>Check if this is an in-memory database.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def is_in_memory(self) -&gt; bool:\n    \"\"\"Check if this is an in-memory database.\"\"\"\n    return self._is_in_memory_url(self.url)\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabase.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Initialize database tables and configure SQLite using Alembic migrations.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>async def init(self) -&gt; None:\n    \"\"\"Initialize database tables and configure SQLite using Alembic migrations.\"\"\"\n    # Import Base here to avoid circular import at module level\n    from servicekit.models import Base\n\n    # Set WAL mode first (if not in-memory)\n    if not self.is_in_memory():\n        async with self.engine.begin() as conn:\n            await conn.exec_driver_sql(\"PRAGMA journal_mode=WAL;\")\n\n    # For in-memory databases or when migrations are disabled, use direct table creation\n    if self.is_in_memory() or not self.auto_migrate:\n        async with self.engine.begin() as conn:\n            await conn.run_sync(Base.metadata.create_all)\n    else:\n        # For file-based databases, use Alembic migrations\n        await super().init()\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder","title":"<code>SqliteDatabaseBuilder</code>","text":"<p>Builder for SQLite database configuration with fluent API.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>class SqliteDatabaseBuilder:\n    \"\"\"Builder for SQLite database configuration with fluent API.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize builder with default values.\"\"\"\n        self._url: str = \"\"\n        self._echo: bool = False\n        self._alembic_dir: Path | None = None\n        self._auto_migrate: bool = True\n        self._pool_size: int = 5\n        self._max_overflow: int = 10\n        self._pool_recycle: int = 3600\n        self._pool_pre_ping: bool = True\n\n    @classmethod\n    def in_memory(cls) -&gt; Self:\n        \"\"\"Create an in-memory SQLite database configuration.\"\"\"\n        builder = cls()\n        builder._url = \"sqlite+aiosqlite:///:memory:\"\n        return builder\n\n    @classmethod\n    def from_file(cls, path: str | Path) -&gt; Self:\n        \"\"\"Create a file-based SQLite database configuration.\"\"\"\n        builder = cls()\n        if isinstance(path, Path):\n            path = str(path)\n        builder._url = f\"sqlite+aiosqlite:///{path}\"\n        return builder\n\n    def with_echo(self, enabled: bool = True) -&gt; Self:\n        \"\"\"Enable SQL query logging.\"\"\"\n        self._echo = enabled\n        return self\n\n    def with_migrations(self, enabled: bool = True, alembic_dir: Path | None = None) -&gt; Self:\n        \"\"\"Configure migration behavior.\"\"\"\n        self._auto_migrate = enabled\n        self._alembic_dir = alembic_dir\n        return self\n\n    def with_pool(\n        self,\n        size: int = 5,\n        max_overflow: int = 10,\n        recycle: int = 3600,\n        pre_ping: bool = True,\n    ) -&gt; Self:\n        \"\"\"Configure connection pool settings.\"\"\"\n        self._pool_size = size\n        self._max_overflow = max_overflow\n        self._pool_recycle = recycle\n        self._pool_pre_ping = pre_ping\n        return self\n\n    def build(self) -&gt; SqliteDatabase:\n        \"\"\"Build and return configured SqliteDatabase instance.\"\"\"\n        if not self._url:\n            raise ValueError(\"Database URL not configured. Use .in_memory() or .from_file()\")\n\n        return SqliteDatabase(\n            url=self._url,\n            echo=self._echo,\n            alembic_dir=self._alembic_dir,\n            auto_migrate=self._auto_migrate,\n            pool_size=self._pool_size,\n            max_overflow=self._max_overflow,\n            pool_recycle=self._pool_recycle,\n            pool_pre_ping=self._pool_pre_ping,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.__init__","title":"<code>__init__()</code>","text":"<p>Initialize builder with default values.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize builder with default values.\"\"\"\n    self._url: str = \"\"\n    self._echo: bool = False\n    self._alembic_dir: Path | None = None\n    self._auto_migrate: bool = True\n    self._pool_size: int = 5\n    self._max_overflow: int = 10\n    self._pool_recycle: int = 3600\n    self._pool_pre_ping: bool = True\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.in_memory","title":"<code>in_memory()</code>  <code>classmethod</code>","text":"<p>Create an in-memory SQLite database configuration.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>@classmethod\ndef in_memory(cls) -&gt; Self:\n    \"\"\"Create an in-memory SQLite database configuration.\"\"\"\n    builder = cls()\n    builder._url = \"sqlite+aiosqlite:///:memory:\"\n    return builder\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a file-based SQLite database configuration.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; Self:\n    \"\"\"Create a file-based SQLite database configuration.\"\"\"\n    builder = cls()\n    if isinstance(path, Path):\n        path = str(path)\n    builder._url = f\"sqlite+aiosqlite:///{path}\"\n    return builder\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.with_echo","title":"<code>with_echo(enabled=True)</code>","text":"<p>Enable SQL query logging.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def with_echo(self, enabled: bool = True) -&gt; Self:\n    \"\"\"Enable SQL query logging.\"\"\"\n    self._echo = enabled\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.with_migrations","title":"<code>with_migrations(enabled=True, alembic_dir=None)</code>","text":"<p>Configure migration behavior.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def with_migrations(self, enabled: bool = True, alembic_dir: Path | None = None) -&gt; Self:\n    \"\"\"Configure migration behavior.\"\"\"\n    self._auto_migrate = enabled\n    self._alembic_dir = alembic_dir\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.with_pool","title":"<code>with_pool(size=5, max_overflow=10, recycle=3600, pre_ping=True)</code>","text":"<p>Configure connection pool settings.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def with_pool(\n    self,\n    size: int = 5,\n    max_overflow: int = 10,\n    recycle: int = 3600,\n    pre_ping: bool = True,\n) -&gt; Self:\n    \"\"\"Configure connection pool settings.\"\"\"\n    self._pool_size = size\n    self._max_overflow = max_overflow\n    self._pool_recycle = recycle\n    self._pool_pre_ping = pre_ping\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.database.SqliteDatabaseBuilder.build","title":"<code>build()</code>","text":"<p>Build and return configured SqliteDatabase instance.</p> Source code in <code>src/servicekit/database.py</code> <pre><code>def build(self) -&gt; SqliteDatabase:\n    \"\"\"Build and return configured SqliteDatabase instance.\"\"\"\n    if not self._url:\n        raise ValueError(\"Database URL not configured. Use .in_memory() or .from_file()\")\n\n    return SqliteDatabase(\n        url=self._url,\n        echo=self._echo,\n        alembic_dir=self._alembic_dir,\n        auto_migrate=self._auto_migrate,\n        pool_size=self._pool_size,\n        max_overflow=self._max_overflow,\n        pool_recycle=self._pool_recycle,\n        pool_pre_ping=self._pool_pre_ping,\n    )\n</code></pre>"},{"location":"api-reference/#models","title":"Models","text":""},{"location":"api-reference/#servicekit.models","title":"<code>models</code>","text":"<p>Base ORM classes for SQLAlchemy models.</p>"},{"location":"api-reference/#servicekit.models-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.models.Base","title":"<code>Base</code>","text":"<p>               Bases: <code>AsyncAttrs</code>, <code>DeclarativeBase</code></p> <p>Root declarative base with async support.</p> Source code in <code>src/servicekit/models.py</code> <pre><code>class Base(AsyncAttrs, DeclarativeBase):\n    \"\"\"Root declarative base with async support.\"\"\"\n</code></pre>"},{"location":"api-reference/#servicekit.models.Entity","title":"<code>Entity</code>","text":"<p>               Bases: <code>Base</code></p> <p>Optional base with common columns for your models.</p> Source code in <code>src/servicekit/models.py</code> <pre><code>class Entity(Base):\n    \"\"\"Optional base with common columns for your models.\"\"\"\n\n    __abstract__ = True\n\n    id: Mapped[ULID] = mapped_column(ULIDType, primary_key=True, default=ULID)\n    created_at: Mapped[datetime.datetime] = mapped_column(server_default=func.now())\n    updated_at: Mapped[datetime.datetime] = mapped_column(server_default=func.now(), onupdate=func.now())\n    tags: Mapped[list[str]] = mapped_column(JSON, nullable=False, default=list)\n</code></pre>"},{"location":"api-reference/#repository","title":"Repository","text":""},{"location":"api-reference/#servicekit.repository","title":"<code>repository</code>","text":"<p>Base repository classes for data access layer.</p>"},{"location":"api-reference/#servicekit.repository-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.repository.Repository","title":"<code>Repository</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract repository interface for data access operations.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>class Repository[T, IdT = ULID](ABC):\n    \"\"\"Abstract repository interface for data access operations.\"\"\"\n\n    @abstractmethod\n    async def save(self, entity: T) -&gt; T:\n        \"\"\"Save an entity to the database.\"\"\"\n        ...\n\n    @abstractmethod\n    async def save_all(self, entities: Iterable[T]) -&gt; Sequence[T]:\n        \"\"\"Save multiple entities to the database.\"\"\"\n        ...\n\n    @abstractmethod\n    async def commit(self) -&gt; None:\n        \"\"\"Commit the current database transaction.\"\"\"\n        ...\n\n    @abstractmethod\n    async def refresh_many(self, entities: Iterable[T]) -&gt; None:\n        \"\"\"Refresh multiple entities from the database.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete(self, entity: T) -&gt; None:\n        \"\"\"Delete an entity from the database.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_by_id(self, id: IdT) -&gt; None:\n        \"\"\"Delete an entity by its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_all(self) -&gt; None:\n        \"\"\"Delete all entities from the database.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n        \"\"\"Delete multiple entities by their IDs.\"\"\"\n        ...\n\n    @abstractmethod\n    async def count(self) -&gt; int:\n        \"\"\"Count the number of entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def exists_by_id(self, id: IdT) -&gt; bool:\n        \"\"\"Check if an entity exists by its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_all(self) -&gt; Sequence[T]:\n        \"\"\"Find all entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_all_paginated(self, offset: int, limit: int) -&gt; Sequence[T]:\n        \"\"\"Find entities with pagination.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; Sequence[T]:\n        \"\"\"Find entities by their IDs.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_by_id(self, id: IdT) -&gt; T | None:\n        \"\"\"Find an entity by its ID.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.repository.Repository.save","title":"<code>save(entity)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save an entity to the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def save(self, entity: T) -&gt; T:\n    \"\"\"Save an entity to the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.save_all","title":"<code>save_all(entities)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save multiple entities to the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def save_all(self, entities: Iterable[T]) -&gt; Sequence[T]:\n    \"\"\"Save multiple entities to the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.commit","title":"<code>commit()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Commit the current database transaction.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def commit(self) -&gt; None:\n    \"\"\"Commit the current database transaction.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.refresh_many","title":"<code>refresh_many(entities)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Refresh multiple entities from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def refresh_many(self, entities: Iterable[T]) -&gt; None:\n    \"\"\"Refresh multiple entities from the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.delete","title":"<code>delete(entity)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete an entity from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def delete(self, entity: T) -&gt; None:\n    \"\"\"Delete an entity from the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete an entity by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def delete_by_id(self, id: IdT) -&gt; None:\n    \"\"\"Delete an entity by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.delete_all","title":"<code>delete_all()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete all entities from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def delete_all(self) -&gt; None:\n    \"\"\"Delete all entities from the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.delete_all_by_id","title":"<code>delete_all_by_id(ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete multiple entities by their IDs.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n    \"\"\"Delete multiple entities by their IDs.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.count","title":"<code>count()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Count the number of entities.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def count(self) -&gt; int:\n    \"\"\"Count the number of entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.exists_by_id","title":"<code>exists_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check if an entity exists by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def exists_by_id(self, id: IdT) -&gt; bool:\n    \"\"\"Check if an entity exists by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.find_all","title":"<code>find_all()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find all entities.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def find_all(self) -&gt; Sequence[T]:\n    \"\"\"Find all entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.find_all_paginated","title":"<code>find_all_paginated(offset, limit)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find entities with pagination.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def find_all_paginated(self, offset: int, limit: int) -&gt; Sequence[T]:\n    \"\"\"Find entities with pagination.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.find_all_by_id","title":"<code>find_all_by_id(ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find entities by their IDs.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def find_all_by_id(self, ids: Sequence[IdT]) -&gt; Sequence[T]:\n    \"\"\"Find entities by their IDs.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.Repository.find_by_id","title":"<code>find_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find an entity by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>@abstractmethod\nasync def find_by_id(self, id: IdT) -&gt; T | None:\n    \"\"\"Find an entity by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository","title":"<code>BaseRepository</code>","text":"<p>               Bases: <code>Repository[T, IdT]</code></p> <p>Base repository implementation with common CRUD operations.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>class BaseRepository[T, IdT = ULID](Repository[T, IdT]):\n    \"\"\"Base repository implementation with common CRUD operations.\"\"\"\n\n    def __init__(self, session: AsyncSession, model: type[T]) -&gt; None:\n        \"\"\"Initialize repository with database session and model type.\"\"\"\n        self.s = session\n        self.model = model\n\n    # ---------- Create ----------\n    async def save(self, entity: T) -&gt; T:\n        \"\"\"Save an entity to the database.\"\"\"\n        self.s.add(entity)\n        return entity\n\n    async def save_all(self, entities: Iterable[T]) -&gt; Sequence[T]:\n        \"\"\"Save multiple entities to the database.\"\"\"\n        entity_list = list(entities)\n        self.s.add_all(entity_list)\n        return entity_list\n\n    async def commit(self) -&gt; None:\n        \"\"\"Commit the current database transaction.\"\"\"\n        await self.s.commit()\n\n    async def refresh_many(self, entities: Iterable[T]) -&gt; None:\n        \"\"\"Refresh multiple entities from the database.\"\"\"\n        for e in entities:\n            await self.s.refresh(e)\n\n    # ---------- Delete ----------\n    async def delete(self, entity: T) -&gt; None:\n        \"\"\"Delete an entity from the database.\"\"\"\n        await self.s.delete(entity)\n\n    async def delete_by_id(self, id: IdT) -&gt; None:\n        \"\"\"Delete an entity by its ID.\"\"\"\n        id_col = getattr(self.model, \"id\")\n        await self.s.execute(delete(self.model).where(id_col == id))\n\n    async def delete_all(self) -&gt; None:\n        \"\"\"Delete all entities from the database.\"\"\"\n        await self.s.execute(delete(self.model))\n\n    async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n        \"\"\"Delete multiple entities by their IDs.\"\"\"\n        if not ids:\n            return\n        # Access the \"id\" column generically\n        id_col = getattr(self.model, \"id\")\n        await self.s.execute(delete(self.model).where(id_col.in_(ids)))\n\n    # ---------- Read / Count ----------\n    async def count(self) -&gt; int:\n        \"\"\"Count the number of entities.\"\"\"\n        return await self.s.scalar(select(func.count()).select_from(self.model)) or 0\n\n    async def exists_by_id(self, id: IdT) -&gt; bool:\n        \"\"\"Check if an entity exists by its ID.\"\"\"\n        # Access the \"id\" column generically\n        id_col = getattr(self.model, \"id\")\n        q = select(select(id_col).where(id_col == id).exists())\n        return await self.s.scalar(q) or False\n\n    async def find_all(self) -&gt; Sequence[T]:\n        \"\"\"Find all entities.\"\"\"\n        result = await self.s.scalars(select(self.model))\n        return result.all()\n\n    async def find_all_paginated(self, offset: int, limit: int) -&gt; Sequence[T]:\n        \"\"\"Find entities with pagination.\"\"\"\n        result = await self.s.scalars(select(self.model).offset(offset).limit(limit))\n        return result.all()\n\n    async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; Sequence[T]:\n        \"\"\"Find entities by their IDs.\"\"\"\n        if not ids:\n            return []\n        id_col = getattr(self.model, \"id\")\n        result = await self.s.scalars(select(self.model).where(id_col.in_(ids)))\n        return result.all()\n\n    async def find_by_id(self, id: IdT) -&gt; T | None:\n        \"\"\"Find an entity by its ID.\"\"\"\n        return await self.s.get(self.model, id)\n\n    async def get_stats(self) -&gt; dict[str, int]:\n        \"\"\"Get collection statistics.\"\"\"\n        total = await self.count()\n        return {\"total\": total}\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.repository.BaseRepository.__init__","title":"<code>__init__(session, model)</code>","text":"<p>Initialize repository with database session and model type.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>def __init__(self, session: AsyncSession, model: type[T]) -&gt; None:\n    \"\"\"Initialize repository with database session and model type.\"\"\"\n    self.s = session\n    self.model = model\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.save","title":"<code>save(entity)</code>  <code>async</code>","text":"<p>Save an entity to the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def save(self, entity: T) -&gt; T:\n    \"\"\"Save an entity to the database.\"\"\"\n    self.s.add(entity)\n    return entity\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.save_all","title":"<code>save_all(entities)</code>  <code>async</code>","text":"<p>Save multiple entities to the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def save_all(self, entities: Iterable[T]) -&gt; Sequence[T]:\n    \"\"\"Save multiple entities to the database.\"\"\"\n    entity_list = list(entities)\n    self.s.add_all(entity_list)\n    return entity_list\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.commit","title":"<code>commit()</code>  <code>async</code>","text":"<p>Commit the current database transaction.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def commit(self) -&gt; None:\n    \"\"\"Commit the current database transaction.\"\"\"\n    await self.s.commit()\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.refresh_many","title":"<code>refresh_many(entities)</code>  <code>async</code>","text":"<p>Refresh multiple entities from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def refresh_many(self, entities: Iterable[T]) -&gt; None:\n    \"\"\"Refresh multiple entities from the database.\"\"\"\n    for e in entities:\n        await self.s.refresh(e)\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.delete","title":"<code>delete(entity)</code>  <code>async</code>","text":"<p>Delete an entity from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def delete(self, entity: T) -&gt; None:\n    \"\"\"Delete an entity from the database.\"\"\"\n    await self.s.delete(entity)\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>async</code>","text":"<p>Delete an entity by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def delete_by_id(self, id: IdT) -&gt; None:\n    \"\"\"Delete an entity by its ID.\"\"\"\n    id_col = getattr(self.model, \"id\")\n    await self.s.execute(delete(self.model).where(id_col == id))\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.delete_all","title":"<code>delete_all()</code>  <code>async</code>","text":"<p>Delete all entities from the database.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def delete_all(self) -&gt; None:\n    \"\"\"Delete all entities from the database.\"\"\"\n    await self.s.execute(delete(self.model))\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.delete_all_by_id","title":"<code>delete_all_by_id(ids)</code>  <code>async</code>","text":"<p>Delete multiple entities by their IDs.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n    \"\"\"Delete multiple entities by their IDs.\"\"\"\n    if not ids:\n        return\n    # Access the \"id\" column generically\n    id_col = getattr(self.model, \"id\")\n    await self.s.execute(delete(self.model).where(id_col.in_(ids)))\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.count","title":"<code>count()</code>  <code>async</code>","text":"<p>Count the number of entities.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def count(self) -&gt; int:\n    \"\"\"Count the number of entities.\"\"\"\n    return await self.s.scalar(select(func.count()).select_from(self.model)) or 0\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.exists_by_id","title":"<code>exists_by_id(id)</code>  <code>async</code>","text":"<p>Check if an entity exists by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def exists_by_id(self, id: IdT) -&gt; bool:\n    \"\"\"Check if an entity exists by its ID.\"\"\"\n    # Access the \"id\" column generically\n    id_col = getattr(self.model, \"id\")\n    q = select(select(id_col).where(id_col == id).exists())\n    return await self.s.scalar(q) or False\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.find_all","title":"<code>find_all()</code>  <code>async</code>","text":"<p>Find all entities.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def find_all(self) -&gt; Sequence[T]:\n    \"\"\"Find all entities.\"\"\"\n    result = await self.s.scalars(select(self.model))\n    return result.all()\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.find_all_paginated","title":"<code>find_all_paginated(offset, limit)</code>  <code>async</code>","text":"<p>Find entities with pagination.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def find_all_paginated(self, offset: int, limit: int) -&gt; Sequence[T]:\n    \"\"\"Find entities with pagination.\"\"\"\n    result = await self.s.scalars(select(self.model).offset(offset).limit(limit))\n    return result.all()\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.find_all_by_id","title":"<code>find_all_by_id(ids)</code>  <code>async</code>","text":"<p>Find entities by their IDs.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; Sequence[T]:\n    \"\"\"Find entities by their IDs.\"\"\"\n    if not ids:\n        return []\n    id_col = getattr(self.model, \"id\")\n    result = await self.s.scalars(select(self.model).where(id_col.in_(ids)))\n    return result.all()\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.find_by_id","title":"<code>find_by_id(id)</code>  <code>async</code>","text":"<p>Find an entity by its ID.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def find_by_id(self, id: IdT) -&gt; T | None:\n    \"\"\"Find an entity by its ID.\"\"\"\n    return await self.s.get(self.model, id)\n</code></pre>"},{"location":"api-reference/#servicekit.repository.BaseRepository.get_stats","title":"<code>get_stats()</code>  <code>async</code>","text":"<p>Get collection statistics.</p> Source code in <code>src/servicekit/repository.py</code> <pre><code>async def get_stats(self) -&gt; dict[str, int]:\n    \"\"\"Get collection statistics.\"\"\"\n    total = await self.count()\n    return {\"total\": total}\n</code></pre>"},{"location":"api-reference/#manager","title":"Manager","text":""},{"location":"api-reference/#servicekit.manager","title":"<code>manager</code>","text":"<p>Base classes for service layer managers with lifecycle hooks.</p>"},{"location":"api-reference/#servicekit.manager-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.manager.LifecycleHooks","title":"<code>LifecycleHooks</code>","text":"<p>Lifecycle hooks for entity operations.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>class LifecycleHooks[ModelT, InSchemaT: BaseModel]:\n    \"\"\"Lifecycle hooks for entity operations.\"\"\"\n\n    def _should_assign_field(self, field: str, value: object) -&gt; bool:\n        \"\"\"Determine if a field should be assigned during update.\"\"\"\n        return True\n\n    async def pre_save(self, entity: ModelT, data: InSchemaT) -&gt; None:\n        \"\"\"Hook called before saving a new entity.\"\"\"\n        pass\n\n    async def post_save(self, entity: ModelT) -&gt; None:\n        \"\"\"Hook called after saving a new entity.\"\"\"\n        pass\n\n    async def pre_update(self, entity: ModelT, data: InSchemaT, old_values: dict[str, object]) -&gt; None:\n        \"\"\"Hook called before updating an existing entity.\"\"\"\n        pass\n\n    async def post_update(self, entity: ModelT, changes: dict[str, tuple[object, object]]) -&gt; None:\n        \"\"\"Hook called after updating an existing entity.\"\"\"\n        pass\n\n    async def pre_delete(self, entity: ModelT) -&gt; None:\n        \"\"\"Hook called before deleting an entity.\"\"\"\n        pass\n\n    async def post_delete(self, entity: ModelT) -&gt; None:\n        \"\"\"Hook called after deleting an entity.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.manager.LifecycleHooks.pre_save","title":"<code>pre_save(entity, data)</code>  <code>async</code>","text":"<p>Hook called before saving a new entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def pre_save(self, entity: ModelT, data: InSchemaT) -&gt; None:\n    \"\"\"Hook called before saving a new entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks.post_save","title":"<code>post_save(entity)</code>  <code>async</code>","text":"<p>Hook called after saving a new entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def post_save(self, entity: ModelT) -&gt; None:\n    \"\"\"Hook called after saving a new entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks.pre_update","title":"<code>pre_update(entity, data, old_values)</code>  <code>async</code>","text":"<p>Hook called before updating an existing entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def pre_update(self, entity: ModelT, data: InSchemaT, old_values: dict[str, object]) -&gt; None:\n    \"\"\"Hook called before updating an existing entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks.post_update","title":"<code>post_update(entity, changes)</code>  <code>async</code>","text":"<p>Hook called after updating an existing entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def post_update(self, entity: ModelT, changes: dict[str, tuple[object, object]]) -&gt; None:\n    \"\"\"Hook called after updating an existing entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks.pre_delete","title":"<code>pre_delete(entity)</code>  <code>async</code>","text":"<p>Hook called before deleting an entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def pre_delete(self, entity: ModelT) -&gt; None:\n    \"\"\"Hook called before deleting an entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.LifecycleHooks.post_delete","title":"<code>post_delete(entity)</code>  <code>async</code>","text":"<p>Hook called after deleting an entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def post_delete(self, entity: ModelT) -&gt; None:\n    \"\"\"Hook called after deleting an entity.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager","title":"<code>Manager</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract manager interface for business logic operations.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>class Manager[InSchemaT: BaseModel, OutSchemaT: BaseModel, IdT](ABC):\n    \"\"\"Abstract manager interface for business logic operations.\"\"\"\n\n    @abstractmethod\n    async def save(self, data: InSchemaT) -&gt; OutSchemaT:\n        \"\"\"Save an entity.\"\"\"\n        ...\n\n    @abstractmethod\n    async def save_all(self, items: Iterable[InSchemaT]) -&gt; list[OutSchemaT]:\n        \"\"\"Save multiple entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_by_id(self, id: IdT) -&gt; None:\n        \"\"\"Delete an entity by its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_all(self) -&gt; None:\n        \"\"\"Delete all entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n        \"\"\"Delete multiple entities by their IDs.\"\"\"\n        ...\n\n    @abstractmethod\n    async def count(self) -&gt; int:\n        \"\"\"Count the number of entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def exists_by_id(self, id: IdT) -&gt; bool:\n        \"\"\"Check if an entity exists by its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_by_id(self, id: IdT) -&gt; OutSchemaT | None:\n        \"\"\"Find an entity by its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_all(self) -&gt; list[OutSchemaT]:\n        \"\"\"Find all entities.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_paginated(self, page: int, size: int) -&gt; tuple[list[OutSchemaT], int]:\n        \"\"\"Find entities with pagination.\"\"\"\n        ...\n\n    @abstractmethod\n    async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; list[OutSchemaT]:\n        \"\"\"Find entities by their IDs.\"\"\"\n        ...\n\n    @abstractmethod\n    async def get_stats(self) -&gt; CollectionStats:\n        \"\"\"Get collection statistics.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.manager.Manager.save","title":"<code>save(data)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save an entity.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def save(self, data: InSchemaT) -&gt; OutSchemaT:\n    \"\"\"Save an entity.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.save_all","title":"<code>save_all(items)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save multiple entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def save_all(self, items: Iterable[InSchemaT]) -&gt; list[OutSchemaT]:\n    \"\"\"Save multiple entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete an entity by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def delete_by_id(self, id: IdT) -&gt; None:\n    \"\"\"Delete an entity by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.delete_all","title":"<code>delete_all()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete all entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def delete_all(self) -&gt; None:\n    \"\"\"Delete all entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.delete_all_by_id","title":"<code>delete_all_by_id(ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete multiple entities by their IDs.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n    \"\"\"Delete multiple entities by their IDs.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.count","title":"<code>count()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Count the number of entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def count(self) -&gt; int:\n    \"\"\"Count the number of entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.exists_by_id","title":"<code>exists_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check if an entity exists by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def exists_by_id(self, id: IdT) -&gt; bool:\n    \"\"\"Check if an entity exists by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.find_by_id","title":"<code>find_by_id(id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find an entity by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def find_by_id(self, id: IdT) -&gt; OutSchemaT | None:\n    \"\"\"Find an entity by its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.find_all","title":"<code>find_all()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find all entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def find_all(self) -&gt; list[OutSchemaT]:\n    \"\"\"Find all entities.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.find_paginated","title":"<code>find_paginated(page, size)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find entities with pagination.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def find_paginated(self, page: int, size: int) -&gt; tuple[list[OutSchemaT], int]:\n    \"\"\"Find entities with pagination.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.find_all_by_id","title":"<code>find_all_by_id(ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Find entities by their IDs.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def find_all_by_id(self, ids: Sequence[IdT]) -&gt; list[OutSchemaT]:\n    \"\"\"Find entities by their IDs.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.Manager.get_stats","title":"<code>get_stats()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get collection statistics.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>@abstractmethod\nasync def get_stats(self) -&gt; CollectionStats:\n    \"\"\"Get collection statistics.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager","title":"<code>BaseManager</code>","text":"<p>               Bases: <code>LifecycleHooks[ModelT, InSchemaT]</code>, <code>Manager[InSchemaT, OutSchemaT, IdT]</code></p> <p>Base manager implementation with CRUD operations and lifecycle hooks.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>class BaseManager[ModelT, InSchemaT: BaseModel, OutSchemaT: BaseModel, IdT](\n    LifecycleHooks[ModelT, InSchemaT],\n    Manager[InSchemaT, OutSchemaT, IdT],\n):\n    \"\"\"Base manager implementation with CRUD operations and lifecycle hooks.\"\"\"\n\n    def __init__(\n        self,\n        repo: BaseRepository[ModelT, IdT],\n        model_cls: type[ModelT],\n        out_schema_cls: type[OutSchemaT],\n    ) -&gt; None:\n        \"\"\"Initialize manager with repository, model class, and output schema class.\"\"\"\n        self.repo = repo\n        self.model_cls = model_cls\n        self.out_schema_cls = out_schema_cls\n\n    def _to_output_schema(self, entity: ModelT) -&gt; OutSchemaT:\n        \"\"\"Convert ORM entity to output schema.\"\"\"\n        return self.out_schema_cls.model_validate(entity, from_attributes=True)\n\n    async def save(self, data: InSchemaT) -&gt; OutSchemaT:\n        \"\"\"Save an entity (create or update).\"\"\"\n        data_dict = data.model_dump(exclude_none=True)\n        entity_id = data_dict.get(\"id\")\n        existing: ModelT | None = None\n\n        if entity_id is not None:\n            existing = await self.repo.find_by_id(entity_id)\n\n        if existing is None:\n            if data_dict.get(\"id\") is None:\n                data_dict.pop(\"id\", None)\n            entity = self.model_cls(**data_dict)\n            await self.pre_save(entity, data)\n            await self.repo.save(entity)\n            await self.repo.commit()\n            await self.repo.refresh_many([entity])\n            await self.post_save(entity)\n            return self._to_output_schema(entity)\n\n        tracked_fields = set(data_dict.keys())\n        if hasattr(existing, \"level\"):  # pragma: no branch\n            tracked_fields.add(\"level\")\n        old_values = {field: getattr(existing, field) for field in tracked_fields if hasattr(existing, field)}\n\n        for key, value in data_dict.items():\n            if key == \"id\":  # pragma: no branch\n                continue\n            if not self._should_assign_field(key, value):\n                continue\n            if hasattr(existing, key):\n                setattr(existing, key, value)\n\n        await self.pre_update(existing, data, old_values)\n\n        changes: dict[str, tuple[object, object]] = {}\n        for field in tracked_fields:\n            if hasattr(existing, field):\n                new_value = getattr(existing, field)\n                old_value = old_values.get(field)\n                if old_value != new_value:\n                    changes[field] = (old_value, new_value)\n\n        await self.repo.save(existing)\n        await self.repo.commit()\n        await self.repo.refresh_many([existing])\n        await self.post_update(existing, changes)\n        return self._to_output_schema(existing)\n\n    async def save_all(self, items: Iterable[InSchemaT]) -&gt; list[OutSchemaT]:\n        entities_to_insert: list[ModelT] = []\n        updates: list[tuple[ModelT, dict[str, tuple[object, object]]]] = []\n        outputs: list[ModelT] = []\n\n        for data in items:\n            data_dict = data.model_dump(exclude_none=True)\n            entity_id = data_dict.get(\"id\")\n            existing: ModelT | None = None\n            if entity_id is not None:\n                existing = await self.repo.find_by_id(entity_id)\n\n            if existing is None:\n                if data_dict.get(\"id\") is None:\n                    data_dict.pop(\"id\", None)\n                entity = self.model_cls(**data_dict)\n                await self.pre_save(entity, data)\n                entities_to_insert.append(entity)\n                outputs.append(entity)\n                continue\n\n            tracked_fields = set(data_dict.keys())\n            if hasattr(existing, \"level\"):  # pragma: no branch\n                tracked_fields.add(\"level\")\n            old_values = {field: getattr(existing, field) for field in tracked_fields if hasattr(existing, field)}\n\n            for key, value in data_dict.items():\n                if key == \"id\":  # pragma: no branch\n                    continue\n                if not self._should_assign_field(key, value):\n                    continue\n                if hasattr(existing, key):\n                    setattr(existing, key, value)\n\n            await self.pre_update(existing, data, old_values)\n\n            changes: dict[str, tuple[object, object]] = {}\n            for field in tracked_fields:\n                if hasattr(existing, field):\n                    new_value = getattr(existing, field)\n                    old_value = old_values.get(field)\n                    if old_value != new_value:\n                        changes[field] = (old_value, new_value)\n\n            updates.append((existing, changes))\n            outputs.append(existing)\n\n        if entities_to_insert:  # pragma: no branch\n            await self.repo.save_all(entities_to_insert)\n        await self.repo.commit()\n        if outputs:  # pragma: no branch\n            await self.repo.refresh_many(outputs)\n\n        for entity in entities_to_insert:\n            await self.post_save(entity)\n        for entity, changes in updates:\n            await self.post_update(entity, changes)\n\n        return [self._to_output_schema(entity) for entity in outputs]\n\n    async def delete_by_id(self, id: IdT) -&gt; None:\n        \"\"\"Delete an entity by its ID.\"\"\"\n        entity = await self.repo.find_by_id(id)\n        if entity is None:\n            return\n        await self.pre_delete(entity)\n        await self.repo.delete(entity)\n        await self.repo.commit()\n        await self.post_delete(entity)\n\n    async def delete_all(self) -&gt; None:\n        \"\"\"Delete all entities.\"\"\"\n        entities = await self.repo.find_all()\n        for entity in entities:\n            await self.pre_delete(entity)\n        await self.repo.delete_all()\n        await self.repo.commit()\n        for entity in entities:\n            await self.post_delete(entity)\n\n    async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n        \"\"\"Delete multiple entities by their IDs.\"\"\"\n        if not ids:\n            return\n        entities = await self.repo.find_all_by_id(ids)\n        for entity in entities:\n            await self.pre_delete(entity)\n        await self.repo.delete_all_by_id(ids)\n        await self.repo.commit()\n        for entity in entities:\n            await self.post_delete(entity)\n\n    async def count(self) -&gt; int:\n        \"\"\"Count the number of entities.\"\"\"\n        return await self.repo.count()\n\n    async def exists_by_id(self, id: IdT) -&gt; bool:\n        \"\"\"Check if an entity exists by its ID.\"\"\"\n        return await self.repo.exists_by_id(id)\n\n    async def find_by_id(self, id: IdT) -&gt; OutSchemaT | None:\n        \"\"\"Find an entity by its ID.\"\"\"\n        entity = await self.repo.find_by_id(id)\n        if entity is None:\n            return None\n        return self._to_output_schema(entity)\n\n    async def find_all(self) -&gt; list[OutSchemaT]:\n        \"\"\"Find all entities.\"\"\"\n        entities = await self.repo.find_all()\n        return [self._to_output_schema(e) for e in entities]\n\n    async def find_paginated(self, page: int, size: int) -&gt; tuple[list[OutSchemaT], int]:\n        \"\"\"Find entities with pagination.\"\"\"\n        offset = (page - 1) * size\n        entities = await self.repo.find_all_paginated(offset, size)\n        total = await self.repo.count()\n        return [self._to_output_schema(e) for e in entities], total\n\n    async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; list[OutSchemaT]:\n        \"\"\"Find entities by their IDs.\"\"\"\n        entities = await self.repo.find_all_by_id(ids)\n        return [self._to_output_schema(e) for e in entities]\n\n    async def get_stats(self) -&gt; CollectionStats:\n        \"\"\"Get collection statistics.\"\"\"\n        from servicekit.schemas import CollectionStats\n\n        raw_stats = await self.repo.get_stats()\n        return CollectionStats(total=raw_stats[\"total\"])\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.manager.BaseManager.__init__","title":"<code>__init__(repo, model_cls, out_schema_cls)</code>","text":"<p>Initialize manager with repository, model class, and output schema class.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>def __init__(\n    self,\n    repo: BaseRepository[ModelT, IdT],\n    model_cls: type[ModelT],\n    out_schema_cls: type[OutSchemaT],\n) -&gt; None:\n    \"\"\"Initialize manager with repository, model class, and output schema class.\"\"\"\n    self.repo = repo\n    self.model_cls = model_cls\n    self.out_schema_cls = out_schema_cls\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.save","title":"<code>save(data)</code>  <code>async</code>","text":"<p>Save an entity (create or update).</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def save(self, data: InSchemaT) -&gt; OutSchemaT:\n    \"\"\"Save an entity (create or update).\"\"\"\n    data_dict = data.model_dump(exclude_none=True)\n    entity_id = data_dict.get(\"id\")\n    existing: ModelT | None = None\n\n    if entity_id is not None:\n        existing = await self.repo.find_by_id(entity_id)\n\n    if existing is None:\n        if data_dict.get(\"id\") is None:\n            data_dict.pop(\"id\", None)\n        entity = self.model_cls(**data_dict)\n        await self.pre_save(entity, data)\n        await self.repo.save(entity)\n        await self.repo.commit()\n        await self.repo.refresh_many([entity])\n        await self.post_save(entity)\n        return self._to_output_schema(entity)\n\n    tracked_fields = set(data_dict.keys())\n    if hasattr(existing, \"level\"):  # pragma: no branch\n        tracked_fields.add(\"level\")\n    old_values = {field: getattr(existing, field) for field in tracked_fields if hasattr(existing, field)}\n\n    for key, value in data_dict.items():\n        if key == \"id\":  # pragma: no branch\n            continue\n        if not self._should_assign_field(key, value):\n            continue\n        if hasattr(existing, key):\n            setattr(existing, key, value)\n\n    await self.pre_update(existing, data, old_values)\n\n    changes: dict[str, tuple[object, object]] = {}\n    for field in tracked_fields:\n        if hasattr(existing, field):\n            new_value = getattr(existing, field)\n            old_value = old_values.get(field)\n            if old_value != new_value:\n                changes[field] = (old_value, new_value)\n\n    await self.repo.save(existing)\n    await self.repo.commit()\n    await self.repo.refresh_many([existing])\n    await self.post_update(existing, changes)\n    return self._to_output_schema(existing)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>async</code>","text":"<p>Delete an entity by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def delete_by_id(self, id: IdT) -&gt; None:\n    \"\"\"Delete an entity by its ID.\"\"\"\n    entity = await self.repo.find_by_id(id)\n    if entity is None:\n        return\n    await self.pre_delete(entity)\n    await self.repo.delete(entity)\n    await self.repo.commit()\n    await self.post_delete(entity)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.delete_all","title":"<code>delete_all()</code>  <code>async</code>","text":"<p>Delete all entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def delete_all(self) -&gt; None:\n    \"\"\"Delete all entities.\"\"\"\n    entities = await self.repo.find_all()\n    for entity in entities:\n        await self.pre_delete(entity)\n    await self.repo.delete_all()\n    await self.repo.commit()\n    for entity in entities:\n        await self.post_delete(entity)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.delete_all_by_id","title":"<code>delete_all_by_id(ids)</code>  <code>async</code>","text":"<p>Delete multiple entities by their IDs.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def delete_all_by_id(self, ids: Sequence[IdT]) -&gt; None:\n    \"\"\"Delete multiple entities by their IDs.\"\"\"\n    if not ids:\n        return\n    entities = await self.repo.find_all_by_id(ids)\n    for entity in entities:\n        await self.pre_delete(entity)\n    await self.repo.delete_all_by_id(ids)\n    await self.repo.commit()\n    for entity in entities:\n        await self.post_delete(entity)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.count","title":"<code>count()</code>  <code>async</code>","text":"<p>Count the number of entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def count(self) -&gt; int:\n    \"\"\"Count the number of entities.\"\"\"\n    return await self.repo.count()\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.exists_by_id","title":"<code>exists_by_id(id)</code>  <code>async</code>","text":"<p>Check if an entity exists by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def exists_by_id(self, id: IdT) -&gt; bool:\n    \"\"\"Check if an entity exists by its ID.\"\"\"\n    return await self.repo.exists_by_id(id)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.find_by_id","title":"<code>find_by_id(id)</code>  <code>async</code>","text":"<p>Find an entity by its ID.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def find_by_id(self, id: IdT) -&gt; OutSchemaT | None:\n    \"\"\"Find an entity by its ID.\"\"\"\n    entity = await self.repo.find_by_id(id)\n    if entity is None:\n        return None\n    return self._to_output_schema(entity)\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.find_all","title":"<code>find_all()</code>  <code>async</code>","text":"<p>Find all entities.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def find_all(self) -&gt; list[OutSchemaT]:\n    \"\"\"Find all entities.\"\"\"\n    entities = await self.repo.find_all()\n    return [self._to_output_schema(e) for e in entities]\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.find_paginated","title":"<code>find_paginated(page, size)</code>  <code>async</code>","text":"<p>Find entities with pagination.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def find_paginated(self, page: int, size: int) -&gt; tuple[list[OutSchemaT], int]:\n    \"\"\"Find entities with pagination.\"\"\"\n    offset = (page - 1) * size\n    entities = await self.repo.find_all_paginated(offset, size)\n    total = await self.repo.count()\n    return [self._to_output_schema(e) for e in entities], total\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.find_all_by_id","title":"<code>find_all_by_id(ids)</code>  <code>async</code>","text":"<p>Find entities by their IDs.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def find_all_by_id(self, ids: Sequence[IdT]) -&gt; list[OutSchemaT]:\n    \"\"\"Find entities by their IDs.\"\"\"\n    entities = await self.repo.find_all_by_id(ids)\n    return [self._to_output_schema(e) for e in entities]\n</code></pre>"},{"location":"api-reference/#servicekit.manager.BaseManager.get_stats","title":"<code>get_stats()</code>  <code>async</code>","text":"<p>Get collection statistics.</p> Source code in <code>src/servicekit/manager.py</code> <pre><code>async def get_stats(self) -&gt; CollectionStats:\n    \"\"\"Get collection statistics.\"\"\"\n    from servicekit.schemas import CollectionStats\n\n    raw_stats = await self.repo.get_stats()\n    return CollectionStats(total=raw_stats[\"total\"])\n</code></pre>"},{"location":"api-reference/#schemas","title":"Schemas","text":""},{"location":"api-reference/#servicekit.schemas","title":"<code>schemas</code>","text":"<p>Core Pydantic schemas for entities, responses, and jobs.</p>"},{"location":"api-reference/#servicekit.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.schemas.EntityIn","title":"<code>EntityIn</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base input schema for entities with optional ID.</p> <p>Tags must contain only letters, numbers, hyphens, and underscores. Whitespace and special characters are not allowed. Tags are case-sensitive and duplicates are not allowed.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class EntityIn(BaseModel):\n    \"\"\"Base input schema for entities with optional ID.\n\n    Tags must contain only letters, numbers, hyphens, and underscores.\n    Whitespace and special characters are not allowed.\n    Tags are case-sensitive and duplicates are not allowed.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: ULID | None = None\n    tags: list[str] = Field(\n        default_factory=list,\n        description=\"Tags for categorization (alphanumeric, hyphens, underscores only; max 50 tags, 100 chars each)\",\n    )\n\n    @field_validator(\"tags\")\n    @classmethod\n    def validate_tags(cls, v: list[str]) -&gt; list[str]:\n        \"\"\"Validate tag format - alphanumeric, hyphens, underscores only.\"\"\"\n        if not v:\n            return []\n\n        errors = []\n\n        for tag in v:\n            # Check empty\n            if not tag:\n                errors.append(\"Empty tags not allowed\")\n                continue\n\n            # Check whitespace\n            if any(c.isspace() for c in tag):\n                errors.append(f\"Tag '{tag}' contains whitespace\")\n\n            # Check valid characters (letters, numbers, -, _)\n            if not all(c.isalnum() or c in \"-_\" for c in tag):\n                errors.append(\n                    f\"Tag '{tag}' contains invalid characters (use letters, numbers, hyphens, underscores only)\"\n                )\n\n            # Check length\n            if len(tag) &gt; 100:\n                errors.append(f\"Tag '{tag}' exceeds 100 character limit\")\n\n        # Check duplicates\n        if len(v) != len(set(v)):\n            duplicates = sorted([tag for tag in set(v) if v.count(tag) &gt; 1])\n            errors.append(f\"Duplicate tags: {duplicates}\")\n\n        # Check max tags\n        if len(v) &gt; 50:\n            errors.append(f\"Maximum 50 tags allowed (got {len(v)})\")\n\n        if errors:\n            raise ValueError(\"; \".join(errors))\n\n        return v  # Return as-is (no mutation)\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.EntityIn-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.schemas.EntityIn.validate_tags","title":"<code>validate_tags(v)</code>  <code>classmethod</code>","text":"<p>Validate tag format - alphanumeric, hyphens, underscores only.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>@field_validator(\"tags\")\n@classmethod\ndef validate_tags(cls, v: list[str]) -&gt; list[str]:\n    \"\"\"Validate tag format - alphanumeric, hyphens, underscores only.\"\"\"\n    if not v:\n        return []\n\n    errors = []\n\n    for tag in v:\n        # Check empty\n        if not tag:\n            errors.append(\"Empty tags not allowed\")\n            continue\n\n        # Check whitespace\n        if any(c.isspace() for c in tag):\n            errors.append(f\"Tag '{tag}' contains whitespace\")\n\n        # Check valid characters (letters, numbers, -, _)\n        if not all(c.isalnum() or c in \"-_\" for c in tag):\n            errors.append(\n                f\"Tag '{tag}' contains invalid characters (use letters, numbers, hyphens, underscores only)\"\n            )\n\n        # Check length\n        if len(tag) &gt; 100:\n            errors.append(f\"Tag '{tag}' exceeds 100 character limit\")\n\n    # Check duplicates\n    if len(v) != len(set(v)):\n        duplicates = sorted([tag for tag in set(v) if v.count(tag) &gt; 1])\n        errors.append(f\"Duplicate tags: {duplicates}\")\n\n    # Check max tags\n    if len(v) &gt; 50:\n        errors.append(f\"Maximum 50 tags allowed (got {len(v)})\")\n\n    if errors:\n        raise ValueError(\"; \".join(errors))\n\n    return v  # Return as-is (no mutation)\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.EntityOut","title":"<code>EntityOut</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base output schema for entities with ID and timestamps.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class EntityOut(BaseModel):\n    \"\"\"Base output schema for entities with ID and timestamps.\"\"\"\n\n    model_config = ConfigDict(from_attributes=True, arbitrary_types_allowed=True)\n\n    id: ULID\n    created_at: datetime\n    updated_at: datetime\n    tags: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.PaginatedResponse","title":"<code>PaginatedResponse</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Paginated response with items, total count, page number, and computed page count.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class PaginatedResponse(BaseModel, Generic[T]):\n    \"\"\"Paginated response with items, total count, page number, and computed page count.\"\"\"\n\n    items: list[T] = Field(description=\"List of items for the current page\")\n    total: int = Field(description=\"Total number of items across all pages\", ge=0)\n    page: int = Field(description=\"Current page number (1-indexed)\", ge=1)\n    size: int = Field(description=\"Number of items per page\", ge=1)\n\n    @computed_field  # type: ignore[prop-decorator]\n    @property\n    def pages(self) -&gt; int:\n        \"\"\"Total number of pages.\"\"\"\n        if self.total == 0:\n            return 0\n        return (self.total + self.size - 1) // self.size\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.PaginatedResponse-attributes","title":"Attributes","text":""},{"location":"api-reference/#servicekit.schemas.PaginatedResponse.pages","title":"<code>pages</code>  <code>property</code>","text":"<p>Total number of pages.</p>"},{"location":"api-reference/#servicekit.schemas.CollectionStats","title":"<code>CollectionStats</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Statistics about an entity collection.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class CollectionStats(BaseModel):\n    \"\"\"Statistics about an entity collection.\"\"\"\n\n    total: int = Field(description=\"Total number of entities in the collection\", ge=0)\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.BulkOperationError","title":"<code>BulkOperationError</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Error information for a single item in a bulk operation.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class BulkOperationError(BaseModel):\n    \"\"\"Error information for a single item in a bulk operation.\"\"\"\n\n    id: str = Field(description=\"Identifier of the item that failed\")\n    reason: str = Field(description=\"Human-readable error message\")\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.BulkOperationResult","title":"<code>BulkOperationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of bulk operation with counts of succeeded/failed items and error details.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class BulkOperationResult(BaseModel):\n    \"\"\"Result of bulk operation with counts of succeeded/failed items and error details.\"\"\"\n\n    total: int = Field(description=\"Total number of items processed\", ge=0)\n    succeeded: int = Field(description=\"Number of items successfully processed\", ge=0)\n    failed: int = Field(description=\"Number of items that failed\", ge=0)\n    errors: list[BulkOperationError] = Field(default_factory=list, description=\"Details of failed items (if any)\")\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.ProblemDetail","title":"<code>ProblemDetail</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>RFC 9457 Problem Details with URN error type, status, and human-readable messages.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class ProblemDetail(BaseModel):\n    \"\"\"RFC 9457 Problem Details with URN error type, status, and human-readable messages.\"\"\"\n\n    type: str = Field(\n        default=\"about:blank\",\n        description=\"URI reference identifying the problem type (URN format for servicekit errors)\",\n    )\n    title: str = Field(description=\"Short, human-readable summary of the problem type\")\n    status: int = Field(description=\"HTTP status code\", ge=100, le=599)\n    detail: str | None = Field(default=None, description=\"Human-readable explanation specific to this occurrence\")\n    instance: str | None = Field(default=None, description=\"URI reference identifying the specific occurrence\")\n    trace_id: str | None = Field(default=None, description=\"Optional trace ID for debugging\")\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"type\": \"urn:servicekit:error:not-found\",\n                    \"title\": \"Resource Not Found\",\n                    \"status\": 404,\n                    \"detail\": \"Config with id 01ABC... not found\",\n                    \"instance\": \"/api/config/01ABC...\",\n                }\n            ]\n        }\n    }\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.JobStatus","title":"<code>JobStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Status of a scheduled job.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class JobStatus(StrEnum):\n    \"\"\"Status of a scheduled job.\"\"\"\n\n    pending = \"pending\"\n    running = \"running\"\n    completed = \"completed\"\n    failed = \"failed\"\n    canceled = \"canceled\"\n</code></pre>"},{"location":"api-reference/#servicekit.schemas.JobRecord","title":"<code>JobRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete record of a scheduled job's state and metadata.</p> Source code in <code>src/servicekit/schemas.py</code> <pre><code>class JobRecord(BaseModel):\n    \"\"\"Complete record of a scheduled job's state and metadata.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: ULID = Field(description=\"Unique job identifier\")\n    status: JobStatus = Field(default=JobStatus.pending, description=\"Current job status\")\n    submitted_at: datetime | None = Field(default=None, description=\"When the job was submitted\")\n    started_at: datetime | None = Field(default=None, description=\"When the job started running\")\n    finished_at: datetime | None = Field(default=None, description=\"When the job finished\")\n    error: str | None = Field(default=None, description=\"User-friendly error message if job failed\")\n    error_traceback: str | None = Field(default=None, description=\"Full error traceback for debugging\")\n</code></pre>"},{"location":"api-reference/#exceptions","title":"Exceptions","text":""},{"location":"api-reference/#servicekit.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions with RFC 9457 Problem Details support.</p>"},{"location":"api-reference/#servicekit.exceptions-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.exceptions.ErrorType","title":"<code>ErrorType</code>","text":"<p>URN-based error type identifiers for RFC 9457 Problem Details.</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class ErrorType:\n    \"\"\"URN-based error type identifiers for RFC 9457 Problem Details.\"\"\"\n\n    NOT_FOUND = \"urn:servicekit:error:not-found\"\n    VALIDATION_FAILED = \"urn:servicekit:error:validation-failed\"\n    CONFLICT = \"urn:servicekit:error:conflict\"\n    INVALID_ULID = \"urn:servicekit:error:invalid-ulid\"\n    INTERNAL_ERROR = \"urn:servicekit:error:internal\"\n    UNAUTHORIZED = \"urn:servicekit:error:unauthorized\"\n    FORBIDDEN = \"urn:servicekit:error:forbidden\"\n    BAD_REQUEST = \"urn:servicekit:error:bad-request\"\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.ServicekitException","title":"<code>ServicekitException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for servicekit with RFC 9457 Problem Details support.</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class ServicekitException(Exception):\n    \"\"\"Base exception for servicekit with RFC 9457 Problem Details support.\"\"\"\n\n    def __init__(\n        self,\n        detail: str,\n        *,\n        type_uri: str = ErrorType.INTERNAL_ERROR,\n        title: str = \"Internal Server Error\",\n        status: int = 500,\n        instance: str | None = None,\n        **extensions: Any,\n    ) -&gt; None:\n        super().__init__(detail)\n        self.type_uri = type_uri\n        self.title = title\n        self.status = status\n        self.detail = detail\n        self.instance = instance\n        self.extensions = extensions\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Resource not found exception (404).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class NotFoundError(ServicekitException):\n    \"\"\"Resource not found exception (404).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.NOT_FOUND,\n            title=\"Resource Not Found\",\n            status=404,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Validation failed exception (400).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class ValidationError(ServicekitException):\n    \"\"\"Validation failed exception (400).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.VALIDATION_FAILED,\n            title=\"Validation Failed\",\n            status=400,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.ConflictError","title":"<code>ConflictError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Resource conflict exception (409).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class ConflictError(ServicekitException):\n    \"\"\"Resource conflict exception (409).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.CONFLICT,\n            title=\"Resource Conflict\",\n            status=409,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.InvalidULIDError","title":"<code>InvalidULIDError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Invalid ULID format exception (400).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class InvalidULIDError(ServicekitException):\n    \"\"\"Invalid ULID format exception (400).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.INVALID_ULID,\n            title=\"Invalid ULID Format\",\n            status=400,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.BadRequestError","title":"<code>BadRequestError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Bad request exception (400).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class BadRequestError(ServicekitException):\n    \"\"\"Bad request exception (400).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.BAD_REQUEST,\n            title=\"Bad Request\",\n            status=400,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.UnauthorizedError","title":"<code>UnauthorizedError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Unauthorized exception (401).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class UnauthorizedError(ServicekitException):\n    \"\"\"Unauthorized exception (401).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.UNAUTHORIZED,\n            title=\"Unauthorized\",\n            status=401,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#servicekit.exceptions.ForbiddenError","title":"<code>ForbiddenError</code>","text":"<p>               Bases: <code>ServicekitException</code></p> <p>Forbidden exception (403).</p> Source code in <code>src/servicekit/exceptions.py</code> <pre><code>class ForbiddenError(ServicekitException):\n    \"\"\"Forbidden exception (403).\"\"\"\n\n    def __init__(self, detail: str, *, instance: str | None = None, **extensions: Any) -&gt; None:\n        super().__init__(\n            detail,\n            type_uri=ErrorType.FORBIDDEN,\n            title=\"Forbidden\",\n            status=403,\n            instance=instance,\n            **extensions,\n        )\n</code></pre>"},{"location":"api-reference/#scheduler","title":"Scheduler","text":""},{"location":"api-reference/#servicekit.scheduler","title":"<code>scheduler</code>","text":"<p>Job scheduler for async task management with in-memory asyncio implementation.</p>"},{"location":"api-reference/#servicekit.scheduler-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.scheduler.JobScheduler","title":"<code>JobScheduler</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract job scheduler interface for async task management.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>class JobScheduler(BaseModel, ABC):\n    \"\"\"Abstract job scheduler interface for async task management.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    async def add_job(\n        self,\n        target: JobTarget,\n        /,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; ULID:\n        \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n        ...\n\n    @abstractmethod\n    async def get_status(self, job_id: ULID) -&gt; JobStatus:\n        \"\"\"Get the status of a job.\"\"\"\n        ...\n\n    @abstractmethod\n    async def get_record(self, job_id: ULID) -&gt; JobRecord:\n        \"\"\"Get the full record of a job.\"\"\"\n        ...\n\n    @abstractmethod\n    async def get_all_records(self) -&gt; list[JobRecord]:\n        \"\"\"Get all job records.\"\"\"\n        ...\n\n    @abstractmethod\n    async def cancel(self, job_id: ULID) -&gt; bool:\n        \"\"\"Cancel a running job.\"\"\"\n        ...\n\n    @abstractmethod\n    async def delete(self, job_id: ULID) -&gt; None:\n        \"\"\"Delete a job record.\"\"\"\n        ...\n\n    @abstractmethod\n    async def wait(self, job_id: ULID, timeout: float | None = None) -&gt; None:\n        \"\"\"Wait for a job to complete.\"\"\"\n        ...\n\n    @abstractmethod\n    async def get_result(self, job_id: ULID) -&gt; Any:\n        \"\"\"Get the result of a completed job.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.scheduler.JobScheduler.add_job","title":"<code>add_job(target, /, *args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Add a job to the scheduler and return its ID.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def add_job(\n    self,\n    target: JobTarget,\n    /,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ULID:\n    \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.get_status","title":"<code>get_status(job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the status of a job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def get_status(self, job_id: ULID) -&gt; JobStatus:\n    \"\"\"Get the status of a job.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.get_record","title":"<code>get_record(job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the full record of a job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def get_record(self, job_id: ULID) -&gt; JobRecord:\n    \"\"\"Get the full record of a job.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.get_all_records","title":"<code>get_all_records()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get all job records.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def get_all_records(self) -&gt; list[JobRecord]:\n    \"\"\"Get all job records.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.cancel","title":"<code>cancel(job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Cancel a running job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def cancel(self, job_id: ULID) -&gt; bool:\n    \"\"\"Cancel a running job.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.delete","title":"<code>delete(job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Delete a job record.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def delete(self, job_id: ULID) -&gt; None:\n    \"\"\"Delete a job record.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.wait","title":"<code>wait(job_id, timeout=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Wait for a job to complete.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def wait(self, job_id: ULID, timeout: float | None = None) -&gt; None:\n    \"\"\"Wait for a job to complete.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.JobScheduler.get_result","title":"<code>get_result(job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the result of a completed job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>@abstractmethod\nasync def get_result(self, job_id: ULID) -&gt; Any:\n    \"\"\"Get the result of a completed job.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler","title":"<code>AIOJobScheduler</code>","text":"<p>               Bases: <code>JobScheduler</code></p> <p>In-memory asyncio scheduler. Sync callables run in thread pool, concurrency controlled via semaphore.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>class AIOJobScheduler(JobScheduler):\n    \"\"\"In-memory asyncio scheduler. Sync callables run in thread pool, concurrency controlled via semaphore.\"\"\"\n\n    name: str = Field(default=\"chap\")\n    max_concurrency: int | None = Field(default=None)\n\n    _records: dict[ULID, JobRecord] = PrivateAttr(default_factory=dict)\n    _results: dict[ULID, Any] = PrivateAttr(default_factory=dict)\n    _tasks: dict[ULID, asyncio.Task[Any]] = PrivateAttr(default_factory=dict)\n    _lock: asyncio.Lock = PrivateAttr(default_factory=asyncio.Lock)\n    _sema: asyncio.Semaphore | None = PrivateAttr(default=None)\n\n    def __init__(self, **data: Any):\n        \"\"\"Initialize scheduler with optional concurrency limit.\"\"\"\n        super().__init__(**data)\n        if self.max_concurrency and self.max_concurrency &gt; 0:\n            self._sema = asyncio.Semaphore(self.max_concurrency)\n\n    async def set_max_concurrency(self, n: int | None) -&gt; None:\n        \"\"\"Set maximum number of concurrent jobs.\"\"\"\n        async with self._lock:\n            self.max_concurrency = n\n            if n and n &gt; 0:\n                self._sema = asyncio.Semaphore(n)\n            else:\n                self._sema = None\n\n    async def add_job(\n        self,\n        target: JobTarget,\n        /,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; ULID:\n        \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n        now = datetime.now(timezone.utc)\n        jid = ULID()\n\n        record = JobRecord(\n            id=jid,\n            status=JobStatus.pending,\n            submitted_at=now,\n        )\n\n        async with self._lock:\n            if jid in self._tasks:\n                raise RuntimeError(f\"Job {jid!r} already scheduled\")\n            self._records[jid] = record\n\n        async def _execute_target() -&gt; Any:\n            if inspect.isawaitable(target):\n                if args or kwargs:\n                    # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                    if inspect.iscoroutine(target):\n                        target.close()\n                    raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n                return await target\n            if inspect.iscoroutinefunction(target):\n                return await target(*args, **kwargs)\n            return await asyncio.to_thread(target, *args, **kwargs)\n\n        async def _runner() -&gt; Any:\n            if self._sema:\n                async with self._sema:\n                    return await self._run_with_state(jid, _execute_target)\n            else:\n                return await self._run_with_state(jid, _execute_target)\n\n        task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n        def _drain(t: asyncio.Task[Any]) -&gt; None:\n            try:\n                t.result()\n            except Exception:\n                pass\n\n        task.add_done_callback(_drain)\n\n        async with self._lock:\n            self._tasks[jid] = task\n\n        return jid\n\n    async def _run_with_state(\n        self,\n        jid: ULID,\n        exec_fn: JobExecutor,\n    ) -&gt; Any:\n        \"\"\"Execute job function and manage its state transitions.\"\"\"\n        async with self._lock:\n            rec = self._records[jid]\n            rec.status = JobStatus.running\n            rec.started_at = datetime.now(timezone.utc)\n\n        try:\n            result = await exec_fn()\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.completed\n                rec.finished_at = datetime.now(timezone.utc)\n                self._results[jid] = result\n\n            return result\n\n        except asyncio.CancelledError:\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.canceled\n                rec.finished_at = datetime.now(timezone.utc)\n\n            raise\n\n        except Exception as e:\n            tb = traceback.format_exc()\n            # Extract clean error message (exception type and message only)\n            error_lines = tb.strip().split(\"\\n\")\n            clean_error = error_lines[-1] if error_lines else str(e)\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.failed\n                rec.finished_at = datetime.now(timezone.utc)\n                rec.error = clean_error\n                rec.error_traceback = tb\n\n            raise\n\n    async def get_all_records(self) -&gt; list[JobRecord]:\n        \"\"\"Get all job records sorted by submission time.\"\"\"\n        async with self._lock:\n            records = [r.model_copy(deep=True) for r in self._records.values()]\n\n        records.sort(\n            key=lambda r: getattr(r, \"submitted_at\", datetime.min.replace(tzinfo=timezone.utc)),\n            reverse=True,\n        )\n\n        return records\n\n    async def get_record(self, job_id: ULID) -&gt; JobRecord:\n        \"\"\"Get the full record of a job.\"\"\"\n        async with self._lock:\n            rec = self._records.get(job_id)\n\n            if rec is None:\n                raise KeyError(\"Job not found\")\n\n            return rec.model_copy(deep=True)\n\n    async def get_status(self, job_id: ULID) -&gt; JobStatus:\n        \"\"\"Get the status of a job.\"\"\"\n        async with self._lock:\n            rec = self._records.get(job_id)\n\n            if rec is None:\n                raise KeyError(\"Job not found\")\n\n            return rec.status\n\n    async def get_result(self, job_id: ULID) -&gt; Any:\n        \"\"\"Get the result of a completed job.\"\"\"\n        async with self._lock:\n            rec = self._records.get(job_id)\n\n            if rec is None:\n                raise KeyError(\"Job not found\")\n\n            if rec.status == JobStatus.completed:\n                return self._results.get(job_id)\n\n            if rec.status == JobStatus.failed:\n                msg = getattr(rec, \"error\", \"Job failed\")\n                raise RuntimeError(msg)\n\n            raise RuntimeError(f\"Job not finished (status={rec.status})\")\n\n    async def wait(self, job_id: ULID, timeout: float | None = None) -&gt; None:\n        \"\"\"Wait for a job to complete.\"\"\"\n        async with self._lock:\n            task = self._tasks.get(job_id)\n\n            if task is None:\n                raise KeyError(\"Job not found\")\n\n        await asyncio.wait_for(asyncio.shield(task), timeout=timeout)\n\n    async def cancel(self, job_id: ULID) -&gt; bool:\n        \"\"\"Cancel a running job.\"\"\"\n        async with self._lock:\n            task = self._tasks.get(job_id)\n            exists = job_id in self._records\n\n        if not exists:\n            raise KeyError(\"Job not found\")\n\n        if not task or task.done():\n            return False\n\n        task.cancel()\n\n        try:\n            await task\n        except asyncio.CancelledError:\n            pass\n\n        return True\n\n    async def delete(self, job_id: ULID) -&gt; None:\n        \"\"\"Delete a job record.\"\"\"\n        async with self._lock:\n            rec = self._records.get(job_id)\n            task = self._tasks.get(job_id)\n\n        if rec is None:\n            raise KeyError(\"Job not found\")\n\n        if task and not task.done():\n            task.cancel()\n\n            try:\n                await task\n            except asyncio.CancelledError:\n                pass\n\n        async with self._lock:\n            self._records.pop(job_id, None)\n            self._tasks.pop(job_id, None)\n            self._results.pop(job_id, None)\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize scheduler with optional concurrency limit.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize scheduler with optional concurrency limit.\"\"\"\n    super().__init__(**data)\n    if self.max_concurrency and self.max_concurrency &gt; 0:\n        self._sema = asyncio.Semaphore(self.max_concurrency)\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.set_max_concurrency","title":"<code>set_max_concurrency(n)</code>  <code>async</code>","text":"<p>Set maximum number of concurrent jobs.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def set_max_concurrency(self, n: int | None) -&gt; None:\n    \"\"\"Set maximum number of concurrent jobs.\"\"\"\n    async with self._lock:\n        self.max_concurrency = n\n        if n and n &gt; 0:\n            self._sema = asyncio.Semaphore(n)\n        else:\n            self._sema = None\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.add_job","title":"<code>add_job(target, /, *args, **kwargs)</code>  <code>async</code>","text":"<p>Add a job to the scheduler and return its ID.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def add_job(\n    self,\n    target: JobTarget,\n    /,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ULID:\n    \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n    now = datetime.now(timezone.utc)\n    jid = ULID()\n\n    record = JobRecord(\n        id=jid,\n        status=JobStatus.pending,\n        submitted_at=now,\n    )\n\n    async with self._lock:\n        if jid in self._tasks:\n            raise RuntimeError(f\"Job {jid!r} already scheduled\")\n        self._records[jid] = record\n\n    async def _execute_target() -&gt; Any:\n        if inspect.isawaitable(target):\n            if args or kwargs:\n                # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                if inspect.iscoroutine(target):\n                    target.close()\n                raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n            return await target\n        if inspect.iscoroutinefunction(target):\n            return await target(*args, **kwargs)\n        return await asyncio.to_thread(target, *args, **kwargs)\n\n    async def _runner() -&gt; Any:\n        if self._sema:\n            async with self._sema:\n                return await self._run_with_state(jid, _execute_target)\n        else:\n            return await self._run_with_state(jid, _execute_target)\n\n    task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n    def _drain(t: asyncio.Task[Any]) -&gt; None:\n        try:\n            t.result()\n        except Exception:\n            pass\n\n    task.add_done_callback(_drain)\n\n    async with self._lock:\n        self._tasks[jid] = task\n\n    return jid\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.get_all_records","title":"<code>get_all_records()</code>  <code>async</code>","text":"<p>Get all job records sorted by submission time.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def get_all_records(self) -&gt; list[JobRecord]:\n    \"\"\"Get all job records sorted by submission time.\"\"\"\n    async with self._lock:\n        records = [r.model_copy(deep=True) for r in self._records.values()]\n\n    records.sort(\n        key=lambda r: getattr(r, \"submitted_at\", datetime.min.replace(tzinfo=timezone.utc)),\n        reverse=True,\n    )\n\n    return records\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.get_record","title":"<code>get_record(job_id)</code>  <code>async</code>","text":"<p>Get the full record of a job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def get_record(self, job_id: ULID) -&gt; JobRecord:\n    \"\"\"Get the full record of a job.\"\"\"\n    async with self._lock:\n        rec = self._records.get(job_id)\n\n        if rec is None:\n            raise KeyError(\"Job not found\")\n\n        return rec.model_copy(deep=True)\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.get_status","title":"<code>get_status(job_id)</code>  <code>async</code>","text":"<p>Get the status of a job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def get_status(self, job_id: ULID) -&gt; JobStatus:\n    \"\"\"Get the status of a job.\"\"\"\n    async with self._lock:\n        rec = self._records.get(job_id)\n\n        if rec is None:\n            raise KeyError(\"Job not found\")\n\n        return rec.status\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.get_result","title":"<code>get_result(job_id)</code>  <code>async</code>","text":"<p>Get the result of a completed job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def get_result(self, job_id: ULID) -&gt; Any:\n    \"\"\"Get the result of a completed job.\"\"\"\n    async with self._lock:\n        rec = self._records.get(job_id)\n\n        if rec is None:\n            raise KeyError(\"Job not found\")\n\n        if rec.status == JobStatus.completed:\n            return self._results.get(job_id)\n\n        if rec.status == JobStatus.failed:\n            msg = getattr(rec, \"error\", \"Job failed\")\n            raise RuntimeError(msg)\n\n        raise RuntimeError(f\"Job not finished (status={rec.status})\")\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.wait","title":"<code>wait(job_id, timeout=None)</code>  <code>async</code>","text":"<p>Wait for a job to complete.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def wait(self, job_id: ULID, timeout: float | None = None) -&gt; None:\n    \"\"\"Wait for a job to complete.\"\"\"\n    async with self._lock:\n        task = self._tasks.get(job_id)\n\n        if task is None:\n            raise KeyError(\"Job not found\")\n\n    await asyncio.wait_for(asyncio.shield(task), timeout=timeout)\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.cancel","title":"<code>cancel(job_id)</code>  <code>async</code>","text":"<p>Cancel a running job.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def cancel(self, job_id: ULID) -&gt; bool:\n    \"\"\"Cancel a running job.\"\"\"\n    async with self._lock:\n        task = self._tasks.get(job_id)\n        exists = job_id in self._records\n\n    if not exists:\n        raise KeyError(\"Job not found\")\n\n    if not task or task.done():\n        return False\n\n    task.cancel()\n\n    try:\n        await task\n    except asyncio.CancelledError:\n        pass\n\n    return True\n</code></pre>"},{"location":"api-reference/#servicekit.scheduler.AIOJobScheduler.delete","title":"<code>delete(job_id)</code>  <code>async</code>","text":"<p>Delete a job record.</p> Source code in <code>src/servicekit/scheduler.py</code> <pre><code>async def delete(self, job_id: ULID) -&gt; None:\n    \"\"\"Delete a job record.\"\"\"\n    async with self._lock:\n        rec = self._records.get(job_id)\n        task = self._tasks.get(job_id)\n\n    if rec is None:\n        raise KeyError(\"Job not found\")\n\n    if task and not task.done():\n        task.cancel()\n\n        try:\n            await task\n        except asyncio.CancelledError:\n            pass\n\n    async with self._lock:\n        self._records.pop(job_id, None)\n        self._tasks.pop(job_id, None)\n        self._results.pop(job_id, None)\n</code></pre>"},{"location":"api-reference/#types","title":"Types","text":""},{"location":"api-reference/#servicekit.types","title":"<code>types</code>","text":"<p>Custom types for servicekit - SQLAlchemy and Pydantic types.</p>"},{"location":"api-reference/#servicekit.types-attributes","title":"Attributes","text":""},{"location":"api-reference/#servicekit.types.JsonSafe","title":"<code>JsonSafe = Annotated[Any, PlainSerializer(_serialize_with_metadata, return_type=Any)]</code>  <code>module-attribute</code>","text":"<p>Pydantic type for JSON-safe serialization with graceful handling of non-serializable values.</p>"},{"location":"api-reference/#servicekit.types-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.types.ULIDType","title":"<code>ULIDType</code>","text":"<p>               Bases: <code>TypeDecorator[ULID]</code></p> <p>SQLAlchemy custom type for ULID stored as 26-character strings.</p> Source code in <code>src/servicekit/types.py</code> <pre><code>class ULIDType(TypeDecorator[ULID]):\n    \"\"\"SQLAlchemy custom type for ULID stored as 26-character strings.\"\"\"\n\n    impl = String(26)\n    cache_ok = True\n\n    def process_bind_param(self, value: ULID | str | None, dialect: Any) -&gt; str | None:\n        \"\"\"Convert ULID to string for database storage.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, str):\n            return str(ULID.from_str(value))  # Validate and normalize\n        return str(value)\n\n    def process_result_value(self, value: str | None, dialect: Any) -&gt; ULID | None:\n        \"\"\"Convert string from database to ULID object.\"\"\"\n        if value is None:\n            return None\n        return ULID.from_str(value)\n</code></pre>"},{"location":"api-reference/#servicekit.types.ULIDType-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.types.ULIDType.process_bind_param","title":"<code>process_bind_param(value, dialect)</code>","text":"<p>Convert ULID to string for database storage.</p> Source code in <code>src/servicekit/types.py</code> <pre><code>def process_bind_param(self, value: ULID | str | None, dialect: Any) -&gt; str | None:\n    \"\"\"Convert ULID to string for database storage.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, str):\n        return str(ULID.from_str(value))  # Validate and normalize\n    return str(value)\n</code></pre>"},{"location":"api-reference/#servicekit.types.ULIDType.process_result_value","title":"<code>process_result_value(value, dialect)</code>","text":"<p>Convert string from database to ULID object.</p> Source code in <code>src/servicekit/types.py</code> <pre><code>def process_result_value(self, value: str | None, dialect: Any) -&gt; ULID | None:\n    \"\"\"Convert string from database to ULID object.\"\"\"\n    if value is None:\n        return None\n    return ULID.from_str(value)\n</code></pre>"},{"location":"api-reference/#logging","title":"Logging","text":""},{"location":"api-reference/#servicekit.logging","title":"<code>logging</code>","text":"<p>Structured logging configuration with request tracing support.</p>"},{"location":"api-reference/#servicekit.logging-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.logging.configure_logging","title":"<code>configure_logging()</code>","text":"<p>Configure structlog and intercept standard library logging.</p> Source code in <code>src/servicekit/logging.py</code> <pre><code>def configure_logging() -&gt; None:\n    \"\"\"Configure structlog and intercept standard library logging.\"\"\"\n    log_format = os.getenv(\"LOG_FORMAT\", \"console\").lower()\n    log_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n    level = getattr(logging, log_level, logging.INFO)\n\n    # Shared processors for structlog\n    shared_processors: list[Processor] = [\n        structlog.contextvars.merge_contextvars,\n        structlog.stdlib.add_log_level,\n        structlog.processors.TimeStamper(fmt=\"iso\", utc=True),\n        structlog.processors.StackInfoRenderer(),\n    ]\n\n    # Choose renderer based on format\n    if log_format == \"json\":\n        formatter_processors = shared_processors + [\n            structlog.stdlib.ProcessorFormatter.remove_processors_meta,\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ]\n    else:\n        formatter_processors = shared_processors + [\n            structlog.stdlib.ProcessorFormatter.remove_processors_meta,\n            structlog.dev.ConsoleRenderer(colors=True),\n        ]\n\n    # Configure structlog to use standard library logging\n    structlog.configure(\n        processors=[\n            structlog.contextvars.merge_contextvars,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\", utc=True),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.CallsiteParameterAdder(\n                [\n                    structlog.processors.CallsiteParameter.FILENAME,\n                    structlog.processors.CallsiteParameter.LINENO,\n                    structlog.processors.CallsiteParameter.FUNC_NAME,\n                ]\n            ),\n            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,\n        ],\n        wrapper_class=structlog.make_filtering_bound_logger(level),\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    # Configure standard library logging to use structlog formatter\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(structlog.stdlib.ProcessorFormatter(processors=formatter_processors))\n\n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.handlers.clear()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(level)\n\n    # Configure uvicorn and gunicorn loggers to use the same handler\n    for logger_name in [\"uvicorn\", \"uvicorn.access\", \"uvicorn.error\", \"gunicorn.access\", \"gunicorn.error\"]:\n        logger = logging.getLogger(logger_name)\n        logger.handlers.clear()\n        logger.addHandler(handler)\n        logger.setLevel(level)\n        logger.propagate = False\n</code></pre>"},{"location":"api-reference/#servicekit.logging.get_logger","title":"<code>get_logger(name=None)</code>","text":"<p>Get a configured structlog logger instance.</p> Source code in <code>src/servicekit/logging.py</code> <pre><code>def get_logger(name: str | None = None) -&gt; Any:\n    \"\"\"Get a configured structlog logger instance.\"\"\"\n    return structlog.get_logger(name)\n</code></pre>"},{"location":"api-reference/#servicekit.logging.add_request_context","title":"<code>add_request_context(**context)</code>","text":"<p>Add context variables that will be included in all log messages.</p> Source code in <code>src/servicekit/logging.py</code> <pre><code>def add_request_context(**context: Any) -&gt; None:\n    \"\"\"Add context variables that will be included in all log messages.\"\"\"\n    structlog.contextvars.bind_contextvars(**context)\n</code></pre>"},{"location":"api-reference/#servicekit.logging.clear_request_context","title":"<code>clear_request_context(*keys)</code>","text":"<p>Clear specific context variables.</p> Source code in <code>src/servicekit/logging.py</code> <pre><code>def clear_request_context(*keys: str) -&gt; None:\n    \"\"\"Clear specific context variables.\"\"\"\n    structlog.contextvars.unbind_contextvars(*keys)\n</code></pre>"},{"location":"api-reference/#servicekit.logging.reset_request_context","title":"<code>reset_request_context()</code>","text":"<p>Clear all context variables.</p> Source code in <code>src/servicekit/logging.py</code> <pre><code>def reset_request_context() -&gt; None:\n    \"\"\"Clear all context variables.\"\"\"\n    structlog.contextvars.clear_contextvars()\n</code></pre>"},{"location":"api-reference/#data-interchange","title":"Data Interchange","text":"<p>Universal data schemas for HTTP APIs.</p>"},{"location":"api-reference/#dataframe","title":"DataFrame","text":""},{"location":"api-reference/#servicekit.data.DataFrame","title":"<code>DataFrame</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Universal interchange format for tabular data from pandas, polars, xarray, and other libraries.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>class DataFrame(BaseModel):\n    \"\"\"Universal interchange format for tabular data from pandas, polars, xarray, and other libraries.\"\"\"\n\n    columns: list[str]\n    data: list[list[Any]]\n\n    @classmethod\n    def from_pandas(cls, df: Any) -&gt; Self:\n        \"\"\"Create schema from pandas DataFrame.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for from_pandas(). Install with: uv add pandas\") from None\n\n        if not isinstance(df, pd.DataFrame):\n            raise TypeError(f\"Expected pandas DataFrame, got {type(df)}\")\n\n        return cls(\n            columns=df.columns.tolist(),\n            data=df.values.tolist(),\n        )\n\n    @classmethod\n    def from_polars(cls, df: Any) -&gt; Self:\n        \"\"\"Create schema from Polars DataFrame.\"\"\"\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\"polars is required for from_polars(). Install with: uv add polars\") from None\n\n        if not isinstance(df, pl.DataFrame):\n            raise TypeError(f\"Expected Polars DataFrame, got {type(df)}\")\n\n        return cls(\n            columns=df.columns,\n            data=[list(row) for row in df.rows()],\n        )\n\n    @classmethod\n    def from_xarray(cls, da: Any) -&gt; Self:\n        \"\"\"Create schema from xarray DataArray (2D only).\"\"\"\n        try:\n            import xarray as xr\n        except ImportError:\n            raise ImportError(\"xarray is required for from_xarray(). Install with: uv add xarray\") from None\n\n        if not isinstance(da, xr.DataArray):\n            raise TypeError(f\"Expected xarray DataArray, got {type(da)}\")\n\n        if len(da.dims) != 2:\n            raise ValueError(f\"Only 2D DataArrays supported, got {len(da.dims)} dimensions\")\n\n        # Convert to pandas then use from_pandas\n        pdf = da.to_pandas()\n        return cls.from_pandas(pdf)\n\n    @classmethod\n    def from_dict(cls, data: dict[str, list[Any]]) -&gt; Self:\n        \"\"\"Create schema from dictionary of columns.\"\"\"\n        if not data:\n            return cls(columns=[], data=[])\n\n        columns = list(data.keys())\n        num_rows = len(next(iter(data.values())))\n\n        if not all(len(vals) == num_rows for vals in data.values()):\n            raise ValueError(\"All columns must have the same length\")\n\n        rows = [[data[col][i] for col in columns] for i in range(num_rows)]\n\n        return cls(columns=columns, data=rows)\n\n    @classmethod\n    def from_records(cls, records: list[dict[str, Any]]) -&gt; Self:\n        \"\"\"Create schema from list of records (row-oriented).\"\"\"\n        if not records:\n            return cls(columns=[], data=[])\n\n        columns = list(records[0].keys())\n        data = [[record[col] for col in columns] for record in records]\n\n        return cls(columns=columns, data=data)\n\n    @classmethod\n    def from_csv(\n        cls,\n        path: str | Path | None = None,\n        *,\n        csv_string: str | None = None,\n        delimiter: str = \",\",\n        has_header: bool = True,\n        encoding: str = \"utf-8\",\n    ) -&gt; Self:\n        \"\"\"Create DataFrame from CSV file or string.\"\"\"\n        # Validate mutually exclusive parameters\n        if path is None and csv_string is None:\n            raise ValueError(\"Either path or csv_string must be provided\")\n        if path is not None and csv_string is not None:\n            raise ValueError(\"path and csv_string are mutually exclusive\")\n\n        # Read CSV data\n        if path is not None:\n            path_obj = Path(path)\n            if not path_obj.exists():\n                raise FileNotFoundError(f\"File not found: {path}\")\n            with path_obj.open(\"r\", encoding=encoding, newline=\"\") as f:\n                reader = csv.reader(f, delimiter=delimiter)\n                rows = list(reader)\n        else:\n            # csv_string is not None\n            string_io = io.StringIO(csv_string)\n            reader = csv.reader(string_io, delimiter=delimiter)\n            rows = list(reader)\n\n        # Handle empty CSV\n        if not rows:\n            return cls(columns=[], data=[])\n\n        # Extract columns and data\n        if has_header:\n            columns = rows[0]\n            data = rows[1:]\n        else:\n            # Generate column names\n            num_cols = len(rows[0]) if rows else 0\n            columns = [f\"col_{i}\" for i in range(num_cols)]\n            data = rows\n\n        return cls(columns=columns, data=data)\n\n    def to_pandas(self) -&gt; Any:\n        \"\"\"Convert schema to pandas DataFrame.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for to_pandas(). Install with: uv add pandas\") from None\n\n        return pd.DataFrame(self.data, columns=self.columns)\n\n    def to_polars(self) -&gt; Any:\n        \"\"\"Convert schema to Polars DataFrame.\"\"\"\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\"polars is required for to_polars(). Install with: uv add polars\") from None\n\n        return pl.DataFrame(self.data, schema=self.columns, orient=\"row\")\n\n    def to_dict(self, orient: Literal[\"dict\", \"list\", \"records\"] = \"dict\") -&gt; Any:\n        \"\"\"Convert schema to dictionary with specified orient (dict, list, or records).\"\"\"\n        if orient == \"dict\":\n            return {col: {i: self.data[i][j] for i in range(len(self.data))} for j, col in enumerate(self.columns)}\n        elif orient == \"list\":\n            return {col: [row[j] for row in self.data] for j, col in enumerate(self.columns)}\n        elif orient == \"records\":\n            return [{col: row[j] for j, col in enumerate(self.columns)} for row in self.data]\n        else:\n            raise ValueError(f\"Invalid orient: {orient}\")\n\n    def to_csv(\n        self,\n        path: str | Path | None = None,\n        *,\n        delimiter: str = \",\",\n        include_header: bool = True,\n        encoding: str = \"utf-8\",\n    ) -&gt; str | None:\n        \"\"\"Export DataFrame to CSV file or string.\"\"\"\n        # Write to string buffer or file\n        if path is None:\n            # Return as string\n            output = io.StringIO()\n            writer = csv.writer(output, delimiter=delimiter)\n\n            if include_header:\n                writer.writerow(self.columns)\n\n            writer.writerows(self.data)\n\n            return output.getvalue()\n        else:\n            # Write to file\n            path_obj = Path(path)\n            with path_obj.open(\"w\", encoding=encoding, newline=\"\") as f:\n                writer = csv.writer(f, delimiter=delimiter)\n\n                if include_header:\n                    writer.writerow(self.columns)\n\n                writer.writerows(self.data)\n\n            return None\n\n    # Convenience aliases\n    from_dataframe = from_pandas\n    to_dataframe = to_pandas\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Return tuple representing dimensionality of the DataFrame.\"\"\"\n        return (len(self.data), len(self.columns))\n\n    @property\n    def empty(self) -&gt; bool:\n        \"\"\"Indicator whether DataFrame is empty.\"\"\"\n        return len(self.data) == 0 or len(self.columns) == 0\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Return int representing number of elements in this object.\"\"\"\n        return len(self.data) * len(self.columns)\n\n    @property\n    def ndim(self) -&gt; int:\n        \"\"\"Return int representing number of axes/array dimensions.\"\"\"\n        return 2\n\n    def head(self, n: int = 5) -&gt; Self:\n        \"\"\"Return first n rows.\"\"\"\n        if n &gt;= 0:\n            selected_data = self.data[:n]\n        else:\n            selected_data = self.data[:n] if n != 0 else self.data\n        return self.__class__(columns=self.columns, data=selected_data)\n\n    def tail(self, n: int = 5) -&gt; Self:\n        \"\"\"Return last n rows.\"\"\"\n        if n &gt;= 0:\n            selected_data = self.data[-n:] if n &gt; 0 else []\n        else:\n            selected_data = self.data[abs(n) :]\n        return self.__class__(columns=self.columns, data=selected_data)\n\n    def sample(\n        self,\n        n: int | None = None,\n        frac: float | None = None,\n        *,\n        random_state: int | None = None,\n    ) -&gt; Self:\n        \"\"\"Return random sample of rows.\"\"\"\n        # Validate parameters\n        if n is None and frac is None:\n            raise ValueError(\"Either n or frac must be provided\")\n        if n is not None and frac is not None:\n            raise ValueError(\"n and frac are mutually exclusive\")\n\n        # Set random seed if provided\n        if random_state is not None:\n            random.seed(random_state)\n\n        # Calculate sample size\n        total_rows = len(self.data)\n        if frac is not None:\n            if frac &gt; 1.0:\n                raise ValueError(\"frac must be &lt;= 1.0\")\n            sample_size = int(total_rows * frac)\n        else:\n            sample_size = min(n, total_rows) if n is not None else 0\n\n        # Sample indices\n        if sample_size &gt;= total_rows:\n            sampled_indices = list(range(total_rows))\n            random.shuffle(sampled_indices)\n        else:\n            sampled_indices = random.sample(range(total_rows), sample_size)\n\n        # Extract sampled rows\n        sampled_data = [self.data[i] for i in sampled_indices]\n\n        return self.__class__(columns=self.columns, data=sampled_data)\n\n    def select(self, columns: list[str]) -&gt; Self:\n        \"\"\"Return DataFrame with only specified columns.\"\"\"\n        # Validate all columns exist\n        for col in columns:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Get column indices\n        indices = [self.columns.index(col) for col in columns]\n\n        # Extract data for selected columns\n        new_data = [[row[i] for i in indices] for row in self.data]\n\n        return self.__class__(columns=columns, data=new_data)\n\n    def drop(self, columns: list[str]) -&gt; Self:\n        \"\"\"Return DataFrame without specified columns.\"\"\"\n        # Validate all columns exist\n        for col in columns:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Get columns to keep\n        keep_cols = [c for c in self.columns if c not in columns]\n\n        # Get indices for columns to keep\n        indices = [self.columns.index(col) for col in keep_cols]\n\n        # Extract data for kept columns\n        new_data = [[row[i] for i in indices] for row in self.data]\n\n        return self.__class__(columns=keep_cols, data=new_data)\n\n    def rename(self, mapper: dict[str, str]) -&gt; Self:\n        \"\"\"Return DataFrame with renamed columns.\"\"\"\n        # Validate all old column names exist\n        for old_name in mapper:\n            if old_name not in self.columns:\n                raise KeyError(f\"Column '{old_name}' not found in DataFrame\")\n\n        # Create new column list\n        new_cols = [mapper.get(col, col) for col in self.columns]\n\n        # Check for duplicates\n        if len(new_cols) != len(set(new_cols)):\n            raise ValueError(\"Renaming would create duplicate column names\")\n\n        return self.__class__(columns=new_cols, data=self.data)\n\n    def rename_columns(self, mapper: dict[str, str]) -&gt; Self:\n        \"\"\"Return DataFrame with renamed columns (alias for rename).\"\"\"\n        return self.rename(mapper)\n\n    def validate_structure(self) -&gt; None:\n        \"\"\"Validate DataFrame structure.\"\"\"\n        # Check for empty column names\n        for i, col in enumerate(self.columns):\n            if col == \"\":\n                raise ValueError(f\"Column at index {i} is empty\")\n\n        # Check for duplicate column names\n        if len(self.columns) != len(set(self.columns)):\n            duplicates = [col for col in self.columns if self.columns.count(col) &gt; 1]\n            raise ValueError(f\"Duplicate column names found: {set(duplicates)}\")\n\n        # Check all rows have same length as columns\n        num_cols = len(self.columns)\n        for i, row in enumerate(self.data):\n            if len(row) != num_cols:\n                raise ValueError(f\"Row {i} has {len(row)} values, expected {num_cols}\")\n\n    def infer_types(self) -&gt; dict[str, str]:\n        \"\"\"Infer column data types.\"\"\"\n        result: dict[str, str] = {}\n\n        for col_idx, col_name in enumerate(self.columns):\n            # Extract all values for this column\n            values = [row[col_idx] for row in self.data]\n\n            # Filter out None values for type checking\n            non_null_values = [v for v in values if v is not None]\n\n            if not non_null_values:\n                result[col_name] = \"null\"\n                continue\n\n            # Check types\n            types_found = set()\n            for val in non_null_values:\n                if isinstance(val, bool):\n                    types_found.add(\"bool\")\n                elif isinstance(val, int):\n                    types_found.add(\"int\")\n                elif isinstance(val, float):\n                    types_found.add(\"float\")\n                elif isinstance(val, str):\n                    types_found.add(\"str\")\n                else:\n                    types_found.add(\"other\")\n\n            # Determine final type\n            if len(types_found) &gt; 1:\n                # Special case: int and float can be treated as float\n                if types_found == {\"int\", \"float\"}:\n                    result[col_name] = \"float\"\n                else:\n                    result[col_name] = \"mixed\"\n            elif \"bool\" in types_found:\n                result[col_name] = \"bool\"\n            elif \"int\" in types_found:\n                result[col_name] = \"int\"\n            elif \"float\" in types_found:\n                result[col_name] = \"float\"\n            elif \"str\" in types_found:\n                result[col_name] = \"str\"\n            else:\n                result[col_name] = \"mixed\"\n\n        return result\n\n    def has_nulls(self) -&gt; dict[str, bool]:\n        \"\"\"Check for null values in each column.\"\"\"\n        result: dict[str, bool] = {}\n\n        for col_idx, col_name in enumerate(self.columns):\n            # Check if any value in this column is None\n            has_null = any(row[col_idx] is None for row in self.data)\n            result[col_name] = has_null\n\n        return result\n\n    # Iteration and length\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of rows.\"\"\"\n        return len(self.data)\n\n    def __iter__(self) -&gt; Any:\n        \"\"\"Iterate over rows as dictionaries.\"\"\"\n        for row in self.data:\n            yield dict(zip(self.columns, row))\n\n    # JSON support\n\n    @classmethod\n    def from_json(cls, json_string: str) -&gt; Self:\n        \"\"\"Create DataFrame from JSON string (array of objects).\"\"\"\n        records = json.loads(json_string)\n        if not isinstance(records, list):\n            raise ValueError(\"JSON must be an array of objects\")\n        return cls.from_records(records)\n\n    def to_json(self, orient: Literal[\"records\", \"columns\"] = \"records\") -&gt; str:\n        \"\"\"Export DataFrame as JSON string.\"\"\"\n        # Map \"columns\" to \"list\" for to_dict()\n        dict_orient: Literal[\"dict\", \"list\", \"records\"] = \"list\" if orient == \"columns\" else orient\n        return json.dumps(self.to_dict(orient=dict_orient))\n\n    # Column access\n\n    def get_column(self, column: str) -&gt; list[Any]:\n        \"\"\"Get all values for a column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n        idx = self.columns.index(column)\n        return [row[idx] for row in self.data]\n\n    def __getitem__(self, key: str | list[str]) -&gt; list[Any] | Self:\n        \"\"\"Support df['col'] and df[['col1', 'col2']].\"\"\"\n        if isinstance(key, str):\n            return self.get_column(key)\n        return self.select(key)\n\n    # Analytics methods\n\n    def unique(self, column: str) -&gt; list[Any]:\n        \"\"\"Get unique values from a column (preserves order).\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        seen = set()\n        result = []\n        for row in self.data:\n            val = row[col_idx]\n            if val not in seen:\n                seen.add(val)\n                result.append(val)\n        return result\n\n    def value_counts(self, column: str) -&gt; dict[Any, int]:\n        \"\"\"Count occurrences of each unique value in column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        counts: dict[Any, int] = {}\n        for row in self.data:\n            val = row[col_idx]\n            counts[val] = counts.get(val, 0) + 1\n        return counts\n\n    def sort(self, by: str, ascending: bool = True) -&gt; Self:\n        \"\"\"Sort DataFrame by column.\"\"\"\n        if by not in self.columns:\n            raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n        col_idx = self.columns.index(by)\n\n        # Sort with None values at the end\n        def sort_key(row: list[Any]) -&gt; tuple[int, Any]:\n            val = row[col_idx]\n            if val is None:\n                # Use a tuple to ensure None sorts last\n                return (1, None) if ascending else (0, None)\n            return (0, val) if ascending else (1, val)\n\n        sorted_data = sorted(self.data, key=sort_key, reverse=not ascending)\n        return self.__class__(columns=self.columns, data=sorted_data)\n\n    # Row filtering and transformation\n\n    def filter(self, predicate: Any) -&gt; Self:\n        \"\"\"Filter rows using a predicate function.\"\"\"\n        filtered_data = []\n        for row in self.data:\n            row_dict = dict(zip(self.columns, row))\n            if predicate(row_dict):\n                filtered_data.append(row)\n        return self.__class__(columns=self.columns, data=filtered_data)\n\n    def apply(self, func: Any, column: str) -&gt; Self:\n        \"\"\"Apply function to column values.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        new_data = []\n        for row in self.data:\n            new_row = row.copy()\n            new_row[col_idx] = func(row[col_idx])\n            new_data.append(new_row)\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def add_column(self, name: str, values: list[Any]) -&gt; Self:\n        \"\"\"Add new column to DataFrame.\"\"\"\n        if name in self.columns:\n            raise ValueError(f\"Column '{name}' already exists\")\n\n        if len(values) != len(self.data):\n            raise ValueError(f\"Values length ({len(values)}) must match row count ({len(self.data)})\")\n\n        new_columns = self.columns + [name]\n        new_data = [row + [values[i]] for i, row in enumerate(self.data)]\n\n        return self.__class__(columns=new_columns, data=new_data)\n\n    def drop_rows(self, indices: list[int]) -&gt; Self:\n        \"\"\"Drop rows by index.\"\"\"\n        indices_set = set(indices)\n        new_data = [row for i, row in enumerate(self.data) if i not in indices_set]\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def drop_duplicates(self, subset: list[str] | None = None) -&gt; Self:\n        \"\"\"Remove duplicate rows.\"\"\"\n        # Validate subset columns\n        if subset is not None:\n            for col in subset:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n            col_indices = [self.columns.index(col) for col in subset]\n        else:\n            col_indices = list(range(len(self.columns)))\n\n        # Track seen values\n        seen = set()\n        new_data = []\n\n        for row in self.data:\n            # Create tuple of relevant column values\n            key = tuple(row[i] for i in col_indices)\n\n            if key not in seen:\n                seen.add(key)\n                new_data.append(row)\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def fillna(self, value: Any | dict[str, Any]) -&gt; Self:\n        \"\"\"Replace None values.\"\"\"\n        if isinstance(value, dict):\n            # Validate column names\n            for col in value:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n            # Create mapping of column index to fill value\n            fill_map = {self.columns.index(col): val for col, val in value.items()}\n\n            # Fill values\n            new_data = []\n            for row in self.data:\n                new_row = [fill_map[i] if i in fill_map and val is None else val for i, val in enumerate(row)]\n                new_data.append(new_row)\n        else:\n            # Single fill value for all None\n            new_data = [[value if val is None else val for val in row] for row in self.data]\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def concat(self, other: Self) -&gt; Self:\n        \"\"\"Concatenate DataFrames vertically (stack rows).\"\"\"\n        if self.columns != other.columns:\n            raise ValueError(f\"Column mismatch: {self.columns} != {other.columns}\")\n\n        combined_data = self.data + other.data\n        return self.__class__(columns=self.columns, data=combined_data)\n\n    def melt(\n        self,\n        id_vars: list[str] | None = None,\n        value_vars: list[str] | None = None,\n        var_name: str = \"variable\",\n        value_name: str = \"value\",\n    ) -&gt; Self:\n        \"\"\"Unpivot DataFrame from wide to long format.\"\"\"\n        # Handle empty DataFrame\n        if not self.columns or not self.data:\n            return self.__class__(columns=[var_name, value_name], data=[])\n\n        # Default id_vars to empty list if not specified\n        if id_vars is None:\n            id_vars = []\n\n        # Validate id_vars exist\n        for col in id_vars:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Default value_vars to all non-id columns\n        if value_vars is None:\n            value_vars = [col for col in self.columns if col not in id_vars]\n        else:\n            # Validate value_vars exist\n            for col in value_vars:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # If no value_vars to melt, return empty result\n        if not value_vars:\n            # Return just id columns if all columns are id_vars\n            if id_vars:\n                return self.select(id_vars)\n            return self.__class__(columns=[var_name, value_name], data=[])\n\n        # Check for column name conflicts\n        new_columns = id_vars + [var_name, value_name]\n        if len(new_columns) != len(set(new_columns)):\n            raise ValueError(\n                f\"Duplicate column names in result: {new_columns}. \"\n                f\"Choose different var_name or value_name to avoid conflicts.\"\n            )\n\n        # Get indices for id and value columns\n        id_indices = [self.columns.index(col) for col in id_vars]\n        value_indices = [(self.columns.index(col), col) for col in value_vars]\n\n        # Build melted data\n        melted_data: list[list[Any]] = []\n\n        for row in self.data:\n            # Extract id values for this row\n            id_values = [row[idx] for idx in id_indices]\n\n            # Create one new row for each value_var\n            for val_idx, var_col_name in value_indices:\n                new_row = id_values + [var_col_name, row[val_idx]]\n                melted_data.append(new_row)\n\n        return self.__class__(columns=new_columns, data=melted_data)\n\n    def pivot(self, index: str, columns: str, values: str) -&gt; Self:\n        \"\"\"Pivot DataFrame from long to wide format.\"\"\"\n        # Validate columns exist\n        for col_name, param in [(index, \"index\"), (columns, \"columns\"), (values, \"values\")]:\n            if col_name not in self.columns:\n                raise KeyError(f\"Column '{col_name}' not found in DataFrame (parameter: {param})\")\n\n        # Get column indices\n        index_idx = self.columns.index(index)\n        columns_idx = self.columns.index(columns)\n        values_idx = self.columns.index(values)\n\n        # Build pivot structure: dict[index_value, dict[column_value, value]]\n        pivot_dict: dict[Any, dict[Any, Any]] = {}\n        column_values_set: set[Any] = set()\n\n        for row in self.data:\n            idx_val = row[index_idx]\n            col_val = row[columns_idx]\n            val = row[values_idx]\n\n            # Track column values for final column list\n            column_values_set.add(col_val)\n\n            # Initialize nested dict if needed\n            if idx_val not in pivot_dict:\n                pivot_dict[idx_val] = {}\n\n            # Check for duplicates\n            if col_val in pivot_dict[idx_val]:\n                raise ValueError(\n                    f\"Duplicate entries found for index='{idx_val}' and columns='{col_val}'. \"\n                    f\"Cannot reshape with duplicate index/column combinations. \"\n                    f\"Consider using aggregation or removing duplicates first.\"\n                )\n\n            pivot_dict[idx_val] = {**pivot_dict[idx_val], col_val: val}\n\n        # Sort column values for consistent ordering\n        column_values = sorted(column_values_set, key=lambda x: (x is None, x))\n\n        # Build result columns: [index_column, col1, col2, ...]\n        result_columns = [index] + column_values\n\n        # Build result data\n        result_data: list[list[Any]] = []\n        for idx_val in sorted(pivot_dict.keys(), key=lambda x: (x is None, x)):\n            row_dict = pivot_dict[idx_val]\n            # Build row: [index_value, value_for_col1, value_for_col2, ...]\n            row = [idx_val] + [row_dict.get(col_val, None) for col_val in column_values]\n            result_data.append(row)\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    def merge(\n        self,\n        other: Self,\n        on: str | list[str] | None = None,\n        how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\",\n        left_on: str | list[str] | None = None,\n        right_on: str | list[str] | None = None,\n        suffixes: tuple[str, str] = (\"_x\", \"_y\"),\n    ) -&gt; Self:\n        \"\"\"Merge DataFrames using database-style join.\"\"\"\n        # Determine join keys\n        if on is not None:\n            if left_on is not None or right_on is not None:\n                raise ValueError(\"Cannot specify both 'on' and 'left_on'/'right_on'\")\n            left_keys = [on] if isinstance(on, str) else on\n            right_keys = left_keys\n        elif left_on is not None and right_on is not None:\n            left_keys = [left_on] if isinstance(left_on, str) else left_on\n            right_keys = [right_on] if isinstance(right_on, str) else right_on\n            if len(left_keys) != len(right_keys):\n                raise ValueError(\"left_on and right_on must have same length\")\n        else:\n            raise ValueError(\"Must specify either 'on' or both 'left_on' and 'right_on'\")\n\n        # Validate join keys exist\n        for key in left_keys:\n            if key not in self.columns:\n                raise KeyError(f\"Join key '{key}' not found in left DataFrame\")\n        for key in right_keys:\n            if key not in other.columns:\n                raise KeyError(f\"Join key '{key}' not found in right DataFrame\")\n\n        # Get indices for join keys\n        left_key_indices = [self.columns.index(k) for k in left_keys]\n        right_key_indices = [other.columns.index(k) for k in right_keys]\n\n        # Build lookup dict for right DataFrame: key_tuple -&gt; list[row_indices]\n        right_lookup: dict[tuple[Any, ...], list[int]] = {}\n        for row_idx, row in enumerate(other.data):\n            key_tuple = tuple(row[idx] for idx in right_key_indices)\n            if key_tuple not in right_lookup:\n                right_lookup[key_tuple] = []\n            right_lookup[key_tuple].append(row_idx)\n\n        # Determine result columns\n        left_suffix, right_suffix = suffixes\n\n        # Start with left DataFrame columns\n        result_columns = self.columns.copy()\n\n        # Add right DataFrame columns (excluding join keys if using 'on')\n        for col in other.columns:\n            if on is not None and col in left_keys:\n                # Skip join key columns from right when using 'on'\n                continue\n\n            if col in result_columns:\n                # Handle collision with suffix\n                result_columns.append(f\"{col}{right_suffix}\")\n                # Also need to rename left column\n                left_col_idx = result_columns.index(col)\n                result_columns[left_col_idx] = f\"{col}{left_suffix}\"\n            else:\n                result_columns.append(col)\n\n        # Get indices of right columns to include\n        right_col_indices = []\n        for col in other.columns:\n            if on is not None and col in right_keys:\n                continue\n            right_col_indices.append(other.columns.index(col))\n\n        # Perform join\n        result_data: list[list[Any]] = []\n        matched_right_indices: set[int] = set()\n\n        for left_row in self.data:\n            # Extract key from left row\n            left_key_tuple = tuple(left_row[idx] for idx in left_key_indices)\n\n            # Find matching rows in right DataFrame\n            right_matches = right_lookup.get(left_key_tuple, [])\n\n            if right_matches:\n                # Join matched rows\n                for right_idx in right_matches:\n                    matched_right_indices.add(right_idx)\n                    right_row = other.data[right_idx]\n\n                    # Build result row: left columns + right columns (excluding join keys)\n                    result_row = left_row.copy()\n                    for col_idx in right_col_indices:\n                        result_row.append(right_row[col_idx])\n\n                    result_data.append(result_row)\n            else:\n                # No match\n                if how in (\"left\", \"outer\"):\n                    # Include left row with None for right columns\n                    result_row = left_row.copy()\n                    result_row.extend([None] * len(right_col_indices))\n                    result_data.append(result_row)\n\n        # Handle right/outer joins - add unmatched right rows\n        if how in (\"right\", \"outer\"):\n            for right_idx, right_row in enumerate(other.data):\n                if right_idx not in matched_right_indices:\n                    # Build row with None for left columns\n                    result_row = [None] * len(self.columns)\n\n                    # Fill in join key values if using 'on'\n                    if on is not None:\n                        for left_idx, right_idx_key in zip(left_key_indices, right_key_indices):\n                            result_row[left_idx] = right_row[right_idx_key]\n\n                    # Add right columns\n                    for col_idx in right_col_indices:\n                        result_row.append(right_row[col_idx])\n\n                    result_data.append(result_row)\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    def transpose(self) -&gt; Self:\n        \"\"\"Transpose DataFrame by swapping rows and columns.\"\"\"\n        if not self.data:\n            # Empty DataFrame - return with swapped structure\n            return self.__class__(columns=[], data=[])\n\n        # First column becomes the new column names\n        # Remaining columns become data rows\n        if not self.columns:\n            return self.__class__(columns=[], data=[])\n\n        # Extract first column values as new column names\n        # Convert to strings to ensure valid column names\n        new_columns = [str(row[0]) for row in self.data]\n\n        # Transpose the remaining columns\n        num_original_cols = len(self.columns)\n        if num_original_cols == 1:\n            # Only one column (the index) - result is just column names as rows\n            single_col_data = [[col] for col in self.columns]\n            return self.__class__(columns=new_columns if new_columns else [\"0\"], data=single_col_data)\n\n        # Build transposed data\n        # Each original column (except first) becomes a row\n        # Each original row becomes a column\n        result_data: list[list[Any]] = []\n\n        for col_idx in range(1, num_original_cols):\n            # Original column name becomes first value in new row\n            row = [self.columns[col_idx]]\n            # Add values from each original row for this column\n            for orig_row in self.data:\n                row.append(orig_row[col_idx])\n            result_data.append(row)\n\n        # New columns: first is placeholder for original column names, rest are from first column\n        result_columns = [\"index\"] + new_columns\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    # Statistical methods\n\n    def describe(self) -&gt; Self:\n        \"\"\"Generate statistical summary for numeric columns.\"\"\"\n        import statistics\n\n        stats_rows: list[list[Any]] = []\n        stat_names = [\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n\n        for col_idx in range(len(self.columns)):\n            # Extract numeric values (filter None and non-numeric)\n            values = []\n            for row in self.data:\n                val = row[col_idx]\n                if val is not None and isinstance(val, (int, float)) and not isinstance(val, bool):\n                    values.append(float(val))\n\n            if not values:\n                # Non-numeric column - fill with None\n                stats_rows.append([None] * len(stat_names))\n                continue\n\n            # Calculate statistics\n            count = len(values)\n            mean = statistics.mean(values)\n            std = statistics.stdev(values) if count &gt; 1 else 0.0\n            min_val = min(values)\n            max_val = max(values)\n\n            # Quantiles\n            sorted_vals = sorted(values)\n            try:\n                q25 = statistics.quantiles(sorted_vals, n=4)[0] if count &gt; 1 else sorted_vals[0]\n                q50 = statistics.median(sorted_vals)\n                q75 = statistics.quantiles(sorted_vals, n=4)[2] if count &gt; 1 else sorted_vals[0]\n            except statistics.StatisticsError:\n                q25 = q50 = q75 = sorted_vals[0] if sorted_vals else 0.0\n\n            stats_rows.append([count, mean, std, min_val, q25, q50, q75, max_val])\n\n        # Transpose to make stats the rows and columns the columns\n        transposed_data = [\n            [stats_rows[col_idx][stat_idx] for col_idx in range(len(self.columns))]\n            for stat_idx in range(len(stat_names))\n        ]\n\n        return self.__class__(columns=self.columns, data=transposed_data).add_column(\"stat\", stat_names)\n\n    def groupby(self, by: str) -&gt; \"GroupBy\":\n        \"\"\"Group DataFrame by column values.\"\"\"\n        if by not in self.columns:\n            raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n        return GroupBy(self, by)\n\n    # Utility methods\n\n    def equals(self, other: Any) -&gt; bool:\n        \"\"\"Check if two DataFrames are identical.\"\"\"\n        if not isinstance(other, DataFrame):\n            return False\n        return self.columns == other.columns and self.data == other.data\n\n    def deepcopy(self) -&gt; Self:\n        \"\"\"Create a deep copy of the DataFrame.\"\"\"\n        import copy\n\n        return self.__class__(columns=self.columns.copy(), data=copy.deepcopy(self.data))\n\n    def isna(self) -&gt; Self:\n        \"\"\"Return DataFrame of booleans showing None locations.\"\"\"\n        null_data = [[val is None for val in row] for row in self.data]\n        return self.__class__(columns=self.columns, data=null_data)\n\n    def notna(self) -&gt; Self:\n        \"\"\"Return DataFrame of booleans showing non-None locations.\"\"\"\n        not_null_data = [[val is not None for val in row] for row in self.data]\n        return self.__class__(columns=self.columns, data=not_null_data)\n\n    def dropna(self, axis: Literal[0, 1] = 0, how: Literal[\"any\", \"all\"] = \"any\") -&gt; Self:\n        \"\"\"Drop rows or columns with None values.\"\"\"\n        if axis == 0:\n            # Drop rows\n            if how == \"any\":\n                # Drop rows with any None\n                new_data = [row for row in self.data if not any(val is None for val in row)]\n            else:\n                # Drop rows with all None\n                new_data = [row for row in self.data if not all(val is None for val in row)]\n            return self.__class__(columns=self.columns, data=new_data)\n        else:\n            # Drop columns (axis=1)\n            cols_to_keep = []\n            indices_to_keep = []\n\n            for col_idx, col_name in enumerate(self.columns):\n                col_values = [row[col_idx] for row in self.data]\n\n                if how == \"any\":\n                    # Keep column if no None values\n                    if not any(val is None for val in col_values):\n                        cols_to_keep.append(col_name)\n                        indices_to_keep.append(col_idx)\n                else:\n                    # Keep column if not all None\n                    if not all(val is None for val in col_values):\n                        cols_to_keep.append(col_name)\n                        indices_to_keep.append(col_idx)\n\n            # Extract data for kept columns\n            new_data = [[row[i] for i in indices_to_keep] for row in self.data]\n            return self.__class__(columns=cols_to_keep, data=new_data)\n\n    def nunique(self, column: str) -&gt; int:\n        \"\"\"Count number of unique values in column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        unique_values = set()\n        for row in self.data:\n            val = row[col_idx]\n            # Count None as a unique value\n            unique_values.add(val)\n        return len(unique_values)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame-attributes","title":"Attributes","text":""},{"location":"api-reference/#servicekit.data.DataFrame.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Return tuple representing dimensionality of the DataFrame.</p>"},{"location":"api-reference/#servicekit.data.DataFrame.empty","title":"<code>empty</code>  <code>property</code>","text":"<p>Indicator whether DataFrame is empty.</p>"},{"location":"api-reference/#servicekit.data.DataFrame.size","title":"<code>size</code>  <code>property</code>","text":"<p>Return int representing number of elements in this object.</p>"},{"location":"api-reference/#servicekit.data.DataFrame.ndim","title":"<code>ndim</code>  <code>property</code>","text":"<p>Return int representing number of axes/array dimensions.</p>"},{"location":"api-reference/#servicekit.data.DataFrame-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.data.DataFrame.from_pandas","title":"<code>from_pandas(df)</code>  <code>classmethod</code>","text":"<p>Create schema from pandas DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df: Any) -&gt; Self:\n    \"\"\"Create schema from pandas DataFrame.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for from_pandas(). Install with: uv add pandas\") from None\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"Expected pandas DataFrame, got {type(df)}\")\n\n    return cls(\n        columns=df.columns.tolist(),\n        data=df.values.tolist(),\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_polars","title":"<code>from_polars(df)</code>  <code>classmethod</code>","text":"<p>Create schema from Polars DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: Any) -&gt; Self:\n    \"\"\"Create schema from Polars DataFrame.\"\"\"\n    try:\n        import polars as pl\n    except ImportError:\n        raise ImportError(\"polars is required for from_polars(). Install with: uv add polars\") from None\n\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"Expected Polars DataFrame, got {type(df)}\")\n\n    return cls(\n        columns=df.columns,\n        data=[list(row) for row in df.rows()],\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_xarray","title":"<code>from_xarray(da)</code>  <code>classmethod</code>","text":"<p>Create schema from xarray DataArray (2D only).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_xarray(cls, da: Any) -&gt; Self:\n    \"\"\"Create schema from xarray DataArray (2D only).\"\"\"\n    try:\n        import xarray as xr\n    except ImportError:\n        raise ImportError(\"xarray is required for from_xarray(). Install with: uv add xarray\") from None\n\n    if not isinstance(da, xr.DataArray):\n        raise TypeError(f\"Expected xarray DataArray, got {type(da)}\")\n\n    if len(da.dims) != 2:\n        raise ValueError(f\"Only 2D DataArrays supported, got {len(da.dims)} dimensions\")\n\n    # Convert to pandas then use from_pandas\n    pdf = da.to_pandas()\n    return cls.from_pandas(pdf)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create schema from dictionary of columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, list[Any]]) -&gt; Self:\n    \"\"\"Create schema from dictionary of columns.\"\"\"\n    if not data:\n        return cls(columns=[], data=[])\n\n    columns = list(data.keys())\n    num_rows = len(next(iter(data.values())))\n\n    if not all(len(vals) == num_rows for vals in data.values()):\n        raise ValueError(\"All columns must have the same length\")\n\n    rows = [[data[col][i] for col in columns] for i in range(num_rows)]\n\n    return cls(columns=columns, data=rows)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_records","title":"<code>from_records(records)</code>  <code>classmethod</code>","text":"<p>Create schema from list of records (row-oriented).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_records(cls, records: list[dict[str, Any]]) -&gt; Self:\n    \"\"\"Create schema from list of records (row-oriented).\"\"\"\n    if not records:\n        return cls(columns=[], data=[])\n\n    columns = list(records[0].keys())\n    data = [[record[col] for col in columns] for record in records]\n\n    return cls(columns=columns, data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_csv","title":"<code>from_csv(path=None, *, csv_string=None, delimiter=',', has_header=True, encoding='utf-8')</code>  <code>classmethod</code>","text":"<p>Create DataFrame from CSV file or string.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    path: str | Path | None = None,\n    *,\n    csv_string: str | None = None,\n    delimiter: str = \",\",\n    has_header: bool = True,\n    encoding: str = \"utf-8\",\n) -&gt; Self:\n    \"\"\"Create DataFrame from CSV file or string.\"\"\"\n    # Validate mutually exclusive parameters\n    if path is None and csv_string is None:\n        raise ValueError(\"Either path or csv_string must be provided\")\n    if path is not None and csv_string is not None:\n        raise ValueError(\"path and csv_string are mutually exclusive\")\n\n    # Read CSV data\n    if path is not None:\n        path_obj = Path(path)\n        if not path_obj.exists():\n            raise FileNotFoundError(f\"File not found: {path}\")\n        with path_obj.open(\"r\", encoding=encoding, newline=\"\") as f:\n            reader = csv.reader(f, delimiter=delimiter)\n            rows = list(reader)\n    else:\n        # csv_string is not None\n        string_io = io.StringIO(csv_string)\n        reader = csv.reader(string_io, delimiter=delimiter)\n        rows = list(reader)\n\n    # Handle empty CSV\n    if not rows:\n        return cls(columns=[], data=[])\n\n    # Extract columns and data\n    if has_header:\n        columns = rows[0]\n        data = rows[1:]\n    else:\n        # Generate column names\n        num_cols = len(rows[0]) if rows else 0\n        columns = [f\"col_{i}\" for i in range(num_cols)]\n        data = rows\n\n    return cls(columns=columns, data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Convert schema to pandas DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; Any:\n    \"\"\"Convert schema to pandas DataFrame.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_pandas(). Install with: uv add pandas\") from None\n\n    return pd.DataFrame(self.data, columns=self.columns)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.to_polars","title":"<code>to_polars()</code>","text":"<p>Convert schema to Polars DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def to_polars(self) -&gt; Any:\n    \"\"\"Convert schema to Polars DataFrame.\"\"\"\n    try:\n        import polars as pl\n    except ImportError:\n        raise ImportError(\"polars is required for to_polars(). Install with: uv add polars\") from None\n\n    return pl.DataFrame(self.data, schema=self.columns, orient=\"row\")\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.to_dict","title":"<code>to_dict(orient='dict')</code>","text":"<p>Convert schema to dictionary with specified orient (dict, list, or records).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def to_dict(self, orient: Literal[\"dict\", \"list\", \"records\"] = \"dict\") -&gt; Any:\n    \"\"\"Convert schema to dictionary with specified orient (dict, list, or records).\"\"\"\n    if orient == \"dict\":\n        return {col: {i: self.data[i][j] for i in range(len(self.data))} for j, col in enumerate(self.columns)}\n    elif orient == \"list\":\n        return {col: [row[j] for row in self.data] for j, col in enumerate(self.columns)}\n    elif orient == \"records\":\n        return [{col: row[j] for j, col in enumerate(self.columns)} for row in self.data]\n    else:\n        raise ValueError(f\"Invalid orient: {orient}\")\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.to_csv","title":"<code>to_csv(path=None, *, delimiter=',', include_header=True, encoding='utf-8')</code>","text":"<p>Export DataFrame to CSV file or string.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def to_csv(\n    self,\n    path: str | Path | None = None,\n    *,\n    delimiter: str = \",\",\n    include_header: bool = True,\n    encoding: str = \"utf-8\",\n) -&gt; str | None:\n    \"\"\"Export DataFrame to CSV file or string.\"\"\"\n    # Write to string buffer or file\n    if path is None:\n        # Return as string\n        output = io.StringIO()\n        writer = csv.writer(output, delimiter=delimiter)\n\n        if include_header:\n            writer.writerow(self.columns)\n\n        writer.writerows(self.data)\n\n        return output.getvalue()\n    else:\n        # Write to file\n        path_obj = Path(path)\n        with path_obj.open(\"w\", encoding=encoding, newline=\"\") as f:\n            writer = csv.writer(f, delimiter=delimiter)\n\n            if include_header:\n                writer.writerow(self.columns)\n\n            writer.writerows(self.data)\n\n        return None\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.head","title":"<code>head(n=5)</code>","text":"<p>Return first n rows.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def head(self, n: int = 5) -&gt; Self:\n    \"\"\"Return first n rows.\"\"\"\n    if n &gt;= 0:\n        selected_data = self.data[:n]\n    else:\n        selected_data = self.data[:n] if n != 0 else self.data\n    return self.__class__(columns=self.columns, data=selected_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.tail","title":"<code>tail(n=5)</code>","text":"<p>Return last n rows.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def tail(self, n: int = 5) -&gt; Self:\n    \"\"\"Return last n rows.\"\"\"\n    if n &gt;= 0:\n        selected_data = self.data[-n:] if n &gt; 0 else []\n    else:\n        selected_data = self.data[abs(n) :]\n    return self.__class__(columns=self.columns, data=selected_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.sample","title":"<code>sample(n=None, frac=None, *, random_state=None)</code>","text":"<p>Return random sample of rows.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def sample(\n    self,\n    n: int | None = None,\n    frac: float | None = None,\n    *,\n    random_state: int | None = None,\n) -&gt; Self:\n    \"\"\"Return random sample of rows.\"\"\"\n    # Validate parameters\n    if n is None and frac is None:\n        raise ValueError(\"Either n or frac must be provided\")\n    if n is not None and frac is not None:\n        raise ValueError(\"n and frac are mutually exclusive\")\n\n    # Set random seed if provided\n    if random_state is not None:\n        random.seed(random_state)\n\n    # Calculate sample size\n    total_rows = len(self.data)\n    if frac is not None:\n        if frac &gt; 1.0:\n            raise ValueError(\"frac must be &lt;= 1.0\")\n        sample_size = int(total_rows * frac)\n    else:\n        sample_size = min(n, total_rows) if n is not None else 0\n\n    # Sample indices\n    if sample_size &gt;= total_rows:\n        sampled_indices = list(range(total_rows))\n        random.shuffle(sampled_indices)\n    else:\n        sampled_indices = random.sample(range(total_rows), sample_size)\n\n    # Extract sampled rows\n    sampled_data = [self.data[i] for i in sampled_indices]\n\n    return self.__class__(columns=self.columns, data=sampled_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.select","title":"<code>select(columns)</code>","text":"<p>Return DataFrame with only specified columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def select(self, columns: list[str]) -&gt; Self:\n    \"\"\"Return DataFrame with only specified columns.\"\"\"\n    # Validate all columns exist\n    for col in columns:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Get column indices\n    indices = [self.columns.index(col) for col in columns]\n\n    # Extract data for selected columns\n    new_data = [[row[i] for i in indices] for row in self.data]\n\n    return self.__class__(columns=columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.drop","title":"<code>drop(columns)</code>","text":"<p>Return DataFrame without specified columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def drop(self, columns: list[str]) -&gt; Self:\n    \"\"\"Return DataFrame without specified columns.\"\"\"\n    # Validate all columns exist\n    for col in columns:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Get columns to keep\n    keep_cols = [c for c in self.columns if c not in columns]\n\n    # Get indices for columns to keep\n    indices = [self.columns.index(col) for col in keep_cols]\n\n    # Extract data for kept columns\n    new_data = [[row[i] for i in indices] for row in self.data]\n\n    return self.__class__(columns=keep_cols, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.rename","title":"<code>rename(mapper)</code>","text":"<p>Return DataFrame with renamed columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def rename(self, mapper: dict[str, str]) -&gt; Self:\n    \"\"\"Return DataFrame with renamed columns.\"\"\"\n    # Validate all old column names exist\n    for old_name in mapper:\n        if old_name not in self.columns:\n            raise KeyError(f\"Column '{old_name}' not found in DataFrame\")\n\n    # Create new column list\n    new_cols = [mapper.get(col, col) for col in self.columns]\n\n    # Check for duplicates\n    if len(new_cols) != len(set(new_cols)):\n        raise ValueError(\"Renaming would create duplicate column names\")\n\n    return self.__class__(columns=new_cols, data=self.data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.rename_columns","title":"<code>rename_columns(mapper)</code>","text":"<p>Return DataFrame with renamed columns (alias for rename).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def rename_columns(self, mapper: dict[str, str]) -&gt; Self:\n    \"\"\"Return DataFrame with renamed columns (alias for rename).\"\"\"\n    return self.rename(mapper)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.validate_structure","title":"<code>validate_structure()</code>","text":"<p>Validate DataFrame structure.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def validate_structure(self) -&gt; None:\n    \"\"\"Validate DataFrame structure.\"\"\"\n    # Check for empty column names\n    for i, col in enumerate(self.columns):\n        if col == \"\":\n            raise ValueError(f\"Column at index {i} is empty\")\n\n    # Check for duplicate column names\n    if len(self.columns) != len(set(self.columns)):\n        duplicates = [col for col in self.columns if self.columns.count(col) &gt; 1]\n        raise ValueError(f\"Duplicate column names found: {set(duplicates)}\")\n\n    # Check all rows have same length as columns\n    num_cols = len(self.columns)\n    for i, row in enumerate(self.data):\n        if len(row) != num_cols:\n            raise ValueError(f\"Row {i} has {len(row)} values, expected {num_cols}\")\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.infer_types","title":"<code>infer_types()</code>","text":"<p>Infer column data types.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def infer_types(self) -&gt; dict[str, str]:\n    \"\"\"Infer column data types.\"\"\"\n    result: dict[str, str] = {}\n\n    for col_idx, col_name in enumerate(self.columns):\n        # Extract all values for this column\n        values = [row[col_idx] for row in self.data]\n\n        # Filter out None values for type checking\n        non_null_values = [v for v in values if v is not None]\n\n        if not non_null_values:\n            result[col_name] = \"null\"\n            continue\n\n        # Check types\n        types_found = set()\n        for val in non_null_values:\n            if isinstance(val, bool):\n                types_found.add(\"bool\")\n            elif isinstance(val, int):\n                types_found.add(\"int\")\n            elif isinstance(val, float):\n                types_found.add(\"float\")\n            elif isinstance(val, str):\n                types_found.add(\"str\")\n            else:\n                types_found.add(\"other\")\n\n        # Determine final type\n        if len(types_found) &gt; 1:\n            # Special case: int and float can be treated as float\n            if types_found == {\"int\", \"float\"}:\n                result[col_name] = \"float\"\n            else:\n                result[col_name] = \"mixed\"\n        elif \"bool\" in types_found:\n            result[col_name] = \"bool\"\n        elif \"int\" in types_found:\n            result[col_name] = \"int\"\n        elif \"float\" in types_found:\n            result[col_name] = \"float\"\n        elif \"str\" in types_found:\n            result[col_name] = \"str\"\n        else:\n            result[col_name] = \"mixed\"\n\n    return result\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.has_nulls","title":"<code>has_nulls()</code>","text":"<p>Check for null values in each column.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def has_nulls(self) -&gt; dict[str, bool]:\n    \"\"\"Check for null values in each column.\"\"\"\n    result: dict[str, bool] = {}\n\n    for col_idx, col_name in enumerate(self.columns):\n        # Check if any value in this column is None\n        has_null = any(row[col_idx] is None for row in self.data)\n        result[col_name] = has_null\n\n    return result\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.__len__","title":"<code>__len__()</code>","text":"<p>Return number of rows.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of rows.\"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over rows as dictionaries.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def __iter__(self) -&gt; Any:\n    \"\"\"Iterate over rows as dictionaries.\"\"\"\n    for row in self.data:\n        yield dict(zip(self.columns, row))\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.from_json","title":"<code>from_json(json_string)</code>  <code>classmethod</code>","text":"<p>Create DataFrame from JSON string (array of objects).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_json(cls, json_string: str) -&gt; Self:\n    \"\"\"Create DataFrame from JSON string (array of objects).\"\"\"\n    records = json.loads(json_string)\n    if not isinstance(records, list):\n        raise ValueError(\"JSON must be an array of objects\")\n    return cls.from_records(records)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.to_json","title":"<code>to_json(orient='records')</code>","text":"<p>Export DataFrame as JSON string.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def to_json(self, orient: Literal[\"records\", \"columns\"] = \"records\") -&gt; str:\n    \"\"\"Export DataFrame as JSON string.\"\"\"\n    # Map \"columns\" to \"list\" for to_dict()\n    dict_orient: Literal[\"dict\", \"list\", \"records\"] = \"list\" if orient == \"columns\" else orient\n    return json.dumps(self.to_dict(orient=dict_orient))\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.get_column","title":"<code>get_column(column)</code>","text":"<p>Get all values for a column.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def get_column(self, column: str) -&gt; list[Any]:\n    \"\"\"Get all values for a column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n    idx = self.columns.index(column)\n    return [row[idx] for row in self.data]\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Support df['col'] and df[['col1', 'col2']].</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def __getitem__(self, key: str | list[str]) -&gt; list[Any] | Self:\n    \"\"\"Support df['col'] and df[['col1', 'col2']].\"\"\"\n    if isinstance(key, str):\n        return self.get_column(key)\n    return self.select(key)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.unique","title":"<code>unique(column)</code>","text":"<p>Get unique values from a column (preserves order).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def unique(self, column: str) -&gt; list[Any]:\n    \"\"\"Get unique values from a column (preserves order).\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    seen = set()\n    result = []\n    for row in self.data:\n        val = row[col_idx]\n        if val not in seen:\n            seen.add(val)\n            result.append(val)\n    return result\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.value_counts","title":"<code>value_counts(column)</code>","text":"<p>Count occurrences of each unique value in column.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def value_counts(self, column: str) -&gt; dict[Any, int]:\n    \"\"\"Count occurrences of each unique value in column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    counts: dict[Any, int] = {}\n    for row in self.data:\n        val = row[col_idx]\n        counts[val] = counts.get(val, 0) + 1\n    return counts\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.sort","title":"<code>sort(by, ascending=True)</code>","text":"<p>Sort DataFrame by column.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def sort(self, by: str, ascending: bool = True) -&gt; Self:\n    \"\"\"Sort DataFrame by column.\"\"\"\n    if by not in self.columns:\n        raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n    col_idx = self.columns.index(by)\n\n    # Sort with None values at the end\n    def sort_key(row: list[Any]) -&gt; tuple[int, Any]:\n        val = row[col_idx]\n        if val is None:\n            # Use a tuple to ensure None sorts last\n            return (1, None) if ascending else (0, None)\n        return (0, val) if ascending else (1, val)\n\n    sorted_data = sorted(self.data, key=sort_key, reverse=not ascending)\n    return self.__class__(columns=self.columns, data=sorted_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.filter","title":"<code>filter(predicate)</code>","text":"<p>Filter rows using a predicate function.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def filter(self, predicate: Any) -&gt; Self:\n    \"\"\"Filter rows using a predicate function.\"\"\"\n    filtered_data = []\n    for row in self.data:\n        row_dict = dict(zip(self.columns, row))\n        if predicate(row_dict):\n            filtered_data.append(row)\n    return self.__class__(columns=self.columns, data=filtered_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.apply","title":"<code>apply(func, column)</code>","text":"<p>Apply function to column values.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def apply(self, func: Any, column: str) -&gt; Self:\n    \"\"\"Apply function to column values.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    new_data = []\n    for row in self.data:\n        new_row = row.copy()\n        new_row[col_idx] = func(row[col_idx])\n        new_data.append(new_row)\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.add_column","title":"<code>add_column(name, values)</code>","text":"<p>Add new column to DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def add_column(self, name: str, values: list[Any]) -&gt; Self:\n    \"\"\"Add new column to DataFrame.\"\"\"\n    if name in self.columns:\n        raise ValueError(f\"Column '{name}' already exists\")\n\n    if len(values) != len(self.data):\n        raise ValueError(f\"Values length ({len(values)}) must match row count ({len(self.data)})\")\n\n    new_columns = self.columns + [name]\n    new_data = [row + [values[i]] for i, row in enumerate(self.data)]\n\n    return self.__class__(columns=new_columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.drop_rows","title":"<code>drop_rows(indices)</code>","text":"<p>Drop rows by index.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def drop_rows(self, indices: list[int]) -&gt; Self:\n    \"\"\"Drop rows by index.\"\"\"\n    indices_set = set(indices)\n    new_data = [row for i, row in enumerate(self.data) if i not in indices_set]\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.drop_duplicates","title":"<code>drop_duplicates(subset=None)</code>","text":"<p>Remove duplicate rows.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def drop_duplicates(self, subset: list[str] | None = None) -&gt; Self:\n    \"\"\"Remove duplicate rows.\"\"\"\n    # Validate subset columns\n    if subset is not None:\n        for col in subset:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n        col_indices = [self.columns.index(col) for col in subset]\n    else:\n        col_indices = list(range(len(self.columns)))\n\n    # Track seen values\n    seen = set()\n    new_data = []\n\n    for row in self.data:\n        # Create tuple of relevant column values\n        key = tuple(row[i] for i in col_indices)\n\n        if key not in seen:\n            seen.add(key)\n            new_data.append(row)\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.fillna","title":"<code>fillna(value)</code>","text":"<p>Replace None values.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def fillna(self, value: Any | dict[str, Any]) -&gt; Self:\n    \"\"\"Replace None values.\"\"\"\n    if isinstance(value, dict):\n        # Validate column names\n        for col in value:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Create mapping of column index to fill value\n        fill_map = {self.columns.index(col): val for col, val in value.items()}\n\n        # Fill values\n        new_data = []\n        for row in self.data:\n            new_row = [fill_map[i] if i in fill_map and val is None else val for i, val in enumerate(row)]\n            new_data.append(new_row)\n    else:\n        # Single fill value for all None\n        new_data = [[value if val is None else val for val in row] for row in self.data]\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.concat","title":"<code>concat(other)</code>","text":"<p>Concatenate DataFrames vertically (stack rows).</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def concat(self, other: Self) -&gt; Self:\n    \"\"\"Concatenate DataFrames vertically (stack rows).\"\"\"\n    if self.columns != other.columns:\n        raise ValueError(f\"Column mismatch: {self.columns} != {other.columns}\")\n\n    combined_data = self.data + other.data\n    return self.__class__(columns=self.columns, data=combined_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.melt","title":"<code>melt(id_vars=None, value_vars=None, var_name='variable', value_name='value')</code>","text":"<p>Unpivot DataFrame from wide to long format.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def melt(\n    self,\n    id_vars: list[str] | None = None,\n    value_vars: list[str] | None = None,\n    var_name: str = \"variable\",\n    value_name: str = \"value\",\n) -&gt; Self:\n    \"\"\"Unpivot DataFrame from wide to long format.\"\"\"\n    # Handle empty DataFrame\n    if not self.columns or not self.data:\n        return self.__class__(columns=[var_name, value_name], data=[])\n\n    # Default id_vars to empty list if not specified\n    if id_vars is None:\n        id_vars = []\n\n    # Validate id_vars exist\n    for col in id_vars:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Default value_vars to all non-id columns\n    if value_vars is None:\n        value_vars = [col for col in self.columns if col not in id_vars]\n    else:\n        # Validate value_vars exist\n        for col in value_vars:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # If no value_vars to melt, return empty result\n    if not value_vars:\n        # Return just id columns if all columns are id_vars\n        if id_vars:\n            return self.select(id_vars)\n        return self.__class__(columns=[var_name, value_name], data=[])\n\n    # Check for column name conflicts\n    new_columns = id_vars + [var_name, value_name]\n    if len(new_columns) != len(set(new_columns)):\n        raise ValueError(\n            f\"Duplicate column names in result: {new_columns}. \"\n            f\"Choose different var_name or value_name to avoid conflicts.\"\n        )\n\n    # Get indices for id and value columns\n    id_indices = [self.columns.index(col) for col in id_vars]\n    value_indices = [(self.columns.index(col), col) for col in value_vars]\n\n    # Build melted data\n    melted_data: list[list[Any]] = []\n\n    for row in self.data:\n        # Extract id values for this row\n        id_values = [row[idx] for idx in id_indices]\n\n        # Create one new row for each value_var\n        for val_idx, var_col_name in value_indices:\n            new_row = id_values + [var_col_name, row[val_idx]]\n            melted_data.append(new_row)\n\n    return self.__class__(columns=new_columns, data=melted_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.pivot","title":"<code>pivot(index, columns, values)</code>","text":"<p>Pivot DataFrame from long to wide format.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def pivot(self, index: str, columns: str, values: str) -&gt; Self:\n    \"\"\"Pivot DataFrame from long to wide format.\"\"\"\n    # Validate columns exist\n    for col_name, param in [(index, \"index\"), (columns, \"columns\"), (values, \"values\")]:\n        if col_name not in self.columns:\n            raise KeyError(f\"Column '{col_name}' not found in DataFrame (parameter: {param})\")\n\n    # Get column indices\n    index_idx = self.columns.index(index)\n    columns_idx = self.columns.index(columns)\n    values_idx = self.columns.index(values)\n\n    # Build pivot structure: dict[index_value, dict[column_value, value]]\n    pivot_dict: dict[Any, dict[Any, Any]] = {}\n    column_values_set: set[Any] = set()\n\n    for row in self.data:\n        idx_val = row[index_idx]\n        col_val = row[columns_idx]\n        val = row[values_idx]\n\n        # Track column values for final column list\n        column_values_set.add(col_val)\n\n        # Initialize nested dict if needed\n        if idx_val not in pivot_dict:\n            pivot_dict[idx_val] = {}\n\n        # Check for duplicates\n        if col_val in pivot_dict[idx_val]:\n            raise ValueError(\n                f\"Duplicate entries found for index='{idx_val}' and columns='{col_val}'. \"\n                f\"Cannot reshape with duplicate index/column combinations. \"\n                f\"Consider using aggregation or removing duplicates first.\"\n            )\n\n        pivot_dict[idx_val] = {**pivot_dict[idx_val], col_val: val}\n\n    # Sort column values for consistent ordering\n    column_values = sorted(column_values_set, key=lambda x: (x is None, x))\n\n    # Build result columns: [index_column, col1, col2, ...]\n    result_columns = [index] + column_values\n\n    # Build result data\n    result_data: list[list[Any]] = []\n    for idx_val in sorted(pivot_dict.keys(), key=lambda x: (x is None, x)):\n        row_dict = pivot_dict[idx_val]\n        # Build row: [index_value, value_for_col1, value_for_col2, ...]\n        row = [idx_val] + [row_dict.get(col_val, None) for col_val in column_values]\n        result_data.append(row)\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.merge","title":"<code>merge(other, on=None, how='inner', left_on=None, right_on=None, suffixes=('_x', '_y'))</code>","text":"<p>Merge DataFrames using database-style join.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def merge(\n    self,\n    other: Self,\n    on: str | list[str] | None = None,\n    how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\",\n    left_on: str | list[str] | None = None,\n    right_on: str | list[str] | None = None,\n    suffixes: tuple[str, str] = (\"_x\", \"_y\"),\n) -&gt; Self:\n    \"\"\"Merge DataFrames using database-style join.\"\"\"\n    # Determine join keys\n    if on is not None:\n        if left_on is not None or right_on is not None:\n            raise ValueError(\"Cannot specify both 'on' and 'left_on'/'right_on'\")\n        left_keys = [on] if isinstance(on, str) else on\n        right_keys = left_keys\n    elif left_on is not None and right_on is not None:\n        left_keys = [left_on] if isinstance(left_on, str) else left_on\n        right_keys = [right_on] if isinstance(right_on, str) else right_on\n        if len(left_keys) != len(right_keys):\n            raise ValueError(\"left_on and right_on must have same length\")\n    else:\n        raise ValueError(\"Must specify either 'on' or both 'left_on' and 'right_on'\")\n\n    # Validate join keys exist\n    for key in left_keys:\n        if key not in self.columns:\n            raise KeyError(f\"Join key '{key}' not found in left DataFrame\")\n    for key in right_keys:\n        if key not in other.columns:\n            raise KeyError(f\"Join key '{key}' not found in right DataFrame\")\n\n    # Get indices for join keys\n    left_key_indices = [self.columns.index(k) for k in left_keys]\n    right_key_indices = [other.columns.index(k) for k in right_keys]\n\n    # Build lookup dict for right DataFrame: key_tuple -&gt; list[row_indices]\n    right_lookup: dict[tuple[Any, ...], list[int]] = {}\n    for row_idx, row in enumerate(other.data):\n        key_tuple = tuple(row[idx] for idx in right_key_indices)\n        if key_tuple not in right_lookup:\n            right_lookup[key_tuple] = []\n        right_lookup[key_tuple].append(row_idx)\n\n    # Determine result columns\n    left_suffix, right_suffix = suffixes\n\n    # Start with left DataFrame columns\n    result_columns = self.columns.copy()\n\n    # Add right DataFrame columns (excluding join keys if using 'on')\n    for col in other.columns:\n        if on is not None and col in left_keys:\n            # Skip join key columns from right when using 'on'\n            continue\n\n        if col in result_columns:\n            # Handle collision with suffix\n            result_columns.append(f\"{col}{right_suffix}\")\n            # Also need to rename left column\n            left_col_idx = result_columns.index(col)\n            result_columns[left_col_idx] = f\"{col}{left_suffix}\"\n        else:\n            result_columns.append(col)\n\n    # Get indices of right columns to include\n    right_col_indices = []\n    for col in other.columns:\n        if on is not None and col in right_keys:\n            continue\n        right_col_indices.append(other.columns.index(col))\n\n    # Perform join\n    result_data: list[list[Any]] = []\n    matched_right_indices: set[int] = set()\n\n    for left_row in self.data:\n        # Extract key from left row\n        left_key_tuple = tuple(left_row[idx] for idx in left_key_indices)\n\n        # Find matching rows in right DataFrame\n        right_matches = right_lookup.get(left_key_tuple, [])\n\n        if right_matches:\n            # Join matched rows\n            for right_idx in right_matches:\n                matched_right_indices.add(right_idx)\n                right_row = other.data[right_idx]\n\n                # Build result row: left columns + right columns (excluding join keys)\n                result_row = left_row.copy()\n                for col_idx in right_col_indices:\n                    result_row.append(right_row[col_idx])\n\n                result_data.append(result_row)\n        else:\n            # No match\n            if how in (\"left\", \"outer\"):\n                # Include left row with None for right columns\n                result_row = left_row.copy()\n                result_row.extend([None] * len(right_col_indices))\n                result_data.append(result_row)\n\n    # Handle right/outer joins - add unmatched right rows\n    if how in (\"right\", \"outer\"):\n        for right_idx, right_row in enumerate(other.data):\n            if right_idx not in matched_right_indices:\n                # Build row with None for left columns\n                result_row = [None] * len(self.columns)\n\n                # Fill in join key values if using 'on'\n                if on is not None:\n                    for left_idx, right_idx_key in zip(left_key_indices, right_key_indices):\n                        result_row[left_idx] = right_row[right_idx_key]\n\n                # Add right columns\n                for col_idx in right_col_indices:\n                    result_row.append(right_row[col_idx])\n\n                result_data.append(result_row)\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.transpose","title":"<code>transpose()</code>","text":"<p>Transpose DataFrame by swapping rows and columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def transpose(self) -&gt; Self:\n    \"\"\"Transpose DataFrame by swapping rows and columns.\"\"\"\n    if not self.data:\n        # Empty DataFrame - return with swapped structure\n        return self.__class__(columns=[], data=[])\n\n    # First column becomes the new column names\n    # Remaining columns become data rows\n    if not self.columns:\n        return self.__class__(columns=[], data=[])\n\n    # Extract first column values as new column names\n    # Convert to strings to ensure valid column names\n    new_columns = [str(row[0]) for row in self.data]\n\n    # Transpose the remaining columns\n    num_original_cols = len(self.columns)\n    if num_original_cols == 1:\n        # Only one column (the index) - result is just column names as rows\n        single_col_data = [[col] for col in self.columns]\n        return self.__class__(columns=new_columns if new_columns else [\"0\"], data=single_col_data)\n\n    # Build transposed data\n    # Each original column (except first) becomes a row\n    # Each original row becomes a column\n    result_data: list[list[Any]] = []\n\n    for col_idx in range(1, num_original_cols):\n        # Original column name becomes first value in new row\n        row = [self.columns[col_idx]]\n        # Add values from each original row for this column\n        for orig_row in self.data:\n            row.append(orig_row[col_idx])\n        result_data.append(row)\n\n    # New columns: first is placeholder for original column names, rest are from first column\n    result_columns = [\"index\"] + new_columns\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.describe","title":"<code>describe()</code>","text":"<p>Generate statistical summary for numeric columns.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def describe(self) -&gt; Self:\n    \"\"\"Generate statistical summary for numeric columns.\"\"\"\n    import statistics\n\n    stats_rows: list[list[Any]] = []\n    stat_names = [\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n\n    for col_idx in range(len(self.columns)):\n        # Extract numeric values (filter None and non-numeric)\n        values = []\n        for row in self.data:\n            val = row[col_idx]\n            if val is not None and isinstance(val, (int, float)) and not isinstance(val, bool):\n                values.append(float(val))\n\n        if not values:\n            # Non-numeric column - fill with None\n            stats_rows.append([None] * len(stat_names))\n            continue\n\n        # Calculate statistics\n        count = len(values)\n        mean = statistics.mean(values)\n        std = statistics.stdev(values) if count &gt; 1 else 0.0\n        min_val = min(values)\n        max_val = max(values)\n\n        # Quantiles\n        sorted_vals = sorted(values)\n        try:\n            q25 = statistics.quantiles(sorted_vals, n=4)[0] if count &gt; 1 else sorted_vals[0]\n            q50 = statistics.median(sorted_vals)\n            q75 = statistics.quantiles(sorted_vals, n=4)[2] if count &gt; 1 else sorted_vals[0]\n        except statistics.StatisticsError:\n            q25 = q50 = q75 = sorted_vals[0] if sorted_vals else 0.0\n\n        stats_rows.append([count, mean, std, min_val, q25, q50, q75, max_val])\n\n    # Transpose to make stats the rows and columns the columns\n    transposed_data = [\n        [stats_rows[col_idx][stat_idx] for col_idx in range(len(self.columns))]\n        for stat_idx in range(len(stat_names))\n    ]\n\n    return self.__class__(columns=self.columns, data=transposed_data).add_column(\"stat\", stat_names)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.groupby","title":"<code>groupby(by)</code>","text":"<p>Group DataFrame by column values.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def groupby(self, by: str) -&gt; \"GroupBy\":\n    \"\"\"Group DataFrame by column values.\"\"\"\n    if by not in self.columns:\n        raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n    return GroupBy(self, by)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.equals","title":"<code>equals(other)</code>","text":"<p>Check if two DataFrames are identical.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def equals(self, other: Any) -&gt; bool:\n    \"\"\"Check if two DataFrames are identical.\"\"\"\n    if not isinstance(other, DataFrame):\n        return False\n    return self.columns == other.columns and self.data == other.data\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.deepcopy","title":"<code>deepcopy()</code>","text":"<p>Create a deep copy of the DataFrame.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def deepcopy(self) -&gt; Self:\n    \"\"\"Create a deep copy of the DataFrame.\"\"\"\n    import copy\n\n    return self.__class__(columns=self.columns.copy(), data=copy.deepcopy(self.data))\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.isna","title":"<code>isna()</code>","text":"<p>Return DataFrame of booleans showing None locations.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def isna(self) -&gt; Self:\n    \"\"\"Return DataFrame of booleans showing None locations.\"\"\"\n    null_data = [[val is None for val in row] for row in self.data]\n    return self.__class__(columns=self.columns, data=null_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.notna","title":"<code>notna()</code>","text":"<p>Return DataFrame of booleans showing non-None locations.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def notna(self) -&gt; Self:\n    \"\"\"Return DataFrame of booleans showing non-None locations.\"\"\"\n    not_null_data = [[val is not None for val in row] for row in self.data]\n    return self.__class__(columns=self.columns, data=not_null_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.dropna","title":"<code>dropna(axis=0, how='any')</code>","text":"<p>Drop rows or columns with None values.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def dropna(self, axis: Literal[0, 1] = 0, how: Literal[\"any\", \"all\"] = \"any\") -&gt; Self:\n    \"\"\"Drop rows or columns with None values.\"\"\"\n    if axis == 0:\n        # Drop rows\n        if how == \"any\":\n            # Drop rows with any None\n            new_data = [row for row in self.data if not any(val is None for val in row)]\n        else:\n            # Drop rows with all None\n            new_data = [row for row in self.data if not all(val is None for val in row)]\n        return self.__class__(columns=self.columns, data=new_data)\n    else:\n        # Drop columns (axis=1)\n        cols_to_keep = []\n        indices_to_keep = []\n\n        for col_idx, col_name in enumerate(self.columns):\n            col_values = [row[col_idx] for row in self.data]\n\n            if how == \"any\":\n                # Keep column if no None values\n                if not any(val is None for val in col_values):\n                    cols_to_keep.append(col_name)\n                    indices_to_keep.append(col_idx)\n            else:\n                # Keep column if not all None\n                if not all(val is None for val in col_values):\n                    cols_to_keep.append(col_name)\n                    indices_to_keep.append(col_idx)\n\n        # Extract data for kept columns\n        new_data = [[row[i] for i in indices_to_keep] for row in self.data]\n        return self.__class__(columns=cols_to_keep, data=new_data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.DataFrame.nunique","title":"<code>nunique(column)</code>","text":"<p>Count number of unique values in column.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def nunique(self, column: str) -&gt; int:\n    \"\"\"Count number of unique values in column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    unique_values = set()\n    for row in self.data:\n        val = row[col_idx]\n        # Count None as a unique value\n        unique_values.add(val)\n    return len(unique_values)\n</code></pre>"},{"location":"api-reference/#groupby","title":"GroupBy","text":""},{"location":"api-reference/#servicekit.data.GroupBy","title":"<code>GroupBy</code>","text":"<p>GroupBy helper for aggregations.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>class GroupBy:\n    \"\"\"GroupBy helper for aggregations.\"\"\"\n\n    def __init__(self, dataframe: DataFrame, by: str):\n        \"\"\"Initialize GroupBy helper.\"\"\"\n        self.dataframe = dataframe\n        self.by = by\n        self.by_idx = dataframe.columns.index(by)\n\n        # Build groups\n        self.groups: dict[Any, list[list[Any]]] = {}\n        for row in dataframe.data:\n            key = row[self.by_idx]\n            if key not in self.groups:\n                self.groups[key] = []\n            self.groups[key].append(row)\n\n    def count(self) -&gt; DataFrame:\n        \"\"\"Count rows per group.\"\"\"\n        data = [[key, len(rows)] for key, rows in self.groups.items()]\n        return DataFrame(columns=[self.by, \"count\"], data=data)\n\n    def sum(self, column: str) -&gt; DataFrame:\n        \"\"\"Sum numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            total = sum(values) if values else None\n            data.append([key, total])\n\n        return DataFrame(columns=[self.by, f\"{column}_sum\"], data=data)\n\n    def mean(self, column: str) -&gt; DataFrame:\n        \"\"\"Calculate mean of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        import statistics\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            avg = statistics.mean(values) if values else None\n            data.append([key, avg])\n\n        return DataFrame(columns=[self.by, f\"{column}_mean\"], data=data)\n\n    def min(self, column: str) -&gt; DataFrame:\n        \"\"\"Find minimum of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            minimum = min(values) if values else None\n            data.append([key, minimum])\n\n        return DataFrame(columns=[self.by, f\"{column}_min\"], data=data)\n\n    def max(self, column: str) -&gt; DataFrame:\n        \"\"\"Find maximum of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            maximum = max(values) if values else None\n            data.append([key, maximum])\n\n        return DataFrame(columns=[self.by, f\"{column}_max\"], data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.data.GroupBy.__init__","title":"<code>__init__(dataframe, by)</code>","text":"<p>Initialize GroupBy helper.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def __init__(self, dataframe: DataFrame, by: str):\n    \"\"\"Initialize GroupBy helper.\"\"\"\n    self.dataframe = dataframe\n    self.by = by\n    self.by_idx = dataframe.columns.index(by)\n\n    # Build groups\n    self.groups: dict[Any, list[list[Any]]] = {}\n    for row in dataframe.data:\n        key = row[self.by_idx]\n        if key not in self.groups:\n            self.groups[key] = []\n        self.groups[key].append(row)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy.count","title":"<code>count()</code>","text":"<p>Count rows per group.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def count(self) -&gt; DataFrame:\n    \"\"\"Count rows per group.\"\"\"\n    data = [[key, len(rows)] for key, rows in self.groups.items()]\n    return DataFrame(columns=[self.by, \"count\"], data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy.sum","title":"<code>sum(column)</code>","text":"<p>Sum numeric column per group.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def sum(self, column: str) -&gt; DataFrame:\n    \"\"\"Sum numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        total = sum(values) if values else None\n        data.append([key, total])\n\n    return DataFrame(columns=[self.by, f\"{column}_sum\"], data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy.mean","title":"<code>mean(column)</code>","text":"<p>Calculate mean of numeric column per group.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def mean(self, column: str) -&gt; DataFrame:\n    \"\"\"Calculate mean of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    import statistics\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        avg = statistics.mean(values) if values else None\n        data.append([key, avg])\n\n    return DataFrame(columns=[self.by, f\"{column}_mean\"], data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy.min","title":"<code>min(column)</code>","text":"<p>Find minimum of numeric column per group.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def min(self, column: str) -&gt; DataFrame:\n    \"\"\"Find minimum of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        minimum = min(values) if values else None\n        data.append([key, minimum])\n\n    return DataFrame(columns=[self.by, f\"{column}_min\"], data=data)\n</code></pre>"},{"location":"api-reference/#servicekit.data.GroupBy.max","title":"<code>max(column)</code>","text":"<p>Find maximum of numeric column per group.</p> Source code in <code>src/servicekit/data/dataframe.py</code> <pre><code>def max(self, column: str) -&gt; DataFrame:\n    \"\"\"Find maximum of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        maximum = max(values) if values else None\n        data.append([key, maximum])\n\n    return DataFrame(columns=[self.by, f\"{column}_max\"], data=data)\n</code></pre>"},{"location":"api-reference/#fastapi-layer","title":"FastAPI Layer","text":"<p>FastAPI-specific components for building web services.</p>"},{"location":"api-reference/#service-builder","title":"Service Builder","text":"<p>Service builder class for composing FastAPI applications.</p>"},{"location":"api-reference/#baseservicebuilder","title":"BaseServiceBuilder","text":""},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder","title":"<code>BaseServiceBuilder</code>","text":"<p>Base service builder providing core FastAPI functionality without module dependencies.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>class BaseServiceBuilder:\n    \"\"\"Base service builder providing core FastAPI functionality without module dependencies.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        info: ServiceInfo,\n        database_url: str = \"sqlite+aiosqlite:///:memory:\",\n        include_error_handlers: bool = True,\n        include_logging: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize base service builder with core options.\"\"\"\n        if info.description is None and info.summary is not None:\n            # Preserve summary as description for FastAPI metadata if description missing\n            self.info = info.model_copy(update={\"description\": info.summary})\n        else:\n            self.info = info\n        self._title = self.info.display_name\n        self._app_description = self.info.summary or self.info.description or \"\"\n        self._version = self.info.version\n        self._database_url = database_url\n        self._database_instance: Database | None = None\n        self._pool_size: int = 5\n        self._max_overflow: int = 10\n        self._pool_recycle: int = 3600\n        self._pool_pre_ping: bool = True\n        self._include_error_handlers = include_error_handlers\n        self._include_logging = include_logging\n        self._health_options: _HealthOptions | None = None\n        self._system_options: _SystemOptions | None = None\n        self._job_options: _JobOptions | None = None\n        self._auth_options: _AuthOptions | None = None\n        self._monitoring_options: _MonitoringOptions | None = None\n        self._registration_options: _RegistrationOptions | None = None\n        self._app_configs: List[App] = []\n        self._custom_routers: List[APIRouter] = []\n        self._dependency_overrides: Dict[DependencyOverride, DependencyOverride] = {}\n        self._startup_hooks: List[LifecycleHook] = []\n        self._shutdown_hooks: List[LifecycleHook] = []\n\n    # --------------------------------------------------------------------- Fluent configuration\n\n    def with_database(\n        self,\n        url_or_instance: str | Database | None = None,\n        *,\n        pool_size: int = 5,\n        max_overflow: int = 10,\n        pool_recycle: int = 3600,\n        pool_pre_ping: bool = True,\n    ) -&gt; Self:\n        \"\"\"Configure database with URL string, Database instance, or default in-memory SQLite.\"\"\"\n        if isinstance(url_or_instance, Database):\n            # Pre-configured instance provided\n            self._database_instance = url_or_instance\n            return self  # Skip pool configuration for instances\n        elif isinstance(url_or_instance, str):\n            # String URL provided\n            self._database_url = url_or_instance\n        elif url_or_instance is None:\n            # Default: in-memory SQLite\n            self._database_url = \"sqlite+aiosqlite:///:memory:\"\n        else:\n            raise TypeError(\n                f\"Expected str, Database, or None, got {type(url_or_instance).__name__}. \"\n                \"Use .with_database() for default, .with_database('url') for custom URL, \"\n                \"or .with_database(db_instance) for pre-configured database.\"\n            )\n\n        # Configure pool settings (only applies to URL-based databases)\n        self._pool_size = pool_size\n        self._max_overflow = max_overflow\n        self._pool_recycle = pool_recycle\n        self._pool_pre_ping = pool_pre_ping\n        return self\n\n    def with_landing_page(self) -&gt; Self:\n        \"\"\"Enable landing page at root path.\"\"\"\n        return self.with_app((\"servicekit.api\", \"apps/landing\"))\n\n    def with_logging(self, enabled: bool = True) -&gt; Self:\n        \"\"\"Enable structured logging with request tracing.\"\"\"\n        self._include_logging = enabled\n        return self\n\n    def with_health(\n        self,\n        *,\n        prefix: str = \"/health\",\n        tags: List[str] | None = None,\n        checks: dict[str, HealthCheck] | None = None,\n        include_database_check: bool = True,\n    ) -&gt; Self:\n        \"\"\"Add health check endpoint with optional custom checks.\"\"\"\n        health_checks = checks or {}\n\n        if include_database_check:\n            health_checks[\"database\"] = self._create_database_health_check()\n\n        self._health_options = _HealthOptions(\n            prefix=prefix,\n            tags=list(tags) if tags is not None else [\"Observability\"],\n            checks=health_checks,\n        )\n        return self\n\n    def with_system(\n        self,\n        *,\n        prefix: str = \"/api/v1/system\",\n        tags: List[str] | None = None,\n    ) -&gt; Self:\n        \"\"\"Add system info endpoint.\"\"\"\n        self._system_options = _SystemOptions(\n            prefix=prefix,\n            tags=list(tags) if tags is not None else [\"Service\"],\n        )\n        return self\n\n    def with_jobs(\n        self,\n        *,\n        prefix: str = \"/api/v1/jobs\",\n        tags: List[str] | None = None,\n        max_concurrency: int | None = None,\n    ) -&gt; Self:\n        \"\"\"Add job scheduler endpoints.\"\"\"\n        self._job_options = _JobOptions(\n            prefix=prefix,\n            tags=list(tags) if tags is not None else [\"Jobs\"],\n            max_concurrency=max_concurrency,\n        )\n        return self\n\n    def with_auth(\n        self,\n        *,\n        api_keys: List[str] | None = None,\n        api_key_file: str | None = None,\n        env_var: str = \"SERVICEKIT_API_KEYS\",\n        header_name: str = \"X-API-Key\",\n        unauthenticated_paths: List[str] | None = None,\n    ) -&gt; Self:\n        \"\"\"Enable API key authentication.\"\"\"\n        keys: set[str] = set()\n        auth_source: str = \"\"  # Track source for later logging\n\n        # Priority 1: Direct list (examples/dev)\n        if api_keys is not None:\n            keys = set(api_keys)\n            auth_source = \"direct_keys\"\n\n        # Priority 2: File (Docker secrets)\n        elif api_key_file is not None:\n            keys = load_api_keys_from_file(api_key_file)\n            auth_source = f\"file:{api_key_file}\"\n\n        # Priority 3: Environment variable (default)\n        else:\n            keys = load_api_keys_from_env(env_var)\n            if keys:\n                auth_source = f\"env:{env_var}\"\n            else:\n                auth_source = f\"env:{env_var}:empty\"\n\n        if not keys:\n            raise ValueError(\"No API keys configured. Provide api_keys, api_key_file, or set environment variable.\")\n\n        # Default unauthenticated paths\n        default_unauth = {\"/docs\", \"/redoc\", \"/openapi.json\", \"/health\", \"/\"}\n        unauth_set = set(unauthenticated_paths) if unauthenticated_paths else default_unauth\n\n        self._auth_options = _AuthOptions(\n            api_keys=keys,\n            header_name=header_name,\n            unauthenticated_paths=unauth_set,\n            source=auth_source,\n        )\n        return self\n\n    def with_monitoring(\n        self,\n        *,\n        prefix: str = \"/metrics\",\n        tags: List[str] | None = None,\n        service_name: str | None = None,\n        enable_traces: bool = False,\n    ) -&gt; Self:\n        \"\"\"Enable OpenTelemetry monitoring with Prometheus endpoint and auto-instrumentation.\"\"\"\n        self._monitoring_options = _MonitoringOptions(\n            prefix=prefix,\n            tags=list(tags) if tags is not None else [\"Observability\"],\n            service_name=service_name,\n            enable_traces=enable_traces,\n        )\n        return self\n\n    def with_registration(\n        self,\n        *,\n        orchestrator_url: str | None = None,\n        host: str | None = None,\n        port: int | None = None,\n        orchestrator_url_env: str = \"SERVICEKIT_ORCHESTRATOR_URL\",\n        host_env: str = \"SERVICEKIT_HOST\",\n        port_env: str = \"SERVICEKIT_PORT\",\n        max_retries: int = 5,\n        retry_delay: float = 2.0,\n        fail_on_error: bool = False,\n        timeout: float = 10.0,\n        enable_keepalive: bool = True,\n        keepalive_interval: float = 10.0,\n        auto_deregister: bool = True,\n    ) -&gt; Self:\n        \"\"\"Enable service registration with orchestrator for service discovery.\"\"\"\n        self._registration_options = _RegistrationOptions(\n            orchestrator_url=orchestrator_url,\n            host=host,\n            port=port,\n            orchestrator_url_env=orchestrator_url_env,\n            host_env=host_env,\n            port_env=port_env,\n            max_retries=max_retries,\n            retry_delay=retry_delay,\n            fail_on_error=fail_on_error,\n            timeout=timeout,\n            enable_keepalive=enable_keepalive,\n            keepalive_interval=keepalive_interval,\n            auto_deregister=auto_deregister,\n        )\n        return self\n\n    def with_app(self, path: str | Path | tuple[str, str], prefix: str | None = None) -&gt; Self:\n        \"\"\"Register static app from filesystem path or package resource tuple.\"\"\"\n        app = AppLoader.load(path, prefix=prefix)\n        self._app_configs.append(app)\n        return self\n\n    def with_apps(self, path: str | Path | tuple[str, str]) -&gt; Self:\n        \"\"\"Auto-discover and register all apps in directory.\"\"\"\n        apps = AppLoader.discover(path)\n        self._app_configs.extend(apps)\n        return self\n\n    def include_router(self, router: APIRouter) -&gt; Self:\n        \"\"\"Include a custom router.\"\"\"\n        self._custom_routers.append(router)\n        return self\n\n    def override_dependency(self, dependency: DependencyOverride, override: DependencyOverride) -&gt; Self:\n        \"\"\"Override a dependency for testing or customization.\"\"\"\n        self._dependency_overrides[dependency] = override\n        return self\n\n    def on_startup(self, hook: LifecycleHook) -&gt; Self:\n        \"\"\"Register a startup hook.\"\"\"\n        self._startup_hooks.append(hook)\n        return self\n\n    def on_shutdown(self, hook: LifecycleHook) -&gt; Self:\n        \"\"\"Register a shutdown hook.\"\"\"\n        self._shutdown_hooks.append(hook)\n        return self\n\n    # --------------------------------------------------------------------- Build mechanics\n\n    def build(self) -&gt; FastAPI:\n        \"\"\"Build and configure the FastAPI application.\"\"\"\n        self._validate_configuration()\n        self._validate_module_configuration()  # Extension point for subclasses\n\n        lifespan = self._build_lifespan()\n        app = FastAPI(\n            title=self._title,\n            description=self._app_description,\n            version=self._version,\n            lifespan=lifespan,\n        )\n        app.state.database_url = self._database_url\n\n        # Override schema generation to clean up generic type names\n        app.openapi = self._create_openapi_customizer(app)  # type: ignore[method-assign]\n\n        if self._include_error_handlers:\n            add_error_handlers(app)\n\n        if self._include_logging:\n            add_logging_middleware(app)\n\n        if self._auth_options:\n            app.add_middleware(\n                APIKeyMiddleware,\n                api_keys=self._auth_options.api_keys,\n                header_name=self._auth_options.header_name,\n                unauthenticated_paths=self._auth_options.unauthenticated_paths,\n            )\n            # Store auth_source for logging during startup\n            app.state.auth_source = self._auth_options.source\n            app.state.auth_key_count = len(self._auth_options.api_keys)\n\n        if self._health_options:\n            health_router = HealthRouter.create(\n                prefix=self._health_options.prefix,\n                tags=self._health_options.tags,\n                checks=self._health_options.checks,\n            )\n            app.include_router(health_router)\n\n        if self._system_options:\n            system_router = SystemRouter.create(\n                prefix=self._system_options.prefix,\n                tags=self._system_options.tags,\n            )\n            app.include_router(system_router)\n\n        if self._job_options:\n            job_router = JobRouter.create(\n                prefix=self._job_options.prefix,\n                tags=self._job_options.tags,\n                scheduler_factory=get_scheduler,\n            )\n            app.include_router(job_router)\n\n        if self._monitoring_options:\n            from .monitoring import setup_monitoring\n\n            metric_reader = setup_monitoring(\n                app,\n                service_name=self._monitoring_options.service_name,\n                enable_traces=self._monitoring_options.enable_traces,\n            )\n            metrics_router = MetricsRouter.create(\n                prefix=self._monitoring_options.prefix,\n                tags=self._monitoring_options.tags,\n                metric_reader=metric_reader,\n            )\n            app.include_router(metrics_router)\n\n        # Extension point for module-specific routers\n        self._register_module_routers(app)\n\n        for router in self._custom_routers:\n            app.include_router(router)\n\n        # Install route endpoints BEFORE mounting apps (routes take precedence over mounts)\n        self._install_info_endpoint(app, info=self.info)\n\n        # Mount apps AFTER all routes (apps act as catch-all for unmatched paths)\n        if self._app_configs:\n            from fastapi.staticfiles import StaticFiles\n\n            # Collect all router prefixes to exclude from redirect middleware\n            # This ensures routes take precedence over app mounts\n            router_prefixes = set()\n            if self._health_options:\n                router_prefixes.add(self._health_options.prefix)\n            if self._system_options:\n                router_prefixes.add(self._system_options.prefix)\n            if self._job_options:\n                router_prefixes.add(self._job_options.prefix)\n            if self._monitoring_options:\n                router_prefixes.add(self._monitoring_options.prefix)\n            for router in self._custom_routers:\n                if hasattr(router, \"prefix\") and router.prefix:\n                    router_prefixes.add(router.prefix)\n\n            # Add middleware to handle trailing slash redirects for app prefixes\n            # Skip prefixes that are already claimed by routes (routes take precedence)\n            from .middleware import AppPrefixRedirectMiddleware\n\n            app_prefixes = [\n                cfg.prefix for cfg in self._app_configs if cfg.prefix != \"/\" and cfg.prefix not in router_prefixes\n            ]\n            if app_prefixes:\n                app.add_middleware(AppPrefixRedirectMiddleware, app_prefixes=app_prefixes)\n\n            # Mount all apps\n            for app_config in self._app_configs:\n                static_files = StaticFiles(directory=str(app_config.directory), html=True)\n                app.mount(app_config.prefix, static_files, name=f\"app_{app_config.manifest.name}\")\n                logger.info(\n                    \"app.mounted\",\n                    name=app_config.manifest.name,\n                    prefix=app_config.prefix,\n                    directory=str(app_config.directory),\n                    is_package=app_config.is_package,\n                )\n\n        # Initialize app manager for metadata queries (always, even if no apps)\n        from .app import AppManager\n        from .dependencies import set_app_manager\n\n        app_manager = AppManager(self._app_configs)\n        set_app_manager(app_manager)\n\n        for dependency, override in self._dependency_overrides.items():\n            app.dependency_overrides[dependency] = override\n\n        return app\n\n    # --------------------------------------------------------------------- Extension points\n\n    def _validate_module_configuration(self) -&gt; None:\n        \"\"\"Extension point for module-specific validation (override in subclasses).\"\"\"\n        pass\n\n    def _register_module_routers(self, app: FastAPI) -&gt; None:\n        \"\"\"Extension point for registering module-specific routers (override in subclasses).\"\"\"\n        pass\n\n    # --------------------------------------------------------------------- Core helpers\n\n    def _validate_configuration(self) -&gt; None:\n        \"\"\"Validate core configuration.\"\"\"\n        # Validate health check names don't contain invalid characters\n        if self._health_options:\n            for name in self._health_options.checks.keys():\n                if not name.replace(\"_\", \"\").replace(\"-\", \"\").isalnum():\n                    raise ValueError(\n                        f\"Health check name '{name}' contains invalid characters. \"\n                        \"Only alphanumeric characters, underscores, and hyphens are allowed.\"\n                    )\n\n        # Validate app configurations\n        if self._app_configs:\n            # Deduplicate apps with same prefix (last one wins)\n            # This allows overriding apps, especially useful for root prefix \"/\"\n            seen_prefixes: dict[str, int] = {}  # prefix -&gt; last index\n            for i, app in enumerate(self._app_configs):\n                if app.prefix in seen_prefixes:\n                    # Log warning about override\n                    prev_idx = seen_prefixes[app.prefix]\n                    prev_app = self._app_configs[prev_idx]\n                    logger.warning(\n                        \"app.prefix.override\",\n                        prefix=app.prefix,\n                        replaced_app=prev_app.manifest.name,\n                        new_app=app.manifest.name,\n                    )\n                seen_prefixes[app.prefix] = i\n\n            # Keep only the last app for each prefix\n            self._app_configs = [self._app_configs[i] for i in sorted(set(seen_prefixes.values()))]\n\n            # Sort so root mounts are last (most specific paths mounted first)\n            # This ensures FastAPI matches more specific routes before catch-all root\n            # Sorting: (is_root, -path_length, path) ensures longer paths before shorter, root last\n            self._app_configs.sort(key=lambda app: (app.prefix == \"/\", -len(app.prefix), app.prefix))\n\n            # Validate that non-root prefixes don't have duplicates (shouldn't happen after dedup, but safety check)\n            prefixes = [app.prefix for app in self._app_configs]\n            if len(prefixes) != len(set(prefixes)):\n                raise ValueError(\"Internal error: duplicate prefixes after deduplication\")\n\n    def _build_lifespan(self) -&gt; LifespanFactory:\n        \"\"\"Build lifespan context manager for app startup/shutdown.\"\"\"\n        database_url = self._database_url\n        database_instance = self._database_instance\n        pool_size = self._pool_size\n        max_overflow = self._max_overflow\n        pool_recycle = self._pool_recycle\n        pool_pre_ping = self._pool_pre_ping\n        job_options = self._job_options\n        include_logging = self._include_logging\n        registration_options = self._registration_options\n        info = self.info\n        startup_hooks = list(self._startup_hooks)\n        shutdown_hooks = list(self._shutdown_hooks)\n\n        @asynccontextmanager\n        async def lifespan(app: FastAPI) -&gt; AsyncIterator[None]:\n            # Configure logging if enabled\n            if include_logging:\n                configure_logging()\n\n            # Use injected database or create new one from URL\n            if database_instance is not None:\n                database = database_instance\n                should_manage_lifecycle = False\n            else:\n                # Create appropriate database type based on URL\n                if \"sqlite\" in database_url.lower():\n                    database = SqliteDatabase(\n                        database_url,\n                        pool_size=pool_size,\n                        max_overflow=max_overflow,\n                        pool_recycle=pool_recycle,\n                        pool_pre_ping=pool_pre_ping,\n                    )\n                else:\n                    database = Database(\n                        database_url,\n                        pool_size=pool_size,\n                        max_overflow=max_overflow,\n                        pool_recycle=pool_recycle,\n                        pool_pre_ping=pool_pre_ping,\n                    )\n                should_manage_lifecycle = True\n\n            # Always initialize database (safe to call multiple times)\n            await database.init()\n\n            set_database(database)\n            app.state.database = database\n\n            # Initialize scheduler if jobs are enabled\n            if job_options is not None:\n                from servicekit.scheduler import AIOJobScheduler\n\n                scheduler = AIOJobScheduler(max_concurrency=job_options.max_concurrency)\n                set_scheduler(scheduler)\n                app.state.scheduler = scheduler\n\n            # Log auth configuration after logging is configured\n            if hasattr(app.state, \"auth_source\"):\n                auth_source = app.state.auth_source\n                key_count = app.state.auth_key_count\n\n                if auth_source == \"direct_keys\":\n                    logger.warning(\n                        \"auth.direct_keys\",\n                        message=\"Using direct API keys - not recommended for production\",\n                        count=key_count,\n                    )\n                elif auth_source.startswith(\"file:\"):\n                    file_path = auth_source.split(\":\", 1)[1]\n                    logger.info(\"auth.loaded_from_file\", file=file_path, count=key_count)\n                elif auth_source.startswith(\"env:\"):\n                    parts = auth_source.split(\":\", 2)\n                    env_var = parts[1]\n                    if len(parts) &gt; 2 and parts[2] == \"empty\":\n                        logger.warning(\n                            \"auth.no_keys\",\n                            message=f\"No API keys found in {env_var}. Service will reject all requests.\",\n                        )\n                    else:\n                        logger.info(\"auth.loaded_from_env\", env_var=env_var, count=key_count)\n\n            for hook in startup_hooks:\n                await hook(app)\n\n            # Register with orchestrator if enabled\n            registration_info = None\n            if registration_options is not None:\n                from .registration import register_service, start_keepalive\n\n                registration_info = await register_service(\n                    orchestrator_url=registration_options.orchestrator_url,\n                    host=registration_options.host,\n                    port=registration_options.port,\n                    info=info,\n                    orchestrator_url_env=registration_options.orchestrator_url_env,\n                    host_env=registration_options.host_env,\n                    port_env=registration_options.port_env,\n                    max_retries=registration_options.max_retries,\n                    retry_delay=registration_options.retry_delay,\n                    fail_on_error=registration_options.fail_on_error,\n                    timeout=registration_options.timeout,\n                )\n\n                # Start keepalive if registration succeeded and enabled\n                if registration_info and registration_options.enable_keepalive:\n                    ping_url = registration_info.get(\"ping_url\")\n                    if ping_url:\n                        await start_keepalive(\n                            ping_url=ping_url,\n                            interval=registration_options.keepalive_interval,\n                            timeout=registration_options.timeout,\n                        )\n\n            try:\n                yield\n            finally:\n                # Stop keepalive and deregister service if enabled\n                if registration_options is not None and registration_info:\n                    from .registration import deregister_service, stop_keepalive\n\n                    # Stop keepalive task\n                    if registration_options.enable_keepalive:\n                        await stop_keepalive()\n\n                    # Deregister from orchestrator\n                    if registration_options.auto_deregister:\n                        service_id = registration_info.get(\"service_id\")\n                        orchestrator_url = registration_info.get(\"orchestrator_url\")\n                        if service_id and orchestrator_url:\n                            await deregister_service(\n                                service_id=service_id,\n                                orchestrator_url=orchestrator_url,\n                                timeout=registration_options.timeout,\n                            )\n\n                for hook in shutdown_hooks:\n                    await hook(app)\n                app.state.database = None\n\n                # Dispose database only if we created it\n                if should_manage_lifecycle:\n                    await database.dispose()\n\n        return lifespan\n\n    @staticmethod\n    def _create_database_health_check() -&gt; HealthCheck:\n        \"\"\"Create database connectivity health check.\"\"\"\n\n        async def check_database() -&gt; tuple[HealthState, str | None]:\n            try:\n                db = get_database()\n                async with db.session() as session:\n                    # Simple connectivity check - execute a trivial query\n                    await session.execute(text(\"SELECT 1\"))\n                    return (HealthState.HEALTHY, None)\n            except Exception as e:\n                return (HealthState.UNHEALTHY, f\"Database connection failed: {str(e)}\")\n\n        return check_database\n\n    @staticmethod\n    def _create_openapi_customizer(app: FastAPI) -&gt; Callable[[], dict[str, Any]]:\n        \"\"\"Create OpenAPI schema customizer that cleans up generic type names.\"\"\"\n\n        def custom_openapi() -&gt; dict[str, Any]:\n            if app.openapi_schema:\n                return app.openapi_schema\n\n            from fastapi.openapi.utils import get_openapi\n\n            openapi_schema = get_openapi(\n                title=app.title,\n                version=app.version,\n                description=app.description,\n                routes=app.routes,\n            )\n\n            # Clean up schema titles by removing generic type parameters\n            if \"components\" in openapi_schema and \"schemas\" in openapi_schema[\"components\"]:\n                schemas = openapi_schema[\"components\"][\"schemas\"]\n                cleaned_schemas: dict[str, Any] = {}\n\n                for schema_name, schema_def in schemas.items():\n                    # Remove generic type parameters from schema names\n                    clean_name = re.sub(r\"\\[.*?\\]\", \"\", schema_name)\n                    # If title exists in schema, clean it too\n                    if isinstance(schema_def, dict) and \"title\" in schema_def:\n                        schema_def[\"title\"] = re.sub(r\"\\[.*?\\]\", \"\", schema_def[\"title\"])\n                    cleaned_schemas[clean_name] = schema_def\n\n                openapi_schema[\"components\"][\"schemas\"] = cleaned_schemas\n\n                # Update all $ref pointers to use cleaned names\n                def clean_refs(obj: Any) -&gt; Any:\n                    if isinstance(obj, dict):\n                        if \"$ref\" in obj:\n                            obj[\"$ref\"] = re.sub(r\"\\[.*?\\]\", \"\", obj[\"$ref\"])\n                        for value in obj.values():\n                            clean_refs(value)\n                    elif isinstance(obj, list):\n                        for item in obj:\n                            clean_refs(item)\n\n                clean_refs(openapi_schema)\n\n            app.openapi_schema = openapi_schema\n            return app.openapi_schema\n\n        return custom_openapi\n\n    @staticmethod\n    def _install_info_endpoint(app: FastAPI, *, info: ServiceInfo) -&gt; None:\n        \"\"\"Install service info endpoint.\"\"\"\n        info_type = type(info)\n\n        @app.get(\"/api/v1/info\", tags=[\"Service\"], include_in_schema=True, response_model=info_type)\n        async def get_info() -&gt; ServiceInfo:\n            return info\n\n    # --------------------------------------------------------------------- Convenience\n\n    @classmethod\n    def create(cls, *, info: ServiceInfo, **kwargs: Any) -&gt; FastAPI:\n        \"\"\"Create and build a FastAPI application in one call.\"\"\"\n        return cls(info=info, **kwargs).build()\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.__init__","title":"<code>__init__(*, info, database_url='sqlite+aiosqlite:///:memory:', include_error_handlers=True, include_logging=False)</code>","text":"<p>Initialize base service builder with core options.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def __init__(\n    self,\n    *,\n    info: ServiceInfo,\n    database_url: str = \"sqlite+aiosqlite:///:memory:\",\n    include_error_handlers: bool = True,\n    include_logging: bool = False,\n) -&gt; None:\n    \"\"\"Initialize base service builder with core options.\"\"\"\n    if info.description is None and info.summary is not None:\n        # Preserve summary as description for FastAPI metadata if description missing\n        self.info = info.model_copy(update={\"description\": info.summary})\n    else:\n        self.info = info\n    self._title = self.info.display_name\n    self._app_description = self.info.summary or self.info.description or \"\"\n    self._version = self.info.version\n    self._database_url = database_url\n    self._database_instance: Database | None = None\n    self._pool_size: int = 5\n    self._max_overflow: int = 10\n    self._pool_recycle: int = 3600\n    self._pool_pre_ping: bool = True\n    self._include_error_handlers = include_error_handlers\n    self._include_logging = include_logging\n    self._health_options: _HealthOptions | None = None\n    self._system_options: _SystemOptions | None = None\n    self._job_options: _JobOptions | None = None\n    self._auth_options: _AuthOptions | None = None\n    self._monitoring_options: _MonitoringOptions | None = None\n    self._registration_options: _RegistrationOptions | None = None\n    self._app_configs: List[App] = []\n    self._custom_routers: List[APIRouter] = []\n    self._dependency_overrides: Dict[DependencyOverride, DependencyOverride] = {}\n    self._startup_hooks: List[LifecycleHook] = []\n    self._shutdown_hooks: List[LifecycleHook] = []\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_database","title":"<code>with_database(url_or_instance=None, *, pool_size=5, max_overflow=10, pool_recycle=3600, pool_pre_ping=True)</code>","text":"<p>Configure database with URL string, Database instance, or default in-memory SQLite.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_database(\n    self,\n    url_or_instance: str | Database | None = None,\n    *,\n    pool_size: int = 5,\n    max_overflow: int = 10,\n    pool_recycle: int = 3600,\n    pool_pre_ping: bool = True,\n) -&gt; Self:\n    \"\"\"Configure database with URL string, Database instance, or default in-memory SQLite.\"\"\"\n    if isinstance(url_or_instance, Database):\n        # Pre-configured instance provided\n        self._database_instance = url_or_instance\n        return self  # Skip pool configuration for instances\n    elif isinstance(url_or_instance, str):\n        # String URL provided\n        self._database_url = url_or_instance\n    elif url_or_instance is None:\n        # Default: in-memory SQLite\n        self._database_url = \"sqlite+aiosqlite:///:memory:\"\n    else:\n        raise TypeError(\n            f\"Expected str, Database, or None, got {type(url_or_instance).__name__}. \"\n            \"Use .with_database() for default, .with_database('url') for custom URL, \"\n            \"or .with_database(db_instance) for pre-configured database.\"\n        )\n\n    # Configure pool settings (only applies to URL-based databases)\n    self._pool_size = pool_size\n    self._max_overflow = max_overflow\n    self._pool_recycle = pool_recycle\n    self._pool_pre_ping = pool_pre_ping\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_landing_page","title":"<code>with_landing_page()</code>","text":"<p>Enable landing page at root path.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_landing_page(self) -&gt; Self:\n    \"\"\"Enable landing page at root path.\"\"\"\n    return self.with_app((\"servicekit.api\", \"apps/landing\"))\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_logging","title":"<code>with_logging(enabled=True)</code>","text":"<p>Enable structured logging with request tracing.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_logging(self, enabled: bool = True) -&gt; Self:\n    \"\"\"Enable structured logging with request tracing.\"\"\"\n    self._include_logging = enabled\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_health","title":"<code>with_health(*, prefix='/health', tags=None, checks=None, include_database_check=True)</code>","text":"<p>Add health check endpoint with optional custom checks.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_health(\n    self,\n    *,\n    prefix: str = \"/health\",\n    tags: List[str] | None = None,\n    checks: dict[str, HealthCheck] | None = None,\n    include_database_check: bool = True,\n) -&gt; Self:\n    \"\"\"Add health check endpoint with optional custom checks.\"\"\"\n    health_checks = checks or {}\n\n    if include_database_check:\n        health_checks[\"database\"] = self._create_database_health_check()\n\n    self._health_options = _HealthOptions(\n        prefix=prefix,\n        tags=list(tags) if tags is not None else [\"Observability\"],\n        checks=health_checks,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_system","title":"<code>with_system(*, prefix='/api/v1/system', tags=None)</code>","text":"<p>Add system info endpoint.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_system(\n    self,\n    *,\n    prefix: str = \"/api/v1/system\",\n    tags: List[str] | None = None,\n) -&gt; Self:\n    \"\"\"Add system info endpoint.\"\"\"\n    self._system_options = _SystemOptions(\n        prefix=prefix,\n        tags=list(tags) if tags is not None else [\"Service\"],\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_jobs","title":"<code>with_jobs(*, prefix='/api/v1/jobs', tags=None, max_concurrency=None)</code>","text":"<p>Add job scheduler endpoints.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_jobs(\n    self,\n    *,\n    prefix: str = \"/api/v1/jobs\",\n    tags: List[str] | None = None,\n    max_concurrency: int | None = None,\n) -&gt; Self:\n    \"\"\"Add job scheduler endpoints.\"\"\"\n    self._job_options = _JobOptions(\n        prefix=prefix,\n        tags=list(tags) if tags is not None else [\"Jobs\"],\n        max_concurrency=max_concurrency,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_auth","title":"<code>with_auth(*, api_keys=None, api_key_file=None, env_var='SERVICEKIT_API_KEYS', header_name='X-API-Key', unauthenticated_paths=None)</code>","text":"<p>Enable API key authentication.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_auth(\n    self,\n    *,\n    api_keys: List[str] | None = None,\n    api_key_file: str | None = None,\n    env_var: str = \"SERVICEKIT_API_KEYS\",\n    header_name: str = \"X-API-Key\",\n    unauthenticated_paths: List[str] | None = None,\n) -&gt; Self:\n    \"\"\"Enable API key authentication.\"\"\"\n    keys: set[str] = set()\n    auth_source: str = \"\"  # Track source for later logging\n\n    # Priority 1: Direct list (examples/dev)\n    if api_keys is not None:\n        keys = set(api_keys)\n        auth_source = \"direct_keys\"\n\n    # Priority 2: File (Docker secrets)\n    elif api_key_file is not None:\n        keys = load_api_keys_from_file(api_key_file)\n        auth_source = f\"file:{api_key_file}\"\n\n    # Priority 3: Environment variable (default)\n    else:\n        keys = load_api_keys_from_env(env_var)\n        if keys:\n            auth_source = f\"env:{env_var}\"\n        else:\n            auth_source = f\"env:{env_var}:empty\"\n\n    if not keys:\n        raise ValueError(\"No API keys configured. Provide api_keys, api_key_file, or set environment variable.\")\n\n    # Default unauthenticated paths\n    default_unauth = {\"/docs\", \"/redoc\", \"/openapi.json\", \"/health\", \"/\"}\n    unauth_set = set(unauthenticated_paths) if unauthenticated_paths else default_unauth\n\n    self._auth_options = _AuthOptions(\n        api_keys=keys,\n        header_name=header_name,\n        unauthenticated_paths=unauth_set,\n        source=auth_source,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_monitoring","title":"<code>with_monitoring(*, prefix='/metrics', tags=None, service_name=None, enable_traces=False)</code>","text":"<p>Enable OpenTelemetry monitoring with Prometheus endpoint and auto-instrumentation.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_monitoring(\n    self,\n    *,\n    prefix: str = \"/metrics\",\n    tags: List[str] | None = None,\n    service_name: str | None = None,\n    enable_traces: bool = False,\n) -&gt; Self:\n    \"\"\"Enable OpenTelemetry monitoring with Prometheus endpoint and auto-instrumentation.\"\"\"\n    self._monitoring_options = _MonitoringOptions(\n        prefix=prefix,\n        tags=list(tags) if tags is not None else [\"Observability\"],\n        service_name=service_name,\n        enable_traces=enable_traces,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_registration","title":"<code>with_registration(*, orchestrator_url=None, host=None, port=None, orchestrator_url_env='SERVICEKIT_ORCHESTRATOR_URL', host_env='SERVICEKIT_HOST', port_env='SERVICEKIT_PORT', max_retries=5, retry_delay=2.0, fail_on_error=False, timeout=10.0, enable_keepalive=True, keepalive_interval=10.0, auto_deregister=True)</code>","text":"<p>Enable service registration with orchestrator for service discovery.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_registration(\n    self,\n    *,\n    orchestrator_url: str | None = None,\n    host: str | None = None,\n    port: int | None = None,\n    orchestrator_url_env: str = \"SERVICEKIT_ORCHESTRATOR_URL\",\n    host_env: str = \"SERVICEKIT_HOST\",\n    port_env: str = \"SERVICEKIT_PORT\",\n    max_retries: int = 5,\n    retry_delay: float = 2.0,\n    fail_on_error: bool = False,\n    timeout: float = 10.0,\n    enable_keepalive: bool = True,\n    keepalive_interval: float = 10.0,\n    auto_deregister: bool = True,\n) -&gt; Self:\n    \"\"\"Enable service registration with orchestrator for service discovery.\"\"\"\n    self._registration_options = _RegistrationOptions(\n        orchestrator_url=orchestrator_url,\n        host=host,\n        port=port,\n        orchestrator_url_env=orchestrator_url_env,\n        host_env=host_env,\n        port_env=port_env,\n        max_retries=max_retries,\n        retry_delay=retry_delay,\n        fail_on_error=fail_on_error,\n        timeout=timeout,\n        enable_keepalive=enable_keepalive,\n        keepalive_interval=keepalive_interval,\n        auto_deregister=auto_deregister,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_app","title":"<code>with_app(path, prefix=None)</code>","text":"<p>Register static app from filesystem path or package resource tuple.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_app(self, path: str | Path | tuple[str, str], prefix: str | None = None) -&gt; Self:\n    \"\"\"Register static app from filesystem path or package resource tuple.\"\"\"\n    app = AppLoader.load(path, prefix=prefix)\n    self._app_configs.append(app)\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.with_apps","title":"<code>with_apps(path)</code>","text":"<p>Auto-discover and register all apps in directory.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def with_apps(self, path: str | Path | tuple[str, str]) -&gt; Self:\n    \"\"\"Auto-discover and register all apps in directory.\"\"\"\n    apps = AppLoader.discover(path)\n    self._app_configs.extend(apps)\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.include_router","title":"<code>include_router(router)</code>","text":"<p>Include a custom router.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def include_router(self, router: APIRouter) -&gt; Self:\n    \"\"\"Include a custom router.\"\"\"\n    self._custom_routers.append(router)\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.override_dependency","title":"<code>override_dependency(dependency, override)</code>","text":"<p>Override a dependency for testing or customization.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def override_dependency(self, dependency: DependencyOverride, override: DependencyOverride) -&gt; Self:\n    \"\"\"Override a dependency for testing or customization.\"\"\"\n    self._dependency_overrides[dependency] = override\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.on_startup","title":"<code>on_startup(hook)</code>","text":"<p>Register a startup hook.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def on_startup(self, hook: LifecycleHook) -&gt; Self:\n    \"\"\"Register a startup hook.\"\"\"\n    self._startup_hooks.append(hook)\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.on_shutdown","title":"<code>on_shutdown(hook)</code>","text":"<p>Register a shutdown hook.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def on_shutdown(self, hook: LifecycleHook) -&gt; Self:\n    \"\"\"Register a shutdown hook.\"\"\"\n    self._shutdown_hooks.append(hook)\n    return self\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.build","title":"<code>build()</code>","text":"<p>Build and configure the FastAPI application.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>def build(self) -&gt; FastAPI:\n    \"\"\"Build and configure the FastAPI application.\"\"\"\n    self._validate_configuration()\n    self._validate_module_configuration()  # Extension point for subclasses\n\n    lifespan = self._build_lifespan()\n    app = FastAPI(\n        title=self._title,\n        description=self._app_description,\n        version=self._version,\n        lifespan=lifespan,\n    )\n    app.state.database_url = self._database_url\n\n    # Override schema generation to clean up generic type names\n    app.openapi = self._create_openapi_customizer(app)  # type: ignore[method-assign]\n\n    if self._include_error_handlers:\n        add_error_handlers(app)\n\n    if self._include_logging:\n        add_logging_middleware(app)\n\n    if self._auth_options:\n        app.add_middleware(\n            APIKeyMiddleware,\n            api_keys=self._auth_options.api_keys,\n            header_name=self._auth_options.header_name,\n            unauthenticated_paths=self._auth_options.unauthenticated_paths,\n        )\n        # Store auth_source for logging during startup\n        app.state.auth_source = self._auth_options.source\n        app.state.auth_key_count = len(self._auth_options.api_keys)\n\n    if self._health_options:\n        health_router = HealthRouter.create(\n            prefix=self._health_options.prefix,\n            tags=self._health_options.tags,\n            checks=self._health_options.checks,\n        )\n        app.include_router(health_router)\n\n    if self._system_options:\n        system_router = SystemRouter.create(\n            prefix=self._system_options.prefix,\n            tags=self._system_options.tags,\n        )\n        app.include_router(system_router)\n\n    if self._job_options:\n        job_router = JobRouter.create(\n            prefix=self._job_options.prefix,\n            tags=self._job_options.tags,\n            scheduler_factory=get_scheduler,\n        )\n        app.include_router(job_router)\n\n    if self._monitoring_options:\n        from .monitoring import setup_monitoring\n\n        metric_reader = setup_monitoring(\n            app,\n            service_name=self._monitoring_options.service_name,\n            enable_traces=self._monitoring_options.enable_traces,\n        )\n        metrics_router = MetricsRouter.create(\n            prefix=self._monitoring_options.prefix,\n            tags=self._monitoring_options.tags,\n            metric_reader=metric_reader,\n        )\n        app.include_router(metrics_router)\n\n    # Extension point for module-specific routers\n    self._register_module_routers(app)\n\n    for router in self._custom_routers:\n        app.include_router(router)\n\n    # Install route endpoints BEFORE mounting apps (routes take precedence over mounts)\n    self._install_info_endpoint(app, info=self.info)\n\n    # Mount apps AFTER all routes (apps act as catch-all for unmatched paths)\n    if self._app_configs:\n        from fastapi.staticfiles import StaticFiles\n\n        # Collect all router prefixes to exclude from redirect middleware\n        # This ensures routes take precedence over app mounts\n        router_prefixes = set()\n        if self._health_options:\n            router_prefixes.add(self._health_options.prefix)\n        if self._system_options:\n            router_prefixes.add(self._system_options.prefix)\n        if self._job_options:\n            router_prefixes.add(self._job_options.prefix)\n        if self._monitoring_options:\n            router_prefixes.add(self._monitoring_options.prefix)\n        for router in self._custom_routers:\n            if hasattr(router, \"prefix\") and router.prefix:\n                router_prefixes.add(router.prefix)\n\n        # Add middleware to handle trailing slash redirects for app prefixes\n        # Skip prefixes that are already claimed by routes (routes take precedence)\n        from .middleware import AppPrefixRedirectMiddleware\n\n        app_prefixes = [\n            cfg.prefix for cfg in self._app_configs if cfg.prefix != \"/\" and cfg.prefix not in router_prefixes\n        ]\n        if app_prefixes:\n            app.add_middleware(AppPrefixRedirectMiddleware, app_prefixes=app_prefixes)\n\n        # Mount all apps\n        for app_config in self._app_configs:\n            static_files = StaticFiles(directory=str(app_config.directory), html=True)\n            app.mount(app_config.prefix, static_files, name=f\"app_{app_config.manifest.name}\")\n            logger.info(\n                \"app.mounted\",\n                name=app_config.manifest.name,\n                prefix=app_config.prefix,\n                directory=str(app_config.directory),\n                is_package=app_config.is_package,\n            )\n\n    # Initialize app manager for metadata queries (always, even if no apps)\n    from .app import AppManager\n    from .dependencies import set_app_manager\n\n    app_manager = AppManager(self._app_configs)\n    set_app_manager(app_manager)\n\n    for dependency, override in self._dependency_overrides.items():\n        app.dependency_overrides[dependency] = override\n\n    return app\n</code></pre>"},{"location":"api-reference/#servicekit.api.service_builder.BaseServiceBuilder.create","title":"<code>create(*, info, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create and build a FastAPI application in one call.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>@classmethod\ndef create(cls, *, info: ServiceInfo, **kwargs: Any) -&gt; FastAPI:\n    \"\"\"Create and build a FastAPI application in one call.\"\"\"\n    return cls(info=info, **kwargs).build()\n</code></pre>"},{"location":"api-reference/#serviceinfo","title":"ServiceInfo","text":""},{"location":"api-reference/#servicekit.api.service_builder.ServiceInfo","title":"<code>ServiceInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Service metadata for FastAPI application.</p> Source code in <code>src/servicekit/api/service_builder.py</code> <pre><code>class ServiceInfo(BaseModel):\n    \"\"\"Service metadata for FastAPI application.\"\"\"\n\n    display_name: str\n    version: str = \"1.0.0\"\n    summary: str | None = None\n    description: str | None = None\n    contact: dict[str, str] | None = None\n    license_info: dict[str, str] | None = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api-reference/#routers","title":"Routers","text":"<p>Base router classes and generic routers.</p>"},{"location":"api-reference/#router","title":"Router","text":""},{"location":"api-reference/#servicekit.api.router.Router","title":"<code>Router</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for FastAPI routers.</p> Source code in <code>src/servicekit/api/router.py</code> <pre><code>class Router(ABC):\n    \"\"\"Base class for FastAPI routers.\"\"\"\n\n    default_response_model_exclude_none: bool = False\n\n    def __init__(self, prefix: str, tags: Sequence[str], **kwargs: Any) -&gt; None:\n        \"\"\"Initialize router with prefix and tags.\"\"\"\n        self.router = APIRouter(prefix=prefix, tags=list(tags), **kwargs)\n        self._register_routes()\n\n    @classmethod\n    def create(cls, prefix: str, tags: Sequence[str], **kwargs: Any) -&gt; APIRouter:\n        \"\"\"Create a router instance and return the FastAPI router.\"\"\"\n        return cls(prefix=prefix, tags=tags, **kwargs).router\n\n    @abstractmethod\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register routes for this router.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#servicekit.api.router.Router-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.router.Router.__init__","title":"<code>__init__(prefix, tags, **kwargs)</code>","text":"<p>Initialize router with prefix and tags.</p> Source code in <code>src/servicekit/api/router.py</code> <pre><code>def __init__(self, prefix: str, tags: Sequence[str], **kwargs: Any) -&gt; None:\n    \"\"\"Initialize router with prefix and tags.\"\"\"\n    self.router = APIRouter(prefix=prefix, tags=list(tags), **kwargs)\n    self._register_routes()\n</code></pre>"},{"location":"api-reference/#servicekit.api.router.Router.create","title":"<code>create(prefix, tags, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a router instance and return the FastAPI router.</p> Source code in <code>src/servicekit/api/router.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str, tags: Sequence[str], **kwargs: Any) -&gt; APIRouter:\n    \"\"\"Create a router instance and return the FastAPI router.\"\"\"\n    return cls(prefix=prefix, tags=tags, **kwargs).router\n</code></pre>"},{"location":"api-reference/#crudrouter","title":"CrudRouter","text":""},{"location":"api-reference/#servicekit.api.crud.CrudRouter","title":"<code>CrudRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>Router base class for standard REST CRUD operations.</p> Source code in <code>src/servicekit/api/crud.py</code> <pre><code>class CrudRouter[InSchemaT: BaseModel, OutSchemaT: BaseModel](Router):\n    \"\"\"Router base class for standard REST CRUD operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        entity_in_type: type[InSchemaT],\n        entity_out_type: type[OutSchemaT],\n        manager_factory: ManagerFactory[InSchemaT, OutSchemaT],\n        *,\n        permissions: CrudPermissions | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize CRUD router with entity types and manager factory.\"\"\"\n        self.manager_factory = manager_factory\n        self.entity_in_type = entity_in_type\n        self.entity_out_type = entity_out_type\n        self._permissions = permissions or CrudPermissions()\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register CRUD routes based on permissions.\"\"\"\n        manager_dependency, manager_annotation = self._manager_dependency()\n        perms = self._permissions\n        if perms.create:\n            self._register_create_route(manager_dependency, manager_annotation)\n        if perms.read:\n            self._register_find_all_route(manager_dependency, manager_annotation)\n            self._register_find_by_id_route(manager_dependency, manager_annotation)\n            self._register_schema_route()\n            self._register_stats_route(manager_dependency, manager_annotation)\n        if perms.update:\n            self._register_update_route(manager_dependency, manager_annotation)\n        if perms.delete:\n            self._register_delete_route(manager_dependency, manager_annotation)\n\n    def register_entity_operation(\n        self,\n        name: str,\n        handler: Callable[..., Any],\n        *,\n        http_method: str = \"GET\",\n        response_model: type[Any] | None = None,\n        status_code: int | None = None,\n        summary: str | None = None,\n        description: str | None = None,\n    ) -&gt; None:\n        \"\"\"Register a custom entity operation with $ prefix.\n\n        Entity operations are automatically inserted before generic {entity_id} routes\n        to ensure proper route matching (e.g., /{entity_id}/$validate should match\n        before /{entity_id}).\n        \"\"\"\n        route = f\"/{{entity_id}}/${name}\"\n        route_kwargs: dict[str, Any] = {}\n\n        if response_model is not None:\n            route_kwargs[\"response_model\"] = response_model\n        if status_code is not None:\n            route_kwargs[\"status_code\"] = status_code\n        if summary is not None:\n            route_kwargs[\"summary\"] = summary\n        if description is not None:\n            route_kwargs[\"description\"] = description\n\n        # Register the route with the appropriate HTTP method\n        http_method_lower = http_method.lower()\n        if http_method_lower == \"get\":\n            self.router.get(route, **route_kwargs)(handler)\n        elif http_method_lower == \"post\":\n            self.router.post(route, **route_kwargs)(handler)\n        elif http_method_lower == \"put\":\n            self.router.put(route, **route_kwargs)(handler)\n        elif http_method_lower == \"patch\":\n            self.router.patch(route, **route_kwargs)(handler)\n        elif http_method_lower == \"delete\":\n            self.router.delete(route, **route_kwargs)(handler)\n        else:\n            raise ValueError(f\"Unsupported HTTP method: {http_method}\")\n\n        # Move the just-added route to before generic parametric routes\n        # Entity operations like /{entity_id}/$validate should match before /{entity_id}\n        if len(self.router.routes) &gt; 1:\n            new_route = self.router.routes.pop()\n            insert_index = self._find_generic_parametric_route_index()\n            self.router.routes.insert(insert_index, new_route)\n\n    def register_collection_operation(\n        self,\n        name: str,\n        handler: Callable[..., Any],\n        *,\n        http_method: str = \"GET\",\n        response_model: type[Any] | None = None,\n        status_code: int | None = None,\n        summary: str | None = None,\n        description: str | None = None,\n    ) -&gt; None:\n        \"\"\"Register a custom collection operation with $ prefix.\n\n        Collection operations are automatically inserted before parametric {entity_id} routes\n        to ensure proper route matching (e.g., /$stats should match before /{entity_id}).\n        \"\"\"\n        route = f\"/${name}\"\n        route_kwargs: dict[str, Any] = {}\n\n        if response_model is not None:\n            route_kwargs[\"response_model\"] = response_model\n        if status_code is not None:\n            route_kwargs[\"status_code\"] = status_code\n        if summary is not None:\n            route_kwargs[\"summary\"] = summary\n        if description is not None:\n            route_kwargs[\"description\"] = description\n\n        # Register the route with the appropriate HTTP method\n        http_method_lower = http_method.lower()\n        if http_method_lower == \"get\":\n            self.router.get(route, **route_kwargs)(handler)\n        elif http_method_lower == \"post\":\n            self.router.post(route, **route_kwargs)(handler)\n        elif http_method_lower == \"put\":\n            self.router.put(route, **route_kwargs)(handler)\n        elif http_method_lower == \"patch\":\n            self.router.patch(route, **route_kwargs)(handler)\n        elif http_method_lower == \"delete\":\n            self.router.delete(route, **route_kwargs)(handler)\n        else:\n            raise ValueError(f\"Unsupported HTTP method: {http_method}\")\n\n        # Move the just-added route to before parametric routes\n        # FastAPI appends to routes list, so the last route is the one we just added\n        if len(self.router.routes) &gt; 1:\n            new_route = self.router.routes.pop()  # Remove the route we just added\n            # Find the first parametric route and insert before it\n            insert_index = self._find_parametric_route_index()\n            self.router.routes.insert(insert_index, new_route)\n\n    # Route registration helpers --------------------------------------\n\n    def _register_create_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        entity_in_annotation: Any = self.entity_in_type\n        entity_out_annotation: Any = self.entity_out_type\n        router_prefix = self.router.prefix\n\n        @self.router.post(\"\", status_code=status.HTTP_201_CREATED, response_model=entity_out_annotation)\n        async def create(\n            entity_in: InSchemaT,\n            request: Request,\n            response: Response,\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; OutSchemaT:\n            from .utilities import build_location_url\n\n            created_entity = await manager.save(entity_in)\n            entity_id = getattr(created_entity, \"id\")\n            response.headers[\"Location\"] = build_location_url(request, f\"{router_prefix}/{entity_id}\")\n            return created_entity\n\n        self._annotate_manager(create, manager_annotation)\n        create.__annotations__[\"entity_in\"] = entity_in_annotation\n        create.__annotations__[\"return\"] = entity_out_annotation\n\n    def _register_find_all_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        entity_out_annotation: Any = self.entity_out_type\n        collection_response_model: Any = list[entity_out_annotation] | PaginatedResponse[entity_out_annotation]\n\n        @self.router.get(\"\", response_model=collection_response_model)\n        async def find_all(\n            page: int | None = None,\n            size: int | None = None,\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; list[OutSchemaT] | PaginatedResponse[OutSchemaT]:\n            from .pagination import create_paginated_response\n\n            # Pagination is opt-in: both page and size must be provided\n            if page is not None and size is not None:\n                items, total = await manager.find_paginated(page, size)\n                return create_paginated_response(items, total, page, size)\n            return await manager.find_all()\n\n        self._annotate_manager(find_all, manager_annotation)\n        find_all.__annotations__[\"return\"] = list[entity_out_annotation] | PaginatedResponse[entity_out_annotation]\n\n    def _register_find_by_id_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        entity_out_annotation: Any = self.entity_out_type\n        router_prefix = self.router.prefix\n\n        @self.router.get(\"/{entity_id}\", response_model=entity_out_annotation)\n        async def find_by_id(\n            entity_id: str,\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; OutSchemaT:\n            from servicekit.exceptions import NotFoundError\n\n            ulid_id = self._parse_ulid(entity_id)\n            entity = await manager.find_by_id(ulid_id)\n            if entity is None:\n                raise NotFoundError(\n                    f\"Entity with id {entity_id} not found\",\n                    instance=f\"{router_prefix}/{entity_id}\",\n                )\n            return entity\n\n        self._annotate_manager(find_by_id, manager_annotation)\n        find_by_id.__annotations__[\"return\"] = entity_out_annotation\n\n    def _register_update_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        entity_in_type = self.entity_in_type\n        entity_in_annotation: Any = entity_in_type\n        entity_out_annotation: Any = self.entity_out_type\n        router_prefix = self.router.prefix\n\n        @self.router.put(\"/{entity_id}\", response_model=entity_out_annotation)\n        async def update(\n            entity_id: str,\n            entity_in: InSchemaT,\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; OutSchemaT:\n            from servicekit.exceptions import NotFoundError\n\n            ulid_id = self._parse_ulid(entity_id)\n            if not await manager.exists_by_id(ulid_id):\n                raise NotFoundError(\n                    f\"Entity with id {entity_id} not found\",\n                    instance=f\"{router_prefix}/{entity_id}\",\n                )\n            entity_dict = entity_in.model_dump(exclude_unset=True)\n            entity_dict[\"id\"] = ulid_id\n            entity_with_id = entity_in_type.model_validate(entity_dict)\n            return await manager.save(entity_with_id)\n\n        self._annotate_manager(update, manager_annotation)\n        update.__annotations__[\"entity_in\"] = entity_in_annotation\n        update.__annotations__[\"return\"] = entity_out_annotation\n\n    def _register_delete_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        router_prefix = self.router.prefix\n\n        @self.router.delete(\"/{entity_id}\", status_code=status.HTTP_204_NO_CONTENT)\n        async def delete_by_id(\n            entity_id: str,\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; None:\n            from servicekit.exceptions import NotFoundError\n\n            ulid_id = self._parse_ulid(entity_id)\n            if not await manager.exists_by_id(ulid_id):\n                raise NotFoundError(\n                    f\"Entity with id {entity_id} not found\",\n                    instance=f\"{router_prefix}/{entity_id}\",\n                )\n            await manager.delete_by_id(ulid_id)\n\n        self._annotate_manager(delete_by_id, manager_annotation)\n\n    def _register_schema_route(self) -&gt; None:\n        \"\"\"Register JSON schema endpoint for the entity output type.\"\"\"\n        entity_out_type = self.entity_out_type\n\n        async def get_schema() -&gt; dict[str, Any]:\n            return entity_out_type.model_json_schema()\n\n        self.register_collection_operation(\n            name=\"schema\",\n            handler=get_schema,\n            http_method=\"GET\",\n            response_model=dict[str, Any],\n        )\n\n    def _register_stats_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        \"\"\"Register collection statistics endpoint.\"\"\"\n        from servicekit.schemas import CollectionStats\n\n        async def get_stats(\n            manager: Manager[InSchemaT, OutSchemaT, ULID] = manager_dependency,\n        ) -&gt; CollectionStats:\n            \"\"\"Get collection statistics.\"\"\"\n            return await manager.get_stats()\n\n        self._annotate_manager(get_stats, manager_annotation)\n\n        self.register_collection_operation(\n            name=\"stats\",\n            handler=get_stats,\n            http_method=\"GET\",\n            response_model=CollectionStats,\n            summary=\"Get collection statistics\",\n            description=\"Returns statistics about the collection including total entity count.\",\n        )\n\n    # Helper utilities -------------------------------------------------\n\n    def _manager_dependency(self) -&gt; tuple[Any, Any]:\n        manager_dependency = Depends(self.manager_factory)\n        manager_annotation: Any = Manager[Any, Any, ULID]\n        return manager_dependency, manager_annotation\n\n    def _annotate_manager(self, endpoint: Any, manager_annotation: Any) -&gt; None:\n        endpoint.__annotations__[\"manager\"] = manager_annotation\n\n    def _parse_ulid(self, entity_id: str) -&gt; ULID:\n        from servicekit.exceptions import InvalidULIDError\n\n        try:\n            return ULID.from_str(entity_id)\n        except ValueError as e:\n            raise InvalidULIDError(\n                f\"Invalid ULID format: {entity_id}\",\n                instance=f\"{self.router.prefix}/{entity_id}\",\n            ) from e\n\n    def _find_parametric_route_index(self) -&gt; int:\n        \"\"\"Find the index of the first parametric route containing {entity_id}.\n\n        Returns the index where collection operations should be inserted to ensure\n        they're matched before parametric routes.\n        \"\"\"\n        for i, route in enumerate(self.router.routes):\n            route_path = getattr(route, \"path\", \"\")\n            if \"{entity_id}\" in route_path:\n                return i\n        # If no parametric route found, append at the end\n        return len(self.router.routes)\n\n    def _find_generic_parametric_route_index(self) -&gt; int:\n        \"\"\"Find the index of the first generic parametric route (/{entity_id} without $).\n\n        Returns the index where entity operations should be inserted to ensure\n        they're matched before generic routes like GET/PUT/DELETE /{entity_id}.\n        \"\"\"\n        for i, route in enumerate(self.router.routes):\n            route_path = getattr(route, \"path\", \"\")\n            # Match routes like /{entity_id} but not /{entity_id}/$operation\n            if \"{entity_id}\" in route_path and \"/$\" not in route_path:\n                return i\n        # If no generic parametric route found, append at the end\n        return len(self.router.routes)\n</code></pre>"},{"location":"api-reference/#servicekit.api.crud.CrudRouter-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.crud.CrudRouter.__init__","title":"<code>__init__(prefix, tags, entity_in_type, entity_out_type, manager_factory, *, permissions=None, **kwargs)</code>","text":"<p>Initialize CRUD router with entity types and manager factory.</p> Source code in <code>src/servicekit/api/crud.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    entity_in_type: type[InSchemaT],\n    entity_out_type: type[OutSchemaT],\n    manager_factory: ManagerFactory[InSchemaT, OutSchemaT],\n    *,\n    permissions: CrudPermissions | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize CRUD router with entity types and manager factory.\"\"\"\n    self.manager_factory = manager_factory\n    self.entity_in_type = entity_in_type\n    self.entity_out_type = entity_out_type\n    self._permissions = permissions or CrudPermissions()\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#servicekit.api.crud.CrudRouter.register_entity_operation","title":"<code>register_entity_operation(name, handler, *, http_method='GET', response_model=None, status_code=None, summary=None, description=None)</code>","text":"<p>Register a custom entity operation with $ prefix.</p> <p>Entity operations are automatically inserted before generic {entity_id} routes to ensure proper route matching (e.g., /{entity_id}/$validate should match before /{entity_id}).</p> Source code in <code>src/servicekit/api/crud.py</code> <pre><code>def register_entity_operation(\n    self,\n    name: str,\n    handler: Callable[..., Any],\n    *,\n    http_method: str = \"GET\",\n    response_model: type[Any] | None = None,\n    status_code: int | None = None,\n    summary: str | None = None,\n    description: str | None = None,\n) -&gt; None:\n    \"\"\"Register a custom entity operation with $ prefix.\n\n    Entity operations are automatically inserted before generic {entity_id} routes\n    to ensure proper route matching (e.g., /{entity_id}/$validate should match\n    before /{entity_id}).\n    \"\"\"\n    route = f\"/{{entity_id}}/${name}\"\n    route_kwargs: dict[str, Any] = {}\n\n    if response_model is not None:\n        route_kwargs[\"response_model\"] = response_model\n    if status_code is not None:\n        route_kwargs[\"status_code\"] = status_code\n    if summary is not None:\n        route_kwargs[\"summary\"] = summary\n    if description is not None:\n        route_kwargs[\"description\"] = description\n\n    # Register the route with the appropriate HTTP method\n    http_method_lower = http_method.lower()\n    if http_method_lower == \"get\":\n        self.router.get(route, **route_kwargs)(handler)\n    elif http_method_lower == \"post\":\n        self.router.post(route, **route_kwargs)(handler)\n    elif http_method_lower == \"put\":\n        self.router.put(route, **route_kwargs)(handler)\n    elif http_method_lower == \"patch\":\n        self.router.patch(route, **route_kwargs)(handler)\n    elif http_method_lower == \"delete\":\n        self.router.delete(route, **route_kwargs)(handler)\n    else:\n        raise ValueError(f\"Unsupported HTTP method: {http_method}\")\n\n    # Move the just-added route to before generic parametric routes\n    # Entity operations like /{entity_id}/$validate should match before /{entity_id}\n    if len(self.router.routes) &gt; 1:\n        new_route = self.router.routes.pop()\n        insert_index = self._find_generic_parametric_route_index()\n        self.router.routes.insert(insert_index, new_route)\n</code></pre>"},{"location":"api-reference/#servicekit.api.crud.CrudRouter.register_collection_operation","title":"<code>register_collection_operation(name, handler, *, http_method='GET', response_model=None, status_code=None, summary=None, description=None)</code>","text":"<p>Register a custom collection operation with $ prefix.</p> <p>Collection operations are automatically inserted before parametric {entity_id} routes to ensure proper route matching (e.g., /$stats should match before /{entity_id}).</p> Source code in <code>src/servicekit/api/crud.py</code> <pre><code>def register_collection_operation(\n    self,\n    name: str,\n    handler: Callable[..., Any],\n    *,\n    http_method: str = \"GET\",\n    response_model: type[Any] | None = None,\n    status_code: int | None = None,\n    summary: str | None = None,\n    description: str | None = None,\n) -&gt; None:\n    \"\"\"Register a custom collection operation with $ prefix.\n\n    Collection operations are automatically inserted before parametric {entity_id} routes\n    to ensure proper route matching (e.g., /$stats should match before /{entity_id}).\n    \"\"\"\n    route = f\"/${name}\"\n    route_kwargs: dict[str, Any] = {}\n\n    if response_model is not None:\n        route_kwargs[\"response_model\"] = response_model\n    if status_code is not None:\n        route_kwargs[\"status_code\"] = status_code\n    if summary is not None:\n        route_kwargs[\"summary\"] = summary\n    if description is not None:\n        route_kwargs[\"description\"] = description\n\n    # Register the route with the appropriate HTTP method\n    http_method_lower = http_method.lower()\n    if http_method_lower == \"get\":\n        self.router.get(route, **route_kwargs)(handler)\n    elif http_method_lower == \"post\":\n        self.router.post(route, **route_kwargs)(handler)\n    elif http_method_lower == \"put\":\n        self.router.put(route, **route_kwargs)(handler)\n    elif http_method_lower == \"patch\":\n        self.router.patch(route, **route_kwargs)(handler)\n    elif http_method_lower == \"delete\":\n        self.router.delete(route, **route_kwargs)(handler)\n    else:\n        raise ValueError(f\"Unsupported HTTP method: {http_method}\")\n\n    # Move the just-added route to before parametric routes\n    # FastAPI appends to routes list, so the last route is the one we just added\n    if len(self.router.routes) &gt; 1:\n        new_route = self.router.routes.pop()  # Remove the route we just added\n        # Find the first parametric route and insert before it\n        insert_index = self._find_parametric_route_index()\n        self.router.routes.insert(insert_index, new_route)\n</code></pre>"},{"location":"api-reference/#crudpermissions","title":"CrudPermissions","text":""},{"location":"api-reference/#servicekit.api.crud.CrudPermissions","title":"<code>CrudPermissions</code>  <code>dataclass</code>","text":"<p>Permissions configuration for CRUD operations.</p> Source code in <code>src/servicekit/api/crud.py</code> <pre><code>@dataclass(slots=True)\nclass CrudPermissions:\n    \"\"\"Permissions configuration for CRUD operations.\"\"\"\n\n    create: bool = True\n    read: bool = True\n    update: bool = True\n    delete: bool = True\n</code></pre>"},{"location":"api-reference/#healthrouter","title":"HealthRouter","text":""},{"location":"api-reference/#servicekit.api.routers.health.HealthRouter","title":"<code>HealthRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>Health check router for service health monitoring.</p> Source code in <code>src/servicekit/api/routers/health.py</code> <pre><code>class HealthRouter(Router):\n    \"\"\"Health check router for service health monitoring.\"\"\"\n\n    default_response_model_exclude_none = True\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        checks: dict[str, HealthCheck] | None = None,\n        **kwargs: object,\n    ) -&gt; None:\n        \"\"\"Initialize health router with optional health checks.\"\"\"\n        self.checks = checks or {}\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register health check endpoint.\"\"\"\n        checks = self.checks\n\n        async def run_health_checks() -&gt; HealthStatus:\n            \"\"\"Run all health checks and aggregate results.\"\"\"\n            if not checks:\n                return HealthStatus(status=HealthState.HEALTHY)\n\n            check_results: dict[str, CheckResult] = {}\n            overall_state = HealthState.HEALTHY\n\n            for name, check_fn in checks.items():\n                try:\n                    state, message = await check_fn()\n                    check_results[name] = CheckResult(state=state, message=message)\n\n                    if state == HealthState.UNHEALTHY:\n                        overall_state = HealthState.UNHEALTHY\n                    elif state == HealthState.DEGRADED and overall_state == HealthState.HEALTHY:\n                        overall_state = HealthState.DEGRADED\n\n                except Exception as e:\n                    check_results[name] = CheckResult(state=HealthState.UNHEALTHY, message=f\"Check failed: {str(e)}\")\n                    overall_state = HealthState.UNHEALTHY\n\n            return HealthStatus(status=overall_state, checks=check_results)\n\n        @self.router.get(\n            \"\",\n            summary=\"Health check\",\n            response_model=HealthStatus,\n            response_model_exclude_none=self.default_response_model_exclude_none,\n        )\n        async def health_check() -&gt; HealthStatus:\n            return await run_health_checks()\n\n        @self.router.get(\n            \"/$stream\",\n            summary=\"Stream health status updates via SSE\",\n            description=\"Real-time Server-Sent Events stream of health status at regular intervals\",\n        )\n        async def stream_health_status(poll_interval: float = 1.0) -&gt; StreamingResponse:\n            \"\"\"Stream real-time health status updates using Server-Sent Events.\"\"\"\n\n            async def event_stream() -&gt; AsyncGenerator[bytes, None]:\n                while True:\n                    status = await run_health_checks()\n                    yield format_sse_model_event(status, exclude_none=self.default_response_model_exclude_none)\n                    await asyncio.sleep(poll_interval)\n\n            return StreamingResponse(\n                event_stream(),\n                media_type=\"text/event-stream\",\n                headers=SSE_HEADERS,\n            )\n</code></pre>"},{"location":"api-reference/#servicekit.api.routers.health.HealthRouter-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.routers.health.HealthRouter.__init__","title":"<code>__init__(prefix, tags, checks=None, **kwargs)</code>","text":"<p>Initialize health router with optional health checks.</p> Source code in <code>src/servicekit/api/routers/health.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    checks: dict[str, HealthCheck] | None = None,\n    **kwargs: object,\n) -&gt; None:\n    \"\"\"Initialize health router with optional health checks.\"\"\"\n    self.checks = checks or {}\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#jobrouter","title":"JobRouter","text":""},{"location":"api-reference/#servicekit.api.routers.job.JobRouter","title":"<code>JobRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>REST API router for job scheduler operations.</p> Source code in <code>src/servicekit/api/routers/job.py</code> <pre><code>class JobRouter(Router):\n    \"\"\"REST API router for job scheduler operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        scheduler_factory: Callable[[], JobScheduler],\n        **kwargs: object,\n    ) -&gt; None:\n        \"\"\"Initialize job router with scheduler factory.\"\"\"\n        self.scheduler_factory = scheduler_factory\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register job management endpoints.\"\"\"\n        scheduler_dependency = Depends(self.scheduler_factory)\n\n        @self.router.get(\"\", summary=\"List all jobs\", response_model=list[JobRecord])\n        async def get_jobs(\n            scheduler: JobScheduler = scheduler_dependency,\n            status_filter: JobStatus | None = None,\n        ) -&gt; list[JobRecord]:\n            jobs = await scheduler.get_all_records()\n            if status_filter:\n                return [job for job in jobs if job.status == status_filter]\n            return jobs\n\n        @self.router.get(\"/$schema\", summary=\"Get jobs list schema\", response_model=dict[str, Any])\n        async def get_jobs_schema() -&gt; dict[str, Any]:\n            \"\"\"Get JSON schema for jobs list response.\"\"\"\n            return TypeAdapter(list[JobRecord]).json_schema()\n\n        @self.router.get(\"/{job_id}\", summary=\"Get job by ID\", response_model=JobRecord)\n        async def get_job(\n            job_id: str,\n            scheduler: JobScheduler = scheduler_dependency,\n        ) -&gt; JobRecord:\n            try:\n                ulid_id = ULID.from_str(job_id)\n                return await scheduler.get_record(ulid_id)\n            except (ValueError, KeyError):\n                raise HTTPException(status_code=404, detail=\"Job not found\")\n\n        @self.router.delete(\"/{job_id}\", summary=\"Cancel and delete job\", status_code=status.HTTP_204_NO_CONTENT)\n        async def delete_job(\n            job_id: str,\n            scheduler: JobScheduler = scheduler_dependency,\n        ) -&gt; Response:\n            try:\n                ulid_id = ULID.from_str(job_id)\n                await scheduler.delete(ulid_id)\n                return Response(status_code=status.HTTP_204_NO_CONTENT)\n            except (ValueError, KeyError):\n                raise HTTPException(status_code=404, detail=\"Job not found\")\n\n        @self.router.get(\n            \"/{job_id}/$stream\",\n            summary=\"Stream job status updates via SSE\",\n            description=\"Real-time Server-Sent Events stream of job status changes until terminal state\",\n        )\n        async def stream_job_status(\n            job_id: str,\n            scheduler: JobScheduler = scheduler_dependency,\n            poll_interval: float = 0.5,\n        ) -&gt; StreamingResponse:\n            \"\"\"Stream real-time job status updates using Server-Sent Events.\"\"\"\n            # Validate job_id format\n            try:\n                ulid_id = ULID.from_str(job_id)\n            except ValueError:\n                raise HTTPException(status_code=400, detail=\"Invalid job ID format\")\n\n            # Check job exists before starting stream\n            try:\n                await scheduler.get_record(ulid_id)\n            except KeyError:\n                raise HTTPException(status_code=404, detail=\"Job not found\")\n\n            # SSE event generator\n            async def event_stream() -&gt; AsyncGenerator[bytes, None]:\n                terminal_states = {\"completed\", \"failed\", \"canceled\"}\n\n                while True:\n                    try:\n                        record = await scheduler.get_record(ulid_id)\n                        # Format as SSE event\n                        yield format_sse_model_event(record)\n\n                        # Stop streaming if job reached terminal state\n                        if record.status in terminal_states:\n                            break\n\n                    except KeyError:\n                        # Job was deleted - send final event and close\n                        yield b'data: {\"status\": \"deleted\"}\\n\\n'\n                        break\n\n                    await asyncio.sleep(poll_interval)\n\n            return StreamingResponse(\n                event_stream(),\n                media_type=\"text/event-stream\",\n                headers=SSE_HEADERS,\n            )\n</code></pre>"},{"location":"api-reference/#servicekit.api.routers.job.JobRouter-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.routers.job.JobRouter.__init__","title":"<code>__init__(prefix, tags, scheduler_factory, **kwargs)</code>","text":"<p>Initialize job router with scheduler factory.</p> Source code in <code>src/servicekit/api/routers/job.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    scheduler_factory: Callable[[], JobScheduler],\n    **kwargs: object,\n) -&gt; None:\n    \"\"\"Initialize job router with scheduler factory.\"\"\"\n    self.scheduler_factory = scheduler_factory\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#systemrouter","title":"SystemRouter","text":""},{"location":"api-reference/#servicekit.api.routers.system.SystemRouter","title":"<code>SystemRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>System information router.</p> Source code in <code>src/servicekit/api/routers/system.py</code> <pre><code>class SystemRouter(Router):\n    \"\"\"System information router.\"\"\"\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register system info endpoint.\"\"\"\n\n        @self.router.get(\n            \"\",\n            summary=\"System information\",\n            response_model=SystemInfo,\n        )\n        async def get_system_info() -&gt; SystemInfo:\n            return SystemInfo(\n                current_time=datetime.now(timezone.utc),\n                timezone=str(datetime.now().astimezone().tzinfo),\n                python_version=f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n                platform=platform.platform(),\n                hostname=platform.node(),\n            )\n\n        @self.router.get(\n            \"/apps\",\n            summary=\"List installed apps\",\n            response_model=list[AppInfo],\n        )\n        async def list_apps(\n            app_manager: Annotated[AppManager, Depends(get_app_manager)],\n        ) -&gt; list[AppInfo]:\n            \"\"\"List all installed apps with their metadata.\"\"\"\n            return [\n                AppInfo(\n                    name=app.manifest.name,\n                    version=app.manifest.version,\n                    prefix=app.prefix,\n                    description=app.manifest.description,\n                    author=app.manifest.author,\n                    entry=app.manifest.entry,\n                    is_package=app.is_package,\n                )\n                for app in app_manager.list()\n            ]\n\n        @self.router.get(\n            \"/apps/$schema\",\n            summary=\"Get apps list schema\",\n            response_model=dict[str, Any],\n        )\n        async def get_apps_schema() -&gt; dict[str, Any]:\n            \"\"\"Get JSON schema for apps list response.\"\"\"\n            return TypeAdapter(list[AppInfo]).json_schema()\n</code></pre>"},{"location":"api-reference/#metricsrouter","title":"MetricsRouter","text":""},{"location":"api-reference/#servicekit.api.routers.metrics.MetricsRouter","title":"<code>MetricsRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>Metrics router for Prometheus metrics exposition.</p> Source code in <code>src/servicekit/api/routers/metrics.py</code> <pre><code>class MetricsRouter(Router):\n    \"\"\"Metrics router for Prometheus metrics exposition.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        metric_reader: PrometheusMetricReader,\n        **kwargs: object,\n    ) -&gt; None:\n        \"\"\"Initialize metrics router with Prometheus metric reader.\"\"\"\n        self.metric_reader = metric_reader\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register Prometheus metrics endpoint.\"\"\"\n\n        @self.router.get(\n            \"\",\n            summary=\"Prometheus metrics\",\n            response_class=Response,\n        )\n        async def get_metrics() -&gt; Response:\n            \"\"\"Expose metrics in Prometheus text format.\"\"\"\n            # Get latest metrics from the reader\n            from prometheus_client import REGISTRY, generate_latest\n\n            metrics_output = generate_latest(REGISTRY)\n\n            return Response(\n                content=metrics_output,\n                media_type=\"text/plain; version=0.0.4; charset=utf-8\",\n            )\n</code></pre>"},{"location":"api-reference/#servicekit.api.routers.metrics.MetricsRouter-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.routers.metrics.MetricsRouter.__init__","title":"<code>__init__(prefix, tags, metric_reader, **kwargs)</code>","text":"<p>Initialize metrics router with Prometheus metric reader.</p> Source code in <code>src/servicekit/api/routers/metrics.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    metric_reader: PrometheusMetricReader,\n    **kwargs: object,\n) -&gt; None:\n    \"\"\"Initialize metrics router with Prometheus metric reader.\"\"\"\n    self.metric_reader = metric_reader\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#app-system","title":"App System","text":"<p>Static web application hosting system.</p>"},{"location":"api-reference/#apploader","title":"AppLoader","text":""},{"location":"api-reference/#servicekit.api.app.AppLoader","title":"<code>AppLoader</code>","text":"<p>Loads and validates apps from filesystem or package resources.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>class AppLoader:\n    \"\"\"Loads and validates apps from filesystem or package resources.\"\"\"\n\n    @staticmethod\n    def load(path: str | Path | tuple[str, str], prefix: str | None = None) -&gt; App:\n        \"\"\"Load and validate app from filesystem path or package resource tuple.\"\"\"\n        # Detect source type and resolve to directory\n        if isinstance(path, tuple):\n            # Package resource\n            dir_path, is_package = AppLoader._resolve_package_path(path)\n        else:\n            # Filesystem path\n            dir_path = Path(path).resolve()\n            is_package = False\n\n            if not dir_path.exists():\n                raise FileNotFoundError(f\"App directory not found: {dir_path}\")\n            if not dir_path.is_dir():\n                raise NotADirectoryError(f\"App path is not a directory: {dir_path}\")\n\n        # Load and validate manifest\n        manifest_path = dir_path / \"manifest.json\"\n        if not manifest_path.exists():\n            raise FileNotFoundError(f\"manifest.json not found in: {dir_path}\")\n\n        try:\n            with manifest_path.open() as f:\n                manifest_data = json.load(f)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON in manifest.json: {e}\") from e\n\n        manifest = AppManifest(**manifest_data)\n\n        # Validate entry file exists\n        entry_path = dir_path / manifest.entry\n        if not entry_path.exists():\n            raise FileNotFoundError(f\"Entry file '{manifest.entry}' not found in: {dir_path}\")\n\n        # Use override or manifest prefix\n        final_prefix = prefix if prefix is not None else manifest.prefix\n\n        # Re-validate prefix if overridden\n        if prefix is not None:\n            validated = AppManifest(\n                name=manifest.name,\n                version=manifest.version,\n                prefix=final_prefix,\n            )\n            final_prefix = validated.prefix\n\n        return App(\n            manifest=manifest,\n            directory=dir_path,\n            prefix=final_prefix,\n            is_package=is_package,\n        )\n\n    @staticmethod\n    def discover(path: str | Path | tuple[str, str]) -&gt; list[App]:\n        \"\"\"Discover all apps with manifest.json in directory.\"\"\"\n        # Resolve directory\n        if isinstance(path, tuple):\n            dir_path, _ = AppLoader._resolve_package_path(path)\n        else:\n            dir_path = Path(path).resolve()\n\n            if not dir_path.exists():\n                raise FileNotFoundError(f\"Apps directory not found: {dir_path}\")\n            if not dir_path.is_dir():\n                raise NotADirectoryError(f\"Apps path is not a directory: {dir_path}\")\n\n        # Scan for subdirectories with manifest.json\n        apps: list[App] = []\n        for subdir in dir_path.iterdir():\n            if subdir.is_dir() and (subdir / \"manifest.json\").exists():\n                try:\n                    # Determine if we're in a package context\n                    if isinstance(path, tuple):\n                        # Build tuple path for subdirectory\n                        package_name: str = path[0]\n                        base_path: str = path[1]\n                        subdir_name = subdir.name\n                        subpath = f\"{base_path}/{subdir_name}\" if base_path else subdir_name\n                        app = AppLoader.load((package_name, subpath))\n                    else:\n                        app = AppLoader.load(subdir)\n                    apps.append(app)\n                except Exception as e:\n                    # Log but don't fail discovery for invalid apps\n                    logger.warning(\n                        \"app.discovery.failed\",\n                        directory=str(subdir),\n                        error=str(e),\n                    )\n\n        return apps\n\n    @staticmethod\n    def _resolve_package_path(package_tuple: tuple[str, str]) -&gt; tuple[Path, bool]:\n        \"\"\"Resolve package resource tuple to filesystem path.\"\"\"\n        package_name, subpath = package_tuple\n\n        # Validate subpath for security\n        if \"..\" in subpath:\n            raise ValueError(f\"subpath cannot contain '..' (got: {subpath})\")\n        if subpath.startswith(\"/\"):\n            raise ValueError(f\"subpath must be relative (got: {subpath})\")\n\n        try:\n            spec = importlib.util.find_spec(package_name)\n        except (ModuleNotFoundError, ValueError) as e:\n            raise ValueError(f\"Package '{package_name}' could not be found\") from e\n\n        if spec is None or spec.origin is None:\n            raise ValueError(f\"Package '{package_name}' could not be found\")\n\n        # Resolve to package directory\n        package_dir = Path(spec.origin).parent\n        app_dir = package_dir / subpath\n\n        # Verify resolved path is still within package directory\n        try:\n            app_dir.resolve().relative_to(package_dir.resolve())\n        except ValueError as e:\n            raise ValueError(f\"App path '{subpath}' escapes package directory\") from e\n\n        if not app_dir.exists():\n            raise FileNotFoundError(f\"App path '{subpath}' not found in package '{package_name}'\")\n        if not app_dir.is_dir():\n            raise NotADirectoryError(f\"App path '{subpath}' in package '{package_name}' is not a directory\")\n\n        return app_dir, True\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppLoader-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.app.AppLoader.load","title":"<code>load(path, prefix=None)</code>  <code>staticmethod</code>","text":"<p>Load and validate app from filesystem path or package resource tuple.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>@staticmethod\ndef load(path: str | Path | tuple[str, str], prefix: str | None = None) -&gt; App:\n    \"\"\"Load and validate app from filesystem path or package resource tuple.\"\"\"\n    # Detect source type and resolve to directory\n    if isinstance(path, tuple):\n        # Package resource\n        dir_path, is_package = AppLoader._resolve_package_path(path)\n    else:\n        # Filesystem path\n        dir_path = Path(path).resolve()\n        is_package = False\n\n        if not dir_path.exists():\n            raise FileNotFoundError(f\"App directory not found: {dir_path}\")\n        if not dir_path.is_dir():\n            raise NotADirectoryError(f\"App path is not a directory: {dir_path}\")\n\n    # Load and validate manifest\n    manifest_path = dir_path / \"manifest.json\"\n    if not manifest_path.exists():\n        raise FileNotFoundError(f\"manifest.json not found in: {dir_path}\")\n\n    try:\n        with manifest_path.open() as f:\n            manifest_data = json.load(f)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in manifest.json: {e}\") from e\n\n    manifest = AppManifest(**manifest_data)\n\n    # Validate entry file exists\n    entry_path = dir_path / manifest.entry\n    if not entry_path.exists():\n        raise FileNotFoundError(f\"Entry file '{manifest.entry}' not found in: {dir_path}\")\n\n    # Use override or manifest prefix\n    final_prefix = prefix if prefix is not None else manifest.prefix\n\n    # Re-validate prefix if overridden\n    if prefix is not None:\n        validated = AppManifest(\n            name=manifest.name,\n            version=manifest.version,\n            prefix=final_prefix,\n        )\n        final_prefix = validated.prefix\n\n    return App(\n        manifest=manifest,\n        directory=dir_path,\n        prefix=final_prefix,\n        is_package=is_package,\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppLoader.discover","title":"<code>discover(path)</code>  <code>staticmethod</code>","text":"<p>Discover all apps with manifest.json in directory.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>@staticmethod\ndef discover(path: str | Path | tuple[str, str]) -&gt; list[App]:\n    \"\"\"Discover all apps with manifest.json in directory.\"\"\"\n    # Resolve directory\n    if isinstance(path, tuple):\n        dir_path, _ = AppLoader._resolve_package_path(path)\n    else:\n        dir_path = Path(path).resolve()\n\n        if not dir_path.exists():\n            raise FileNotFoundError(f\"Apps directory not found: {dir_path}\")\n        if not dir_path.is_dir():\n            raise NotADirectoryError(f\"Apps path is not a directory: {dir_path}\")\n\n    # Scan for subdirectories with manifest.json\n    apps: list[App] = []\n    for subdir in dir_path.iterdir():\n        if subdir.is_dir() and (subdir / \"manifest.json\").exists():\n            try:\n                # Determine if we're in a package context\n                if isinstance(path, tuple):\n                    # Build tuple path for subdirectory\n                    package_name: str = path[0]\n                    base_path: str = path[1]\n                    subdir_name = subdir.name\n                    subpath = f\"{base_path}/{subdir_name}\" if base_path else subdir_name\n                    app = AppLoader.load((package_name, subpath))\n                else:\n                    app = AppLoader.load(subdir)\n                apps.append(app)\n            except Exception as e:\n                # Log but don't fail discovery for invalid apps\n                logger.warning(\n                    \"app.discovery.failed\",\n                    directory=str(subdir),\n                    error=str(e),\n                )\n\n    return apps\n</code></pre>"},{"location":"api-reference/#appmanifest","title":"AppManifest","text":""},{"location":"api-reference/#servicekit.api.app.AppManifest","title":"<code>AppManifest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>App manifest configuration.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>class AppManifest(BaseModel):\n    \"\"\"App manifest configuration.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: str = Field(description=\"Human-readable app name\")\n    version: str = Field(description=\"Semantic version\")\n    prefix: str = Field(description=\"URL prefix for mounting\")\n    description: str | None = Field(default=None, description=\"App description\")\n    author: str | None = Field(default=None, description=\"Author name\")\n    entry: str = Field(default=\"index.html\", description=\"Entry point filename\")\n\n    @field_validator(\"prefix\")\n    @classmethod\n    def validate_prefix(cls, v: str) -&gt; str:\n        \"\"\"Validate mount prefix format.\"\"\"\n        if not v.startswith(\"/\"):\n            raise ValueError(\"prefix must start with '/'\")\n        if \"..\" in v:\n            raise ValueError(\"prefix cannot contain '..'\")\n        if v.startswith(\"/api/\") or v == \"/api\":\n            raise ValueError(\"prefix cannot be '/api' or start with '/api/'\")\n        return v\n\n    @field_validator(\"entry\")\n    @classmethod\n    def validate_entry(cls, v: str) -&gt; str:\n        \"\"\"Validate entry file path for security.\"\"\"\n        if \"..\" in v:\n            raise ValueError(\"entry cannot contain '..'\")\n        if v.startswith(\"/\"):\n            raise ValueError(\"entry must be a relative path\")\n        # Normalize and check for path traversal\n        normalized = Path(v).as_posix()\n        if normalized.startswith(\"../\") or \"/../\" in normalized:\n            raise ValueError(\"entry cannot contain path traversal\")\n        return v\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppManifest-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.app.AppManifest.validate_prefix","title":"<code>validate_prefix(v)</code>  <code>classmethod</code>","text":"<p>Validate mount prefix format.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>@field_validator(\"prefix\")\n@classmethod\ndef validate_prefix(cls, v: str) -&gt; str:\n    \"\"\"Validate mount prefix format.\"\"\"\n    if not v.startswith(\"/\"):\n        raise ValueError(\"prefix must start with '/'\")\n    if \"..\" in v:\n        raise ValueError(\"prefix cannot contain '..'\")\n    if v.startswith(\"/api/\") or v == \"/api\":\n        raise ValueError(\"prefix cannot be '/api' or start with '/api/'\")\n    return v\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppManifest.validate_entry","title":"<code>validate_entry(v)</code>  <code>classmethod</code>","text":"<p>Validate entry file path for security.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>@field_validator(\"entry\")\n@classmethod\ndef validate_entry(cls, v: str) -&gt; str:\n    \"\"\"Validate entry file path for security.\"\"\"\n    if \"..\" in v:\n        raise ValueError(\"entry cannot contain '..'\")\n    if v.startswith(\"/\"):\n        raise ValueError(\"entry must be a relative path\")\n    # Normalize and check for path traversal\n    normalized = Path(v).as_posix()\n    if normalized.startswith(\"../\") or \"/../\" in normalized:\n        raise ValueError(\"entry cannot contain path traversal\")\n    return v\n</code></pre>"},{"location":"api-reference/#app","title":"App","text":""},{"location":"api-reference/#servicekit.api.app.App","title":"<code>App</code>  <code>dataclass</code>","text":"<p>Represents a loaded app with manifest and directory.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>@dataclass\nclass App:\n    \"\"\"Represents a loaded app with manifest and directory.\"\"\"\n\n    manifest: AppManifest\n    directory: Path\n    prefix: str  # May differ from manifest if overridden\n    is_package: bool  # True if loaded from package resources\n</code></pre>"},{"location":"api-reference/#appmanager","title":"AppManager","text":""},{"location":"api-reference/#servicekit.api.app.AppManager","title":"<code>AppManager</code>","text":"<p>Lightweight manager for app metadata queries.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>class AppManager:\n    \"\"\"Lightweight manager for app metadata queries.\"\"\"\n\n    def __init__(self, apps: list[App]):\n        \"\"\"Initialize with loaded apps.\"\"\"\n        self._apps = apps\n\n    def list(self) -&gt; list[App]:\n        \"\"\"Return all installed apps.\"\"\"\n        return self._apps\n\n    def get(self, prefix: str) -&gt; App | None:\n        \"\"\"Get app by mount prefix.\"\"\"\n        return next((app for app in self._apps if app.prefix == prefix), None)\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppManager-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.app.AppManager.__init__","title":"<code>__init__(apps)</code>","text":"<p>Initialize with loaded apps.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>def __init__(self, apps: list[App]):\n    \"\"\"Initialize with loaded apps.\"\"\"\n    self._apps = apps\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppManager.list","title":"<code>list()</code>","text":"<p>Return all installed apps.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>def list(self) -&gt; list[App]:\n    \"\"\"Return all installed apps.\"\"\"\n    return self._apps\n</code></pre>"},{"location":"api-reference/#servicekit.api.app.AppManager.get","title":"<code>get(prefix)</code>","text":"<p>Get app by mount prefix.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>def get(self, prefix: str) -&gt; App | None:\n    \"\"\"Get app by mount prefix.\"\"\"\n    return next((app for app in self._apps if app.prefix == prefix), None)\n</code></pre>"},{"location":"api-reference/#appinfo","title":"AppInfo","text":""},{"location":"api-reference/#servicekit.api.app.AppInfo","title":"<code>AppInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>App metadata for API responses.</p> Source code in <code>src/servicekit/api/app.py</code> <pre><code>class AppInfo(BaseModel):\n    \"\"\"App metadata for API responses.\"\"\"\n\n    name: str = Field(description=\"Human-readable app name\")\n    version: str = Field(description=\"Semantic version\")\n    prefix: str = Field(description=\"URL prefix for mounting\")\n    description: str | None = Field(default=None, description=\"App description\")\n    author: str | None = Field(default=None, description=\"Author name\")\n    entry: str = Field(description=\"Entry point filename\")\n    is_package: bool = Field(description=\"Whether app is loaded from package resources\")\n</code></pre>"},{"location":"api-reference/#authentication","title":"Authentication","text":"<p>API key authentication middleware and utilities.</p>"},{"location":"api-reference/#apikeymiddleware","title":"APIKeyMiddleware","text":""},{"location":"api-reference/#servicekit.api.auth.APIKeyMiddleware","title":"<code>APIKeyMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware for API key authentication via X-API-Key header.</p> Source code in <code>src/servicekit/api/auth.py</code> <pre><code>class APIKeyMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware for API key authentication via X-API-Key header.\"\"\"\n\n    def __init__(\n        self,\n        app: Any,\n        *,\n        api_keys: Set[str],\n        header_name: str = \"X-API-Key\",\n        unauthenticated_paths: Set[str],\n    ) -&gt; None:\n        \"\"\"Initialize API key middleware.\n\n        Args:\n            app: ASGI application\n            api_keys: Set of valid API keys\n            header_name: HTTP header name for API key\n            unauthenticated_paths: Paths that don't require authentication\n        \"\"\"\n        super().__init__(app)\n        self.api_keys = api_keys\n        self.header_name = header_name\n        self.unauthenticated_paths = unauthenticated_paths\n\n    async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n        \"\"\"Process request with API key authentication.\"\"\"\n        # Allow unauthenticated access to specific paths\n        if request.url.path in self.unauthenticated_paths:\n            return await call_next(request)\n\n        # Extract API key from header\n        api_key = request.headers.get(self.header_name)\n\n        if not api_key:\n            logger.warning(\n                \"auth.missing_key\",\n                path=request.url.path,\n                method=request.method,\n            )\n            problem = ProblemDetail(\n                type=\"urn:servicekit:error:unauthorized\",\n                title=\"Unauthorized\",\n                status=status.HTTP_401_UNAUTHORIZED,\n                detail=f\"Missing authentication header: {self.header_name}\",\n                instance=str(request.url.path),\n            )\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content=problem.model_dump(exclude_none=True),\n                media_type=\"application/problem+json\",\n            )\n\n        # Validate API key\n        if api_key not in self.api_keys:\n            # Log only prefix for security\n            key_prefix = api_key[:7] if len(api_key) &gt;= 7 else \"***\"\n            logger.warning(\n                \"auth.invalid_key\",\n                key_prefix=key_prefix,\n                path=request.url.path,\n                method=request.method,\n            )\n            problem = ProblemDetail(\n                type=\"urn:servicekit:error:unauthorized\",\n                title=\"Unauthorized\",\n                status=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Invalid API key\",\n                instance=str(request.url.path),\n            )\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content=problem.model_dump(exclude_none=True),\n                media_type=\"application/problem+json\",\n            )\n\n        # Attach key prefix to request state for logging\n        request.state.api_key_prefix = api_key[:7] if len(api_key) &gt;= 7 else \"***\"\n\n        logger.info(\n            \"auth.success\",\n            key_prefix=request.state.api_key_prefix,\n            path=request.url.path,\n        )\n\n        return await call_next(request)\n</code></pre>"},{"location":"api-reference/#servicekit.api.auth.APIKeyMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.auth.APIKeyMiddleware.__init__","title":"<code>__init__(app, *, api_keys, header_name='X-API-Key', unauthenticated_paths)</code>","text":"<p>Initialize API key middleware.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Any</code> <p>ASGI application</p> required <code>api_keys</code> <code>Set[str]</code> <p>Set of valid API keys</p> required <code>header_name</code> <code>str</code> <p>HTTP header name for API key</p> <code>'X-API-Key'</code> <code>unauthenticated_paths</code> <code>Set[str]</code> <p>Paths that don't require authentication</p> required Source code in <code>src/servicekit/api/auth.py</code> <pre><code>def __init__(\n    self,\n    app: Any,\n    *,\n    api_keys: Set[str],\n    header_name: str = \"X-API-Key\",\n    unauthenticated_paths: Set[str],\n) -&gt; None:\n    \"\"\"Initialize API key middleware.\n\n    Args:\n        app: ASGI application\n        api_keys: Set of valid API keys\n        header_name: HTTP header name for API key\n        unauthenticated_paths: Paths that don't require authentication\n    \"\"\"\n    super().__init__(app)\n    self.api_keys = api_keys\n    self.header_name = header_name\n    self.unauthenticated_paths = unauthenticated_paths\n</code></pre>"},{"location":"api-reference/#servicekit.api.auth.APIKeyMiddleware.dispatch","title":"<code>dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Process request with API key authentication.</p> Source code in <code>src/servicekit/api/auth.py</code> <pre><code>async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n    \"\"\"Process request with API key authentication.\"\"\"\n    # Allow unauthenticated access to specific paths\n    if request.url.path in self.unauthenticated_paths:\n        return await call_next(request)\n\n    # Extract API key from header\n    api_key = request.headers.get(self.header_name)\n\n    if not api_key:\n        logger.warning(\n            \"auth.missing_key\",\n            path=request.url.path,\n            method=request.method,\n        )\n        problem = ProblemDetail(\n            type=\"urn:servicekit:error:unauthorized\",\n            title=\"Unauthorized\",\n            status=status.HTTP_401_UNAUTHORIZED,\n            detail=f\"Missing authentication header: {self.header_name}\",\n            instance=str(request.url.path),\n        )\n        return JSONResponse(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            content=problem.model_dump(exclude_none=True),\n            media_type=\"application/problem+json\",\n        )\n\n    # Validate API key\n    if api_key not in self.api_keys:\n        # Log only prefix for security\n        key_prefix = api_key[:7] if len(api_key) &gt;= 7 else \"***\"\n        logger.warning(\n            \"auth.invalid_key\",\n            key_prefix=key_prefix,\n            path=request.url.path,\n            method=request.method,\n        )\n        problem = ProblemDetail(\n            type=\"urn:servicekit:error:unauthorized\",\n            title=\"Unauthorized\",\n            status=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\",\n            instance=str(request.url.path),\n        )\n        return JSONResponse(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            content=problem.model_dump(exclude_none=True),\n            media_type=\"application/problem+json\",\n        )\n\n    # Attach key prefix to request state for logging\n    request.state.api_key_prefix = api_key[:7] if len(api_key) &gt;= 7 else \"***\"\n\n    logger.info(\n        \"auth.success\",\n        key_prefix=request.state.api_key_prefix,\n        path=request.url.path,\n    )\n\n    return await call_next(request)\n</code></pre>"},{"location":"api-reference/#middleware","title":"Middleware","text":"<p>Error handling and logging middleware.</p>"},{"location":"api-reference/#servicekit.api.middleware","title":"<code>middleware</code>","text":"<p>FastAPI middleware for error handling, CORS, and other cross-cutting concerns.</p>"},{"location":"api-reference/#servicekit.api.middleware-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.api.middleware.AppPrefixRedirectMiddleware","title":"<code>AppPrefixRedirectMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware to redirect app prefix requests without trailing slash to version with trailing slash.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>class AppPrefixRedirectMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware to redirect app prefix requests without trailing slash to version with trailing slash.\"\"\"\n\n    def __init__(self, app: Any, app_prefixes: list[str]) -&gt; None:\n        \"\"\"Initialize middleware with list of app prefixes to handle.\"\"\"\n        super().__init__(app)\n        self.app_prefixes = set(app_prefixes)\n\n    async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n        \"\"\"Redirect requests to app prefixes without trailing slash.\"\"\"\n        # Check if path matches one of our app prefixes exactly (no trailing slash)\n        if request.url.path in self.app_prefixes and request.method in (\"GET\", \"HEAD\"):\n            from fastapi.responses import RedirectResponse\n\n            # Redirect to same path with trailing slash\n            redirect_url = request.url.replace(path=f\"{request.url.path}/\")\n            return RedirectResponse(url=str(redirect_url), status_code=307)\n\n        # Continue processing\n        return await call_next(request)\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.AppPrefixRedirectMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.middleware.AppPrefixRedirectMiddleware.__init__","title":"<code>__init__(app, app_prefixes)</code>","text":"<p>Initialize middleware with list of app prefixes to handle.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>def __init__(self, app: Any, app_prefixes: list[str]) -&gt; None:\n    \"\"\"Initialize middleware with list of app prefixes to handle.\"\"\"\n    super().__init__(app)\n    self.app_prefixes = set(app_prefixes)\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.AppPrefixRedirectMiddleware.dispatch","title":"<code>dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Redirect requests to app prefixes without trailing slash.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n    \"\"\"Redirect requests to app prefixes without trailing slash.\"\"\"\n    # Check if path matches one of our app prefixes exactly (no trailing slash)\n    if request.url.path in self.app_prefixes and request.method in (\"GET\", \"HEAD\"):\n        from fastapi.responses import RedirectResponse\n\n        # Redirect to same path with trailing slash\n        redirect_url = request.url.replace(path=f\"{request.url.path}/\")\n        return RedirectResponse(url=str(redirect_url), status_code=307)\n\n    # Continue processing\n    return await call_next(request)\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.RequestLoggingMiddleware","title":"<code>RequestLoggingMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware for logging HTTP requests with unique request IDs and context binding.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>class RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware for logging HTTP requests with unique request IDs and context binding.\"\"\"\n\n    async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n        \"\"\"Process request with logging and context binding.\"\"\"\n        request_id = str(ULID())\n        start_time = time.perf_counter()\n\n        # Bind request context\n        add_request_context(\n            request_id=request_id,\n            method=request.method,\n            path=request.url.path,\n            client_host=request.client.host if request.client else None,\n        )\n\n        # Add request_id to request state for access in endpoints\n        request.state.request_id = request_id\n\n        logger.info(\n            \"http.request.start\",\n            query_params=str(request.url.query) if request.url.query else None,\n        )\n\n        try:\n            response = await call_next(request)\n            duration_ms = (time.perf_counter() - start_time) * 1000\n\n            logger.info(\n                \"http.request.complete\",\n                status_code=response.status_code,\n                duration_ms=round(duration_ms, 2),\n            )\n\n            # Add request_id to response headers for tracing\n            response.headers[\"X-Request-ID\"] = request_id\n\n            return response\n\n        except Exception as exc:\n            duration_ms = (time.perf_counter() - start_time) * 1000\n\n            logger.error(\n                \"http.request.error\",\n                duration_ms=round(duration_ms, 2),\n                error=str(exc),\n                exc_info=True,\n            )\n            raise\n\n        finally:\n            # Clear request context after response\n            reset_request_context()\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.RequestLoggingMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.middleware.RequestLoggingMiddleware.dispatch","title":"<code>dispatch(request, call_next)</code>  <code>async</code>","text":"<p>Process request with logging and context binding.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>async def dispatch(self, request: Request, call_next: MiddlewareCallNext) -&gt; Response:\n    \"\"\"Process request with logging and context binding.\"\"\"\n    request_id = str(ULID())\n    start_time = time.perf_counter()\n\n    # Bind request context\n    add_request_context(\n        request_id=request_id,\n        method=request.method,\n        path=request.url.path,\n        client_host=request.client.host if request.client else None,\n    )\n\n    # Add request_id to request state for access in endpoints\n    request.state.request_id = request_id\n\n    logger.info(\n        \"http.request.start\",\n        query_params=str(request.url.query) if request.url.query else None,\n    )\n\n    try:\n        response = await call_next(request)\n        duration_ms = (time.perf_counter() - start_time) * 1000\n\n        logger.info(\n            \"http.request.complete\",\n            status_code=response.status_code,\n            duration_ms=round(duration_ms, 2),\n        )\n\n        # Add request_id to response headers for tracing\n        response.headers[\"X-Request-ID\"] = request_id\n\n        return response\n\n    except Exception as exc:\n        duration_ms = (time.perf_counter() - start_time) * 1000\n\n        logger.error(\n            \"http.request.error\",\n            duration_ms=round(duration_ms, 2),\n            error=str(exc),\n            exc_info=True,\n        )\n        raise\n\n    finally:\n        # Clear request context after response\n        reset_request_context()\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.middleware.database_error_handler","title":"<code>database_error_handler(request, exc)</code>  <code>async</code>","text":"<p>Handle database errors and return error response.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>async def database_error_handler(request: Request, exc: Exception) -&gt; JSONResponse:\n    \"\"\"Handle database errors and return error response.\"\"\"\n    logger.error(\n        \"database.error\",\n        error=str(exc),\n        path=request.url.path,\n        exc_info=True,\n    )\n    return JSONResponse(\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        content={\"detail\": \"Database error occurred\", \"error\": str(exc)},\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.validation_error_handler","title":"<code>validation_error_handler(request, exc)</code>  <code>async</code>","text":"<p>Handle validation errors and return error response.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>async def validation_error_handler(request: Request, exc: Exception) -&gt; JSONResponse:\n    \"\"\"Handle validation errors and return error response.\"\"\"\n    logger.warning(\n        \"validation.error\",\n        error=str(exc),\n        path=request.url.path,\n    )\n    return JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n        content={\"detail\": \"Validation error\", \"errors\": str(exc)},\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.servicekit_exception_handler","title":"<code>servicekit_exception_handler(request, exc)</code>  <code>async</code>","text":"<p>Handle ServicekitException and return RFC 9457 Problem Details.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>async def servicekit_exception_handler(request: Request, exc: ServicekitException) -&gt; JSONResponse:\n    \"\"\"Handle ServicekitException and return RFC 9457 Problem Details.\"\"\"\n    logger.warning(\n        \"servicekit.error\",\n        error_type=exc.type_uri,\n        status=exc.status,\n        detail=exc.detail,\n        path=request.url.path,\n    )\n\n    problem = ProblemDetail(\n        type=exc.type_uri,\n        title=exc.title,\n        status=exc.status,\n        detail=exc.detail,\n        instance=exc.instance or str(request.url),\n        **exc.extensions,\n    )\n\n    return JSONResponse(\n        status_code=exc.status,\n        content=problem.model_dump(exclude_none=True),\n        media_type=\"application/problem+json\",\n    )\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.add_error_handlers","title":"<code>add_error_handlers(app)</code>","text":"<p>Add error handlers to FastAPI application.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>def add_error_handlers(app: Any) -&gt; None:\n    \"\"\"Add error handlers to FastAPI application.\"\"\"\n    from pydantic import ValidationError\n    from sqlalchemy.exc import SQLAlchemyError\n\n    app.add_exception_handler(ServicekitException, servicekit_exception_handler)\n    app.add_exception_handler(SQLAlchemyError, database_error_handler)\n    app.add_exception_handler(ValidationError, validation_error_handler)\n</code></pre>"},{"location":"api-reference/#servicekit.api.middleware.add_logging_middleware","title":"<code>add_logging_middleware(app)</code>","text":"<p>Add request logging middleware to FastAPI application.</p> Source code in <code>src/servicekit/api/middleware.py</code> <pre><code>def add_logging_middleware(app: Any) -&gt; None:\n    \"\"\"Add request logging middleware to FastAPI application.\"\"\"\n    app.add_middleware(RequestLoggingMiddleware)\n</code></pre>"},{"location":"api-reference/#dependencies","title":"Dependencies","text":"<p>FastAPI dependency injection functions.</p>"},{"location":"api-reference/#servicekit.api.dependencies","title":"<code>dependencies</code>","text":"<p>Generic FastAPI dependency injection for database and scheduler.</p>"},{"location":"api-reference/#servicekit.api.dependencies-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.api.dependencies-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.dependencies.set_database","title":"<code>set_database(database)</code>","text":"<p>Set the global database instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def set_database(database: Database) -&gt; None:\n    \"\"\"Set the global database instance.\"\"\"\n    global _database\n    _database = database\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.get_database","title":"<code>get_database()</code>","text":"<p>Get the global database instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def get_database() -&gt; Database:\n    \"\"\"Get the global database instance.\"\"\"\n    if _database is None:\n        raise RuntimeError(\"Database not initialized. Call set_database() during app startup.\")\n    return _database\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.get_session","title":"<code>get_session(db)</code>  <code>async</code>","text":"<p>Get a database session for dependency injection.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>async def get_session(db: Annotated[Database, Depends(get_database)]) -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Get a database session for dependency injection.\"\"\"\n    async with db.session() as session:\n        yield session\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.set_scheduler","title":"<code>set_scheduler(scheduler)</code>","text":"<p>Set the global scheduler instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def set_scheduler(scheduler: JobScheduler) -&gt; None:\n    \"\"\"Set the global scheduler instance.\"\"\"\n    global _scheduler\n    _scheduler = scheduler\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.get_scheduler","title":"<code>get_scheduler()</code>","text":"<p>Get the global scheduler instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def get_scheduler() -&gt; JobScheduler:\n    \"\"\"Get the global scheduler instance.\"\"\"\n    if _scheduler is None:\n        raise RuntimeError(\"Scheduler not initialized. Call set_scheduler() during app startup.\")\n    return _scheduler\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.set_app_manager","title":"<code>set_app_manager(manager)</code>","text":"<p>Set the global app manager instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def set_app_manager(manager: AppManager) -&gt; None:\n    \"\"\"Set the global app manager instance.\"\"\"\n    global _app_manager\n    _app_manager = manager\n</code></pre>"},{"location":"api-reference/#servicekit.api.dependencies.get_app_manager","title":"<code>get_app_manager()</code>","text":"<p>Get the global app manager instance.</p> Source code in <code>src/servicekit/api/dependencies.py</code> <pre><code>def get_app_manager() -&gt; AppManager:\n    \"\"\"Get the global app manager instance.\"\"\"\n    if _app_manager is None:\n        raise RuntimeError(\"AppManager not initialized. Call set_app_manager() during app startup.\")\n    return _app_manager\n</code></pre>"},{"location":"api-reference/#pagination","title":"Pagination","text":"<p>Pagination helpers for collection endpoints.</p>"},{"location":"api-reference/#servicekit.api.pagination","title":"<code>pagination</code>","text":"<p>Pagination utilities for API endpoints.</p>"},{"location":"api-reference/#servicekit.api.pagination-classes","title":"Classes","text":""},{"location":"api-reference/#servicekit.api.pagination.PaginationParams","title":"<code>PaginationParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Query parameters for opt-in pagination (both page and size required).</p> Source code in <code>src/servicekit/api/pagination.py</code> <pre><code>class PaginationParams(BaseModel):\n    \"\"\"Query parameters for opt-in pagination (both page and size required).\"\"\"\n\n    page: int | None = Field(default=None, ge=1, description=\"Page number (1-indexed)\")\n    size: int | None = Field(default=None, ge=1, le=100, description=\"Number of items per page (max 100)\")\n\n    def is_paginated(self) -&gt; bool:\n        \"\"\"Check if both page and size parameters are provided.\"\"\"\n        return self.page is not None and self.size is not None\n</code></pre>"},{"location":"api-reference/#servicekit.api.pagination.PaginationParams-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.pagination.PaginationParams.is_paginated","title":"<code>is_paginated()</code>","text":"<p>Check if both page and size parameters are provided.</p> Source code in <code>src/servicekit/api/pagination.py</code> <pre><code>def is_paginated(self) -&gt; bool:\n    \"\"\"Check if both page and size parameters are provided.\"\"\"\n    return self.page is not None and self.size is not None\n</code></pre>"},{"location":"api-reference/#servicekit.api.pagination-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.pagination.create_paginated_response","title":"<code>create_paginated_response(items, total, page, size)</code>","text":"<p>Create paginated response with items and metadata.</p> Source code in <code>src/servicekit/api/pagination.py</code> <pre><code>def create_paginated_response(items: list[T], total: int, page: int, size: int) -&gt; PaginatedResponse[T]:\n    \"\"\"Create paginated response with items and metadata.\"\"\"\n    return PaginatedResponse(items=items, total=total, page=page, size=size)\n</code></pre>"},{"location":"api-reference/#utilities","title":"Utilities","text":"<p>Utility functions for FastAPI applications.</p>"},{"location":"api-reference/#servicekit.api.utilities","title":"<code>utilities</code>","text":"<p>Utility functions for FastAPI routers and endpoints.</p>"},{"location":"api-reference/#servicekit.api.utilities-functions","title":"Functions","text":""},{"location":"api-reference/#servicekit.api.utilities.build_location_url","title":"<code>build_location_url(request, path)</code>","text":"<p>Build a full URL for the Location header.</p> Source code in <code>src/servicekit/api/utilities.py</code> <pre><code>def build_location_url(request: Request, path: str) -&gt; str:\n    \"\"\"Build a full URL for the Location header.\"\"\"\n    return f\"{request.url.scheme}://{request.url.netloc}{path}\"\n</code></pre>"},{"location":"api-reference/#servicekit.api.utilities.run_app","title":"<code>run_app(app, *, host=None, port=None, workers=None, reload=None, log_level=None, **uvicorn_kwargs)</code>","text":"<p>Run FastAPI app with Uvicorn development server.</p> <p>For reload to work, pass a string in \"module:app\" format. App instance disables reload automatically.</p> <p>Examples:</p> <pre><code># Direct execution (reload disabled)\nif __name__ == \"__main__\":\n    run_app(app)\n\n# With module path (reload enabled)\nrun_app(\"examples.config_api:app\")\n\n# Production: multiple workers\nrun_app(app, workers=4)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Any | str</code> <p>FastAPI app instance OR string \"module:app\" path</p> required <code>host</code> <code>str | None</code> <p>Server host (default: \"127.0.0.1\", env: HOST)</p> <code>None</code> <code>port</code> <code>int | None</code> <p>Server port (default: 8000, env: PORT)</p> <code>None</code> <code>workers</code> <code>int | None</code> <p>Number of worker processes (default: 1, env: WORKERS)</p> <code>None</code> <code>reload</code> <code>bool | None</code> <p>Enable auto-reload (default: True for string, False for instance)</p> <code>None</code> <code>log_level</code> <code>str | None</code> <p>Logging level (default: from LOG_LEVEL env var or \"info\")</p> <code>None</code> <code>**uvicorn_kwargs</code> <code>Any</code> <p>Additional uvicorn.run() arguments</p> <code>{}</code> Source code in <code>src/servicekit/api/utilities.py</code> <pre><code>def run_app(\n    app: Any | str,\n    *,\n    host: str | None = None,\n    port: int | None = None,\n    workers: int | None = None,\n    reload: bool | None = None,\n    log_level: str | None = None,\n    **uvicorn_kwargs: Any,\n) -&gt; None:\n    \"\"\"Run FastAPI app with Uvicorn development server.\n\n    For reload to work, pass a string in \"module:app\" format.\n    App instance disables reload automatically.\n\n    Examples:\n    --------\n        # Direct execution (reload disabled)\n        if __name__ == \"__main__\":\n            run_app(app)\n\n        # With module path (reload enabled)\n        run_app(\"examples.config_api:app\")\n\n        # Production: multiple workers\n        run_app(app, workers=4)\n\n    Args:\n        app: FastAPI app instance OR string \"module:app\" path\n        host: Server host (default: \"127.0.0.1\", env: HOST)\n        port: Server port (default: 8000, env: PORT)\n        workers: Number of worker processes (default: 1, env: WORKERS)\n        reload: Enable auto-reload (default: True for string, False for instance)\n        log_level: Logging level (default: from LOG_LEVEL env var or \"info\")\n        **uvicorn_kwargs: Additional uvicorn.run() arguments\n    \"\"\"\n    import uvicorn\n\n    # Configure structured logging before uvicorn starts\n    from servicekit.logging import configure_logging\n\n    configure_logging()\n\n    # Read from environment variables with defaults\n    resolved_host: str = host if host is not None else os.getenv(\"HOST\", \"127.0.0.1\")\n    resolved_port: int = port if port is not None else int(os.getenv(\"PORT\", \"8000\"))\n    resolved_workers: int = workers if workers is not None else int(os.getenv(\"WORKERS\", \"1\"))\n    resolved_log_level: str = log_level if log_level is not None else os.getenv(\"LOG_LEVEL\", \"info\").lower()\n\n    # Auto-detect reload behavior if not specified\n    if reload is None:\n        reload = isinstance(app, str)  # Enable reload for string paths, disable for instances\n\n    # Auto-reload is incompatible with multiple workers\n    if resolved_workers &gt; 1 and reload:\n        reload = False\n\n    uvicorn.run(\n        app,\n        host=resolved_host,\n        port=resolved_port,\n        workers=resolved_workers,\n        reload=reload,\n        log_level=resolved_log_level,\n        log_config=None,  # Disable uvicorn's default logging config\n        **uvicorn_kwargs,\n    )\n</code></pre>"},{"location":"guides/app-hosting/","title":"App Hosting","text":"<p>Servicekit enables hosting static web applications (HTML/JS/CSS) alongside your FastAPI service, allowing you to serve dashboards, admin panels, documentation sites, and other web UIs from the same server as your API.</p>"},{"location":"guides/app-hosting/#quick-start","title":"Quick Start","text":""},{"location":"guides/app-hosting/#mount-a-single-app","title":"Mount a Single App","text":"<pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()\n    .with_app(\"./apps/dashboard\")  # Mount app from filesystem\n    .build()\n)\n</code></pre> <p>Your dashboard is now available at the prefix defined in <code>manifest.json</code> (e.g., <code>/dashboard</code>).</p>"},{"location":"guides/app-hosting/#auto-discover-multiple-apps","title":"Auto-Discover Multiple Apps","text":"<pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()\n    .with_apps(\"./apps\")  # Discovers all subdirectories with manifest.json\n    .build()\n)\n</code></pre> <p>All apps in the <code>apps/</code> directory are automatically discovered and mounted.</p>"},{"location":"guides/app-hosting/#app-structure","title":"App Structure","text":"<p>Each app is a directory containing a <code>manifest.json</code> file and static files (HTML, CSS, JavaScript, images).</p>"},{"location":"guides/app-hosting/#directory-layout","title":"Directory Layout","text":"<pre><code>my-app/\n\u251c\u2500\u2500 manifest.json    # Required: App metadata and configuration\n\u251c\u2500\u2500 index.html       # Required: Entry point (configurable)\n\u251c\u2500\u2500 style.css        # Optional: Stylesheets\n\u251c\u2500\u2500 script.js        # Optional: JavaScript\n\u2514\u2500\u2500 assets/          # Optional: Images, fonts, etc.\n    \u2514\u2500\u2500 logo.png\n</code></pre>"},{"location":"guides/app-hosting/#manifest-format","title":"Manifest Format","text":"<p>manifest.json: <pre><code>{\n  \"name\": \"My Dashboard\",\n  \"version\": \"1.0.0\",\n  \"prefix\": \"/dashboard\",\n  \"description\": \"Interactive data dashboard\",\n  \"author\": \"Your Name\",\n  \"entry\": \"index.html\"\n}\n</code></pre></p> <p>Required fields: - name (<code>string</code>): Human-readable app name - version (<code>string</code>): Semantic version (e.g., \"1.0.0\") - prefix (<code>string</code>): URL prefix for mounting (must start with <code>/</code>)</p> <p>Optional fields: - description (<code>string</code>): Brief description of the app - author (<code>string</code>): Author name or organization - entry (<code>string</code>): Entry point filename. Default: <code>\"index.html\"</code></p>"},{"location":"guides/app-hosting/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/app-hosting/#single-app-with_app","title":"Single App: <code>.with_app()</code>","text":"<p>Mount a single app from a filesystem path or package resource:</p> <pre><code># Mount from filesystem\n.with_app(\"./apps/dashboard\")\n\n# Mount from filesystem with custom prefix\n.with_app(\"./apps/dashboard\", prefix=\"/admin\")\n\n# Mount from Python package\n.with_app((\"mycompany.apps\", \"dashboard\"))\n</code></pre> <p>Parameters: - path (<code>str | Path | tuple[str, str]</code>): Filesystem path or package tuple - prefix (<code>str | None</code>): Override the prefix from manifest.json</p>"},{"location":"guides/app-hosting/#multiple-apps-with_apps","title":"Multiple Apps: <code>.with_apps()</code>","text":"<p>Auto-discover and mount all apps in a directory or package:</p> <pre><code># Discover from filesystem directory\n.with_apps(\"./apps\")\n\n# Discover from Python package\n.with_apps((\"mycompany.apps\", \"webapps\"))\n</code></pre> <p>Parameters: - path (<code>str | Path | tuple[str, str]</code>): Directory path or package tuple</p>"},{"location":"guides/app-hosting/#path-types","title":"Path Types","text":""},{"location":"guides/app-hosting/#filesystem-paths","title":"Filesystem Paths","text":"<p>Paths are resolved relative to the current working directory (where the service runs):</p> <pre><code># Relative paths\n.with_app(\"./apps/dashboard\")\n.with_app(\"apps/dashboard\")\n\n# Absolute paths\n.with_app(\"/opt/myproject/apps/dashboard\")\n</code></pre> <p>Project structure: <pre><code>myproject/\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u251c\u2500\u2500 manifest.json\n\u2502   \u2502   \u2514\u2500\u2500 index.html\n\u2502   \u2514\u2500\u2500 admin/\n\u2502       \u251c\u2500\u2500 manifest.json\n\u2502       \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre></p> <p>Usage in main.py: <pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_app(\"./apps/dashboard\")      # Single app\n    .with_apps(\"./apps\")               # All apps\n    .build()\n)\n</code></pre></p>"},{"location":"guides/app-hosting/#package-resources","title":"Package Resources","text":"<p>Bundle apps with your Python package using tuple syntax <code>(package_name, subpath)</code>:</p> <pre><code># Single app from package\n.with_app((\"mycompany.apps\", \"dashboard\"))\n\n# All apps from package directory\n.with_apps((\"mycompany.apps\", \"webapps\"))\n</code></pre> <p>Package structure: <pre><code>mycompany/\n  apps/\n    webapps/\n      dashboard/\n        manifest.json\n        index.html\n      admin/\n        manifest.json\n        index.html\n</code></pre></p> <p>Why use package resources? - Ship default apps with your library - Version apps alongside Python code - Distribute apps via PyPI - Easy deployment (no external files needed)</p>"},{"location":"guides/app-hosting/#override-semantics","title":"Override Semantics","text":""},{"location":"guides/app-hosting/#multiple-app-calls","title":"Multiple App Calls","text":"<p>Calling <code>.with_app()</code> and <code>.with_apps()</code> multiple times is cumulative - all apps from all calls are combined:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_apps(\"./apps/set1\")      # Discover apps from set1/\n    .with_apps(\"./apps/set2\")      # Discover apps from set2/\n    .with_app(\"./apps/custom\")     # Add single custom app\n    .build()\n)\n</code></pre> <p>All apps from both directories plus the custom app will be mounted.</p> <p>This works for all path types:</p> <pre><code># Filesystem paths\n.with_apps(\"./apps/dir1\").with_apps(\"./apps/dir2\")\n\n# Package resources\n.with_apps((\"pkg1\", \"apps\")).with_apps((\"pkg2\", \"apps\"))\n\n# Mixed approaches\n.with_apps(\"./apps\").with_apps((\"mypackage\", \"bundled_apps\"))\n</code></pre>"},{"location":"guides/app-hosting/#duplicate-prefixes","title":"Duplicate Prefixes","text":"<p>When multiple apps use the same prefix, the last one wins:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_app(\"apps/dashboard\")                    # Mounts at /dashboard\n    .with_app(\"apps/better-dashboard\", prefix=\"/dashboard\")  # Replaces first\n    .build()\n)\n</code></pre> <p>This applies to duplicates from multiple <code>.with_app()</code> or <code>.with_apps()</code> calls as well. If <code>./apps/set1</code> contains a dashboard at <code>/dashboard</code> and <code>./apps/set2</code> also contains a dashboard at <code>/dashboard</code>, the one from <code>set2</code> wins (assuming <code>set2</code> was added last).</p> <p>The service logs a warning when an app overrides another: <pre><code>app.prefix.override prefix=/dashboard replaced_app=Dashboard new_app=BetterDashboard\n</code></pre></p>"},{"location":"guides/app-hosting/#landing-page-override","title":"Landing Page Override","text":"<p><code>.with_landing_page()</code> internally mounts a built-in app at <code>/</code>. You can override it:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_landing_page()                  # Built-in landing page at /\n    .with_app(\"apps/custom-home\", prefix=\"/\")  # Replace with custom\n    .build()\n)\n</code></pre>"},{"location":"guides/app-hosting/#root-apps","title":"Root Apps","text":"<p>Apps can mount at root (<code>/</code>), but be aware of a limitation:</p> <p>Root mounts intercept trailing slash redirects. Use exact paths for API endpoints: - Correct: <code>/api/v1/configs</code> - Incorrect: <code>/api/v1/configs/</code> (may return 404)</p> <p>API routes always take precedence over apps (routes are registered first).</p>"},{"location":"guides/app-hosting/#restrictions","title":"Restrictions","text":""},{"location":"guides/app-hosting/#blocked-prefixes","title":"Blocked Prefixes","text":"<p>Apps cannot mount at <code>/api</code> or <code>/api/**</code> (reserved for API endpoints):</p> <pre><code># This will raise ValueError\n.with_app(\"apps/api-dashboard\", prefix=\"/api/dashboard\")\n</code></pre>"},{"location":"guides/app-hosting/#prefix-format","title":"Prefix Format","text":"<p>Prefixes must: - Start with <code>/</code> - Not contain <code>..</code> (path traversal protection) - Be valid URL paths</p> <pre><code># Valid prefixes\n.with_app(\"apps/dashboard\", prefix=\"/dashboard\")\n.with_app(\"apps/admin\", prefix=\"/admin/panel\")\n.with_app(\"apps/home\", prefix=\"/\")\n\n# Invalid prefixes\n.with_app(\"apps/bad\", prefix=\"dashboard\")    # Missing leading /\n.with_app(\"apps/bad\", prefix=\"/../../etc\")   # Path traversal\n</code></pre>"},{"location":"guides/app-hosting/#testing-apps","title":"Testing Apps","text":""},{"location":"guides/app-hosting/#with-curl","title":"With cURL","text":"<pre><code># Test app is accessible\ncurl http://localhost:8000/dashboard/\n\n# Test app assets\ncurl http://localhost:8000/dashboard/style.css\n\n# Test API still works\ncurl http://localhost:8000/api/v1/configs\n</code></pre>"},{"location":"guides/app-hosting/#with-browser","title":"With Browser","text":"<ol> <li>Start your service: <code>fastapi dev your_file.py</code></li> <li>Navigate to app: http://localhost:8000/dashboard</li> <li>Check browser console for errors</li> <li>Verify API requests work: http://localhost:8000/api/v1/configs</li> </ol>"},{"location":"guides/app-hosting/#in-tests","title":"In Tests","text":"<pre><code>from starlette.testclient import TestClient\n\ndef test_app_is_accessible():\n    with TestClient(app) as client:\n        # Test app loads\n        response = client.get(\"/dashboard/\")\n        assert response.status_code == 200\n        assert b\"Dashboard\" in response.content\n\n        # Test app assets\n        response = client.get(\"/dashboard/style.css\")\n        assert response.status_code == 200\n\n        # Test API still works\n        response = client.get(\"/api/v1/configs\")\n        assert response.status_code == 200\n</code></pre>"},{"location":"guides/app-hosting/#docker-deployment","title":"Docker Deployment","text":""},{"location":"guides/app-hosting/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.13-slim\n\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install dependencies\nRUN pip install -e .\n\n# Copy apps directory\nCOPY ./apps /app/apps\n\n# Expose port\nEXPOSE 8000\n\n# Run service\nCMD [\"fastapi\", \"run\", \"main.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"guides/app-hosting/#docker-compose","title":"Docker Compose","text":"<p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      # Mount apps directory for development\n      - ./apps:/app/apps:ro\n    environment:\n      - LOG_LEVEL=INFO\n</code></pre></p> <p>Run: <pre><code>docker compose up\n</code></pre></p> <p>Access: - App: http://localhost:8000/dashboard - API: http://localhost:8000/api/v1/configs - Docs: http://localhost:8000/docs</p>"},{"location":"guides/app-hosting/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"guides/app-hosting/#configmap-for-apps","title":"ConfigMap for Apps","text":"<p>apps-configmap.yaml: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dashboard-app\ndata:\n  manifest.json: |\n    {\n      \"name\": \"Dashboard\",\n      \"version\": \"1.0.0\",\n      \"prefix\": \"/dashboard\"\n    }\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;head&gt;&lt;title&gt;Dashboard&lt;/title&gt;&lt;/head&gt;\n      &lt;body&gt;\n        &lt;h1&gt;Dashboard&lt;/h1&gt;\n        &lt;div id=\"app\"&gt;&lt;/div&gt;\n      &lt;/body&gt;\n    &lt;/html&gt;\n</code></pre></p>"},{"location":"guides/app-hosting/#deployment","title":"Deployment","text":"<p>deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: servicekit-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: servicekit-service\n  template:\n    metadata:\n      labels:\n        app: servicekit-service\n    spec:\n      containers:\n      - name: app\n        image: your-servicekit-app:latest\n        ports:\n        - containerPort: 8000\n        volumeMounts:\n        - name: dashboard-app\n          mountPath: /app/apps/dashboard\n          readOnly: true\n      volumes:\n      - name: dashboard-app\n        configMap:\n          name: dashboard-app\n</code></pre></p> <p>service.yaml: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: servicekit-service\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8000\n  selector:\n    app: servicekit-service\n</code></pre></p>"},{"location":"guides/app-hosting/#security","title":"Security","text":""},{"location":"guides/app-hosting/#path-traversal-protection","title":"Path Traversal Protection","text":"<p>Servicekit validates all paths and rejects path traversal attempts:</p> <pre><code># All rejected with ValueError\n.with_app(\"apps/../../etc\")                          # Prefix traversal\n.with_app((\"mypackage\", \"../../../etc\"))             # Package traversal\n\n# manifest.json with path traversal also rejected:\n{\n  \"prefix\": \"/../../admin\",     # Rejected\n  \"entry\": \"../../../passwd\"    # Rejected\n}\n</code></pre>"},{"location":"guides/app-hosting/#api-protection","title":"API Protection","text":"<p>API endpoints are protected from app conflicts:</p> <ol> <li>Apps cannot mount at <code>/api</code> or <code>/api/**</code></li> <li>Apps are mounted after routes, so API routes take precedence</li> <li>Static files never override API endpoints</li> </ol>"},{"location":"guides/app-hosting/#validation","title":"Validation","text":"<ul> <li>Build-time validation: All errors detected during <code>.build()</code> (fail fast)</li> <li>Manifest validation: Pydantic validates all fields and types</li> <li>File validation: Entry files must exist before mounting</li> <li>Prefix validation: Duplicate prefixes detected and logged</li> </ul>"},{"location":"guides/app-hosting/#best-practices","title":"Best Practices","text":""},{"location":"guides/app-hosting/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Separate apps directory: Keep apps in <code>./apps</code> outside source code</li> <li>Version apps: Use semantic versioning in manifest.json</li> <li>Test locally: Run <code>fastapi dev</code> before deploying</li> <li>Use package resources: For default/bundled apps in libraries</li> <li>Document prefixes: List all app URLs in README</li> <li>Keep apps small: Under 10MB per app for fast loading</li> <li>Use CDN for assets: For production apps with large assets</li> </ul>"},{"location":"guides/app-hosting/#avoid","title":"Avoid","text":"<ul> <li>Hardcoding paths: Use relative paths, not absolute</li> <li>Path traversal: Never use <code>..</code> in paths or prefixes</li> <li>Large binaries: Don't bundle videos/large files in apps</li> <li>Duplicate prefixes: Causes confusion (service logs warnings)</li> <li>API prefix conflicts: Never mount apps at <code>/api</code></li> <li>Missing manifest: All apps must have manifest.json</li> </ul>"},{"location":"guides/app-hosting/#app-organization","title":"App Organization","text":"<pre><code>apps/\n\u251c\u2500\u2500 dashboard/          # Main dashboard\n\u2502   \u251c\u2500\u2500 manifest.json\n\u2502   \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 admin/              # Admin panel\n\u2502   \u251c\u2500\u2500 manifest.json\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 docs/               # Documentation site\n    \u251c\u2500\u2500 manifest.json\n    \u2514\u2500\u2500 index.html\n</code></pre>"},{"location":"guides/app-hosting/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"guides/app-hosting/#with-authentication","title":"With Authentication","text":"<pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_auth(\n        unauthenticated_paths=[\n            \"/health\",\n            \"/metrics\",\n            \"/\",           # Landing page (root app)\n            \"/docs\",       # API documentation\n        ]\n    )\n    .with_landing_page()    # Public landing page\n    .with_app(\"apps/admin\") # Admin panel (requires auth)\n    .build()\n)\n</code></pre>"},{"location":"guides/app-hosting/#with-system-endpoint","title":"With System Endpoint","text":"<p>Query installed apps programmatically:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_system()          # Enables /api/v1/system/apps\n    .with_apps(\"./apps\")\n    .build()\n)\n</code></pre> <p>Test: <pre><code>curl http://localhost:8000/api/v1/system/apps\n</code></pre></p> <p>Response: <pre><code>[\n  {\n    \"name\": \"Dashboard\",\n    \"version\": \"1.0.0\",\n    \"prefix\": \"/dashboard\",\n    \"description\": \"Interactive dashboard\",\n    \"author\": \"Your Name\",\n    \"entry\": \"index.html\",\n    \"is_package\": false\n  }\n]\n</code></pre></p>"},{"location":"guides/app-hosting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/app-hosting/#app-returns-404","title":"App Returns 404","text":"<p>Problem: Accessing <code>/dashboard/</code> returns 404.</p> <p>Solutions: 1. Verify app directory exists: <code>ls ./apps/dashboard</code> 2. Check manifest.json exists: <code>cat ./apps/dashboard/manifest.json</code> 3. Verify prefix matches URL: Check <code>\"prefix\"</code> field in manifest 4. Check service logs for mount messages:    <pre><code>app.mounted name=Dashboard prefix=/dashboard directory=./apps/dashboard\n</code></pre></p>"},{"location":"guides/app-hosting/#assets-not-loading","title":"Assets Not Loading","text":"<p>Problem: HTML loads but CSS/JS return 404.</p> <p>Solutions: 1. Check file paths in HTML are relative: <code>&lt;link href=\"style.css\"&gt;</code> not <code>&lt;link href=\"/style.css\"&gt;</code> 2. Verify assets exist in app directory: <code>ls ./apps/dashboard/</code> 3. Test asset URLs: <code>curl http://localhost:8000/dashboard/style.css</code></p>"},{"location":"guides/app-hosting/#manifest-validation-error","title":"Manifest Validation Error","text":"<p>Problem: Service fails with \"Invalid JSON in manifest.json\".</p> <p>Solutions: 1. Validate JSON syntax: <code>python -m json.tool manifest.json</code> 2. Check required fields: <code>name</code>, <code>version</code>, <code>prefix</code> 3. Check field types: <code>version</code> must be string, not number 4. Remove unknown fields (Pydantic rejects extras)</p>"},{"location":"guides/app-hosting/#app-not-discovered","title":"App Not Discovered","text":"<p>Problem: <code>.with_apps()</code> doesn't find the app.</p> <p>Solutions: 1. Verify directory structure: App must be in subdirectory with manifest.json 2. Check manifest.json is valid JSON 3. Review discovery logs for errors:    <pre><code>app.discovery.failed directory=./apps/broken error=\"Entry file 'index.html' not found\"\n</code></pre></p>"},{"location":"guides/app-hosting/#duplicate-prefix-warning","title":"Duplicate Prefix Warning","text":"<p>Problem: Seeing \"app.prefix.override\" warnings in logs.</p> <p>Solutions: 1. Check for multiple <code>.with_app()</code> calls with same prefix 2. Check multiple manifest.json files with same prefix 3. This is usually intentional (override), but verify it's expected</p>"},{"location":"guides/app-hosting/#api-endpoints-conflict","title":"API Endpoints Conflict","text":"<p>Problem: Cannot mount app because prefix conflicts with API.</p> <p>Solutions: 1. Use different prefix: <code>/admin</code> instead of <code>/api/admin</code> 2. API endpoints always take precedence (by design) 3. Mount apps at unique, non-API prefixes</p>"},{"location":"guides/app-hosting/#examples","title":"Examples","text":""},{"location":"guides/app-hosting/#basic-dashboard","title":"Basic Dashboard","text":"<p>apps/dashboard/manifest.json: <pre><code>{\n  \"name\": \"Dashboard\",\n  \"version\": \"1.0.0\",\n  \"prefix\": \"/dashboard\",\n  \"description\": \"Real-time metrics dashboard\"\n}\n</code></pre></p> <p>apps/dashboard/index.html: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Dashboard&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Dashboard&lt;/h1&gt;\n    &lt;div id=\"metrics\"&gt;&lt;/div&gt;\n    &lt;script src=\"script.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>apps/dashboard/script.js: <pre><code>// Fetch data from your API\nfetch('/api/v1/configs')\n  .then(response =&gt; response.json())\n  .then(data =&gt; {\n    document.getElementById('metrics').innerHTML =\n      `&lt;pre&gt;${JSON.stringify(data, null, 2)}&lt;/pre&gt;`;\n  });\n</code></pre></p>"},{"location":"guides/app-hosting/#multi-app-service","title":"Multi-App Service","text":"<pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Multi-App Service\"))\n    .with_health()\n    .with_system()\n    .with_landing_page()           # Built-in landing at /\n    .with_apps(\"./apps\")           # Dashboard, admin, docs\n    .build()\n)\n</code></pre> <p>URLs: - <code>/</code> - Landing page - <code>/dashboard</code> - Main dashboard - <code>/admin</code> - Admin panel - <code>/docs</code> - API documentation - <code>/api/v1/*</code> - API endpoints</p>"},{"location":"guides/app-hosting/#next-steps","title":"Next Steps","text":"<ul> <li>SPA Support: Apps use <code>html=True</code> mode (serves index.html for directories)</li> <li>Custom Landing Page: Override built-in with <code>.with_app(..., prefix=\"/\")</code></li> <li>Package Apps: Distribute apps via PyPI with your library</li> <li>Authentication: Combine with <code>.with_auth()</code> for protected apps</li> </ul>"},{"location":"guides/app-hosting/#further-reading","title":"Further Reading","text":"<p>For more examples, see: - <code>examples/app_hosting_api.py</code> - Complete app hosting example - <code>examples/apps/sample-dashboard/</code> - Sample dashboard app - <code>designs/app-system.md</code> - Technical design document - <code>CLAUDE.md</code> - Development guide with app system section</p>"},{"location":"guides/authentication/","title":"Authentication","text":"<p>Servicekit provides simple API key authentication for service-to-service communication in Docker Compose and Kubernetes environments.</p>"},{"location":"guides/authentication/#quick-start","title":"Quick Start","text":""},{"location":"guides/authentication/#environment-variables-recommended-for-production","title":"Environment Variables (Recommended for Production)","text":"<p>The simplest and most secure approach for production deployments:</p> <pre><code>from servicekit.api import BaseBaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseBaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_auth()  # Reads from SERVICEKIT_API_KEYS environment variable\n    .with_database(\"sqlite+aiosqlite:///./data.db\")\n    .build()\n)\n</code></pre> <p>Set the environment variable:</p> <pre><code>export SERVICEKIT_API_KEYS=\"sk_prod_abc123,sk_prod_xyz789\"\nfastapi run your_file.py\n</code></pre>"},{"location":"guides/authentication/#docker-secrets-most-secure-for-production","title":"Docker Secrets (Most Secure for Production)","text":"<p>For Docker Swarm or Kubernetes deployments:</p> <pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_auth(api_key_file=\"/run/secrets/api_keys\")\n    .build()\n)\n</code></pre> <p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  app:\n    image: your-app\n    secrets:\n      - api_keys\n\nsecrets:\n  api_keys:\n    file: ./secrets/api_keys.txt\n</code></pre></p> <p>secrets/api_keys.txt: <pre><code>sk_prod_abc123\nsk_prod_xyz789\n</code></pre></p>"},{"location":"guides/authentication/#direct-keys-development-only","title":"Direct Keys (Development Only)","text":"<p>WARNING: Never use this in production!</p> <pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_auth(api_keys=[\"sk_dev_test123\"])  # NOT for production\n    .build()\n)\n</code></pre>"},{"location":"guides/authentication/#configuration-options","title":"Configuration Options","text":"<p>The <code>.with_auth()</code> method accepts these parameters:</p> <pre><code>.with_auth(\n    api_keys=None,                      # Direct list (dev only)\n    api_key_file=None,                  # File path (Docker secrets)\n    env_var=\"SERVICEKIT_API_KEYS\",         # Environment variable name\n    header_name=\"X-API-Key\",            # HTTP header for API key\n    unauthenticated_paths=None,         # Paths without auth\n)\n</code></pre>"},{"location":"guides/authentication/#priority","title":"Priority","text":"<p>Servicekit uses the first non-None value in this order: 1. <code>api_keys</code> (direct list) 2. <code>api_key_file</code> (file path) 3. <code>env_var</code> (environment variable, default: <code>SERVICEKIT_API_KEYS</code>)</p>"},{"location":"guides/authentication/#parameters","title":"Parameters","text":"<ul> <li>api_keys (<code>List[str] | None</code>): Direct list of API keys. Only for examples and local development.</li> <li>api_key_file (<code>str | None</code>): Path to file containing keys (one per line). For Docker secrets.</li> <li>env_var (<code>str</code>): Environment variable name to read keys from. Default: <code>SERVICEKIT_API_KEYS</code>.</li> <li>header_name (<code>str</code>): HTTP header name for API key. Default: <code>X-API-Key</code>.</li> <li>unauthenticated_paths (<code>List[str] | None</code>): Paths that don't require authentication.</li> </ul>"},{"location":"guides/authentication/#key-format-conventions","title":"Key Format Conventions","text":"<p>Recommended format: <code>sk_{environment}_{random}</code></p>"},{"location":"guides/authentication/#examples","title":"Examples","text":"<pre><code>sk_prod_a1b2c3d4e5f6g7h8     # Production\nsk_staging_x1y2z3a4b5c6d7e8  # Staging\nsk_dev_test123               # Development\n</code></pre>"},{"location":"guides/authentication/#why-this-format","title":"Why This Format?","text":"<ul> <li>sk_ prefix: Easily identifiable as a secret key</li> <li>environment: Know which environment the key belongs to</li> <li>random: Unique identifier (16+ characters recommended)</li> </ul> <p>Servicekit logs only the first 7 characters (<code>sk_prod_****</code>) for security.</p>"},{"location":"guides/authentication/#key-rotation","title":"Key Rotation","text":"<p>To rotate API keys without downtime:</p> <ol> <li>Add new key (keep old key active)</li> <li>Update clients to use new key</li> <li>Remove old key after all clients updated</li> </ol>"},{"location":"guides/authentication/#example-rotation","title":"Example Rotation","text":"<pre><code># Step 1: Both keys active\nexport SERVICEKIT_API_KEYS=\"sk_prod_old123,sk_prod_new456\"\n\n# Deploy and verify service restarts\nfastapi run your_file.py\n\n# Step 2: Update all clients to use sk_prod_new456\n# Test that clients work with new key\n\n# Step 3: Remove old key (after confirming all clients updated)\nexport SERVICEKIT_API_KEYS=\"sk_prod_new456\"\n\n# Restart service\nfastapi run your_file.py\n</code></pre>"},{"location":"guides/authentication/#unauthenticated-paths","title":"Unauthenticated Paths","text":"<p>By default, these paths don't require authentication:</p> <ul> <li><code>/docs</code> - Swagger UI</li> <li><code>/redoc</code> - ReDoc</li> <li><code>/openapi.json</code> - OpenAPI schema</li> <li><code>/health</code> - Health check</li> <li><code>/</code> - Landing page</li> </ul>"},{"location":"guides/authentication/#custom-unauthenticated-paths","title":"Custom Unauthenticated Paths","text":"<pre><code>.with_auth(\n    unauthenticated_paths=[\"/health\", \"/public\", \"/status\"]\n)\n</code></pre> <p>This replaces the default list. To add to the default list:</p> <pre><code>default_paths = [\"/docs\", \"/redoc\", \"/openapi.json\", \"/health\", \"/\"]\ncustom_paths = default_paths + [\"/public\", \"/status\"]\n\n.with_auth(unauthenticated_paths=custom_paths)\n</code></pre>"},{"location":"guides/authentication/#testing-authenticated-apis","title":"Testing Authenticated APIs","text":""},{"location":"guides/authentication/#with-curl","title":"With cURL","text":"<pre><code># Valid request\ncurl -H \"X-API-Key: sk_dev_test123\" http://localhost:8000/api/v1/configs\n\n# Missing key (returns 401)\ncurl http://localhost:8000/api/v1/configs\n\n# Invalid key (returns 401)\ncurl -H \"X-API-Key: invalid_key\" http://localhost:8000/api/v1/configs\n\n# Unauthenticated path (no key needed)\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"guides/authentication/#with-python-requests","title":"With Python requests","text":"<pre><code>import requests\n\nheaders = {\"X-API-Key\": \"sk_dev_test123\"}\n\n# Authenticated request\nresponse = requests.get(\n    \"http://localhost:8000/api/v1/configs\",\n    headers=headers\n)\n\n# Check response\nassert response.status_code == 200\n</code></pre>"},{"location":"guides/authentication/#with-httpx-async","title":"With httpx (async)","text":"<pre><code>import httpx\n\nheaders = {\"X-API-Key\": \"sk_dev_test123\"}\n\nasync with httpx.AsyncClient() as client:\n    response = await client.get(\n        \"http://localhost:8000/api/v1/configs\",\n        headers=headers\n    )\n    assert response.status_code == 200\n</code></pre>"},{"location":"guides/authentication/#docker-deployment","title":"Docker Deployment","text":""},{"location":"guides/authentication/#docker-compose","title":"Docker Compose","text":"<p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  servicekit-service:\n    image: your-servicekit-app\n    ports:\n      - \"8000:8000\"\n    environment:\n      # Option 1: Environment variable\n      SERVICEKIT_API_KEYS: sk_prod_abc123,sk_prod_xyz789\n\n      # Option 2: Point to secrets file\n      # SERVICEKIT_API_KEY_FILE: /run/secrets/api_keys\n\n    # Option 2 (continued): Mount secrets\n    # secrets:\n    #   - api_keys\n\n# secrets:\n#   api_keys:\n#     file: ./secrets/api_keys.txt\n</code></pre></p> <p>secrets/api_keys.txt: <pre><code># Production API keys\n# One key per line, comments allowed\nsk_prod_abc123\nsk_prod_xyz789\n</code></pre></p> <p>.gitignore: <pre><code># Never commit secrets!\nsecrets/api_keys.txt\n</code></pre></p> <p>secrets/api_keys.txt.example: <pre><code># Example API keys file\n# Copy to api_keys.txt and replace with real keys\nsk_prod_example1\nsk_prod_example2\n</code></pre></p>"},{"location":"guides/authentication/#docker-swarm","title":"Docker Swarm","text":"<pre><code># Create secret\necho -e \"sk_prod_abc123\\nsk_prod_xyz789\" | \\\n  docker secret create servicekit_api_keys -\n\n# Deploy service\ndocker service create \\\n  --name my-servicekit-service \\\n  --secret servicekit_api_keys \\\n  -e SERVICEKIT_API_KEY_FILE=/run/secrets/servicekit_api_keys \\\n  -p 8000:8000 \\\n  your-servicekit-app\n</code></pre>"},{"location":"guides/authentication/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>secret.yaml: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: servicekit-api-keys\ntype: Opaque\nstringData:\n  api_keys.txt: |\n    sk_prod_abc123\n    sk_prod_xyz789\n</code></pre></p> <p>deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: servicekit-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: servicekit-service\n  template:\n    metadata:\n      labels:\n        app: servicekit-service\n    spec:\n      containers:\n      - name: app\n        image: your-servicekit-app\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SERVICEKIT_API_KEY_FILE\n          value: /etc/secrets/api_keys.txt\n        volumeMounts:\n        - name: api-keys\n          mountPath: /etc/secrets\n          readOnly: true\n      volumes:\n      - name: api-keys\n        secret:\n          secretName: servicekit-api-keys\n</code></pre></p>"},{"location":"guides/authentication/#logging","title":"Logging","text":"<p>Servicekit automatically logs authentication events with masked keys for security.</p>"},{"location":"guides/authentication/#successful-authentication","title":"Successful Authentication","text":"<pre><code>{\n  \"event\": \"auth.success\",\n  \"key_prefix\": \"sk_prod\",\n  \"path\": \"/api/v1/configs\"\n}\n</code></pre>"},{"location":"guides/authentication/#failed-authentication","title":"Failed Authentication","text":"<pre><code>{\n  \"event\": \"auth.invalid_key\",\n  \"key_prefix\": \"sk_unkn\",\n  \"path\": \"/api/v1/configs\",\n  \"method\": \"GET\"\n}\n</code></pre>"},{"location":"guides/authentication/#missing-key","title":"Missing Key","text":"<pre><code>{\n  \"event\": \"auth.missing_key\",\n  \"path\": \"/api/v1/configs\",\n  \"method\": \"GET\"\n}\n</code></pre> <p>Only the first 7 characters of keys are logged. Full keys are never logged.</p>"},{"location":"guides/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"guides/authentication/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Use environment variables or Docker secrets in production</li> <li>Use <code>sk_env_random</code> format for easy identification in logs</li> <li>Rotate keys regularly (quarterly recommended)</li> <li>Use different keys for different services/environments</li> <li>Keep <code>.env</code> files in <code>.gitignore</code></li> <li>Use minimum 16 characters for key randomness</li> <li>Monitor authentication logs for failed attempts</li> </ul>"},{"location":"guides/authentication/#avoid","title":"Avoid","text":"<ul> <li>Committing API keys to git (use <code>.gitignore</code>)</li> <li>Using <code>api_keys=</code> parameter in production (only for examples)</li> <li>Reusing keys across environments (dev/staging/prod)</li> <li>Using weak/short keys (minimum 16 characters)</li> <li>Sharing keys via email/Slack (use secrets management)</li> <li>Hardcoding keys in source code</li> </ul>"},{"location":"guides/authentication/#error-responses","title":"Error Responses","text":"<p>All authentication errors follow RFC 9457 Problem Details format.</p>"},{"location":"guides/authentication/#missing-api-key-401","title":"Missing API Key (401)","text":"<pre><code>{\n  \"type\": \"urn:servicekit:error:unauthorized\",\n  \"title\": \"Unauthorized\",\n  \"status\": 401,\n  \"detail\": \"Missing authentication header: X-API-Key\",\n  \"instance\": \"/api/v1/configs\"\n}\n</code></pre>"},{"location":"guides/authentication/#invalid-api-key-401","title":"Invalid API Key (401)","text":"<pre><code>{\n  \"type\": \"urn:servicekit:error:unauthorized\",\n  \"title\": \"Unauthorized\",\n  \"status\": 401,\n  \"detail\": \"Invalid API key\",\n  \"instance\": \"/api/v1/configs\"\n}\n</code></pre>"},{"location":"guides/authentication/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/authentication/#custom-header-name","title":"Custom Header Name","text":"<pre><code>.with_auth(\n    header_name=\"X-Custom-API-Key\"\n)\n</code></pre> <p>Test with: <pre><code>curl -H \"X-Custom-API-Key: sk_dev_test123\" http://localhost:8000/api/v1/configs\n</code></pre></p>"},{"location":"guides/authentication/#multiple-environments","title":"Multiple Environments","text":"<p>Development: <pre><code># dev.py\n.with_auth(api_keys=[\"sk_dev_test123\"])\n</code></pre></p> <p>Production: <pre><code># prod.py\n.with_auth()  # Reads from SERVICEKIT_API_KEYS env var\n</code></pre></p>"},{"location":"guides/authentication/#service-to-service-communication","title":"Service-to-Service Communication","text":"<pre><code># Service A (client)\nimport httpx\n\nheaders = {\"X-API-Key\": os.getenv(\"SERVICE_B_API_KEY\")}\nasync with httpx.AsyncClient() as client:\n    response = await client.get(\n        \"http://service-b:8000/api/v1/data\",\n        headers=headers\n    )\n\n# Service B (server)\napp = BaseServiceBuilder(info=info).with_auth().build()\n</code></pre>"},{"location":"guides/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/authentication/#no-api-keys-configured-error","title":"\"No API keys configured\" Error","text":"<p>Problem: Service fails to start with error message.</p> <p>Solution: Ensure you've provided keys via one of these methods: <pre><code># Environment variable\nexport SERVICEKIT_API_KEYS=\"sk_dev_test123\"\n\n# Or in Python (dev only)\n.with_auth(api_keys=[\"sk_dev_test123\"])\n\n# Or via file\n.with_auth(api_key_file=\"/path/to/keys.txt\")\n</code></pre></p>"},{"location":"guides/authentication/#401-unauthorized-on-health-check","title":"401 Unauthorized on Health Check","text":"<p>Problem: Health check returns 401 instead of 200.</p> <p>Solution: Health checks are unauthenticated by default. If you customized <code>unauthenticated_paths</code>, add <code>/health</code> back:</p> <pre><code>.with_auth(\n    unauthenticated_paths=[\n        \"/docs\", \"/redoc\", \"/openapi.json\",\n        \"/health\",  # Add this\n        \"/\", \"/custom/path\"\n    ]\n)\n</code></pre>"},{"location":"guides/authentication/#keys-not-loading-from-file","title":"Keys Not Loading from File","text":"<p>Problem: <code>FileNotFoundError: API key file not found</code></p> <p>Solution: 1. Verify file path is absolute: <code>/run/secrets/api_keys</code> (not relative) 2. Check file exists: <code>ls -la /run/secrets/api_keys</code> 3. Verify container has access (Docker secrets mount at <code>/run/secrets/</code>)</p>"},{"location":"guides/authentication/#keys-not-loading-from-environment","title":"Keys Not Loading from Environment","text":"<p>Problem: \"No API keys found in SERVICEKIT_API_KEYS\"</p> <p>Solution: 1. Verify env var is set: <code>echo $SERVICEKIT_API_KEYS</code> 2. Check for typos in variable name 3. Ensure keys are comma-separated: <code>key1,key2,key3</code> 4. No spaces around commas: <code>sk_dev_1,sk_dev_2</code> (not <code>sk_dev_1, sk_dev_2</code>)</p>"},{"location":"guides/authentication/#next-steps","title":"Next Steps","text":"<ul> <li>ML Services: Combine with <code>.with_ml()</code> for authenticated ML endpoints</li> <li>Rate Limiting: See roadmap for per-key rate limiting (P2)</li> <li>Key Scoping: See roadmap for endpoint-specific keys (P2)</li> <li>Monitoring: Track authentication metrics with Prometheus (P1)</li> </ul> <p>For more examples, see: - <code>examples/auth_basic.py</code> - Basic authentication example - <code>CLAUDE.md</code> - Comprehensive development guide</p>"},{"location":"guides/dataframe/","title":"DataFrame Data Interchange","text":"<p>Servicekit provides a universal DataFrame class for seamless data interchange between different data libraries (pandas, polars, xarray) and file formats (CSV, Parquet). It's designed to be lightweight, framework-agnostic, and easy to use in API services.</p>"},{"location":"guides/dataframe/#quick-start","title":"Quick Start","text":""},{"location":"guides/dataframe/#basic-usage","title":"Basic Usage","text":"<pre><code>from servicekit.data import DataFrame\n\n# Create from dictionary\ndf = DataFrame.from_dict({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"NYC\", \"SF\", \"LA\"]\n})\n\n# Inspect data\nprint(df.shape)  # (3, 3)\nprint(df.head(2))\n\n# Convert to other libraries\npandas_df = df.to_pandas()\npolars_df = df.to_polars()\n</code></pre>"},{"location":"guides/dataframe/#in-fastapi-services","title":"In FastAPI Services","text":"<pre><code>from fastapi import FastAPI, UploadFile\nfrom fastapi.responses import Response\nfrom servicekit.data import DataFrame\n\napp = FastAPI()\n\n@app.post(\"/data/$upload\")\nasync def upload_csv(file: UploadFile):\n    \"\"\"Accept CSV upload and process.\"\"\"\n    content = await file.read()\n    df = DataFrame.from_csv(csv_string=content.decode())\n\n    # Process data\n    df = df.select([\"name\", \"age\"]).head(100)\n\n    return {\"rows\": df.shape[0], \"columns\": df.columns}\n\n@app.get(\"/data/$download\")\nasync def download_csv():\n    \"\"\"Export data as CSV.\"\"\"\n    df = get_data()  # Your data source\n    csv_data = df.to_csv()\n    return Response(content=csv_data, media_type=\"text/csv\")\n</code></pre>"},{"location":"guides/dataframe/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/dataframe/#data-structure","title":"Data Structure","text":"<p>DataFrame uses a simple columnar structure:</p> <pre><code>df = DataFrame(\n    columns=[\"name\", \"age\"],\n    data=[\n        [\"Alice\", 25],\n        [\"Bob\", 30]\n    ]\n)\n</code></pre> <ul> <li>columns: List of column names (strings)</li> <li>data: List of rows, where each row is a list of values</li> <li>Type-agnostic: Values can be any Python type</li> </ul>"},{"location":"guides/dataframe/#design-principles","title":"Design Principles","text":"<ul> <li>Lightweight: No required dependencies beyond Pydantic</li> <li>Framework-agnostic: Works with any Python environment</li> <li>Lazy imports: Optional libraries loaded only when needed</li> <li>Immutable: Methods return new DataFrames (no in-place modification)</li> <li>API consistency: All methods follow <code>from_X()</code> / <code>to_X()</code> pattern</li> </ul>"},{"location":"guides/dataframe/#creating-dataframes","title":"Creating DataFrames","text":""},{"location":"guides/dataframe/#from-dictionary","title":"From Dictionary","text":"<pre><code># Column-oriented (dict of lists)\ndf = DataFrame.from_dict({\n    \"name\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 30]\n})\n</code></pre>"},{"location":"guides/dataframe/#from-records","title":"From Records","text":"<pre><code># Row-oriented (list of dicts)\ndf = DataFrame.from_records([\n    {\"name\": \"Alice\", \"age\": 25},\n    {\"name\": \"Bob\", \"age\": 30}\n])\n</code></pre>"},{"location":"guides/dataframe/#from-csv","title":"From CSV","text":"<pre><code># From file\ndf = DataFrame.from_csv(\"data.csv\")\n\n# From string\ncsv_string = \"name,age\\nAlice,25\\nBob,30\"\ndf = DataFrame.from_csv(csv_string=csv_string)\n\n# Custom delimiter\ndf = DataFrame.from_csv(\"data.tsv\", delimiter=\"\\t\")\n\n# Without header\ndf = DataFrame.from_csv(\"data.csv\", has_header=False)\n# Generates columns: col_0, col_1, ...\n</code></pre>"},{"location":"guides/dataframe/#from-other-libraries","title":"From Other Libraries","text":"<pre><code># From pandas\nimport pandas as pd\npandas_df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf = DataFrame.from_pandas(pandas_df)\n\n# From polars\nimport polars as pl\npolars_df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf = DataFrame.from_polars(polars_df)\n\n# From xarray (2D only)\nimport xarray as xr\ndata_array = xr.DataArray([[1, 2], [3, 4]])\ndf = DataFrame.from_xarray(data_array)\n</code></pre>"},{"location":"guides/dataframe/#exporting-dataframes","title":"Exporting DataFrames","text":""},{"location":"guides/dataframe/#to-csv","title":"To CSV","text":"<pre><code># To file\ndf.to_csv(\"output.csv\")\n\n# To string\ncsv_string = df.to_csv()\n\n# Without header\ndf.to_csv(\"output.csv\", include_header=False)\n\n# Custom delimiter\ndf.to_csv(\"output.tsv\", delimiter=\"\\t\")\n</code></pre>"},{"location":"guides/dataframe/#to-dictionary","title":"To Dictionary","text":"<pre><code># As dict of lists (default)\ndata = df.to_dict(orient=\"list\")\n# {\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]}\n\n# As list of records\ndata = df.to_dict(orient=\"records\")\n# [{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]\n\n# As dict of dicts\ndata = df.to_dict(orient=\"dict\")\n# {\"name\": {0: \"Alice\", 1: \"Bob\"}, \"age\": {0: 25, 1: 30}}\n</code></pre>"},{"location":"guides/dataframe/#to-other-libraries","title":"To Other Libraries","text":"<pre><code># To pandas\npandas_df = df.to_pandas()\n\n# To polars\npolars_df = df.to_polars()\n</code></pre>"},{"location":"guides/dataframe/#data-inspection","title":"Data Inspection","text":""},{"location":"guides/dataframe/#properties","title":"Properties","text":"<pre><code># Shape (rows, columns)\nprint(df.shape)  # (100, 5)\n\n# Number of rows\nprint(df.shape[0])  # 100\nprint(len(df))  # 100 - can also use len()\n\n# Number of columns\nprint(df.shape[1])  # 5\n\n# Check if empty\nprint(df.empty)  # False\n\n# Number of dimensions (always 2)\nprint(df.ndim)  # 2\n\n# Total elements\nprint(df.size)  # 500 (100 * 5)\n</code></pre>"},{"location":"guides/dataframe/#viewing-data","title":"Viewing Data","text":"<pre><code># First 5 rows (default)\ndf.head()\n\n# First n rows\ndf.head(10)\n\n# Last 5 rows (default)\ndf.tail()\n\n# Last n rows\ndf.tail(10)\n\n# Negative indexing (pandas-style)\ndf.head(-3)  # All except last 3 rows\ndf.tail(-3)  # All except first 3 rows\n</code></pre>"},{"location":"guides/dataframe/#random-sampling","title":"Random Sampling","text":"<pre><code># Sample n rows\nsample = df.sample(n=100)\n\n# Sample fraction\nsample = df.sample(frac=0.1)  # 10% of rows\n\n# Reproducible sampling\nsample = df.sample(n=50, random_state=42)\n</code></pre>"},{"location":"guides/dataframe/#iteration","title":"Iteration","text":"<pre><code># Iterate over rows as dictionaries\nfor row in df:\n    print(row)  # {'name': 'Alice', 'age': 25}\n\n# Get number of rows with len()\nnum_rows = len(df)\n\n# Use in list comprehensions\nnames = [row['name'] for row in df]\n</code></pre>"},{"location":"guides/dataframe/#column-operations","title":"Column Operations","text":""},{"location":"guides/dataframe/#accessing-columns","title":"Accessing Columns","text":"<pre><code># Get column values as list\nages = df.get_column(\"age\")  # [25, 30, 35]\nages = df[\"age\"]  # Same using [] syntax\n\n# Select multiple columns as DataFrame\ndf_subset = df[[\"name\", \"age\"]]\ndf_subset = df.select([\"name\", \"age\"])  # Equivalent\n</code></pre>"},{"location":"guides/dataframe/#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf_subset = df.select([\"name\", \"age\"])\n\n# Single column\ndf_single = df.select([\"age\"])\n</code></pre>"},{"location":"guides/dataframe/#dropping-columns","title":"Dropping Columns","text":"<pre><code># Drop specific columns\ndf_clean = df.drop([\"temp_column\", \"debug_field\"])\n\n# Drop multiple\ndf_clean = df.drop([\"col1\", \"col2\", \"col3\"])\n</code></pre>"},{"location":"guides/dataframe/#renaming-columns","title":"Renaming Columns","text":"<pre><code># Rename specific columns\ndf_renamed = df.rename({\n    \"old_name\": \"new_name\",\n    \"user_id\": \"id\"\n})\n\n# Partial rename (other columns unchanged)\ndf_renamed = df.rename({\"age\": \"years\"})\n\n# Alternative: use rename_columns() (same behavior)\ndf_renamed = df.rename_columns({\"age\": \"years\"})\n</code></pre> <p>The <code>rename_columns()</code> method is an alias for <code>rename()</code>, provided for improved code readability and discoverability.</p>"},{"location":"guides/dataframe/#validation-and-type-inference","title":"Validation and Type Inference","text":""},{"location":"guides/dataframe/#structure-validation","title":"Structure Validation","text":"<pre><code># Validate DataFrame structure\ntry:\n    df.validate_structure()\n    print(\"DataFrame is valid\")\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n\n# Checks performed:\n# - All rows have same length as columns\n# - Column names are unique\n# - No empty column names\n</code></pre>"},{"location":"guides/dataframe/#type-inference","title":"Type Inference","text":"<pre><code># Infer column data types\ntypes = df.infer_types()\nprint(types)\n# {\"age\": \"int\", \"name\": \"str\", \"score\": \"float\"}\n\n# Supported types:\n# - \"int\": All integers\n# - \"float\": All floats (or mix of int/float)\n# - \"str\": All strings\n# - \"bool\": All booleans\n# - \"null\": All None values\n# - \"mixed\": Multiple different types\n</code></pre>"},{"location":"guides/dataframe/#null-detection","title":"Null Detection","text":"<pre><code># Check for None values per column\nnulls = df.has_nulls()\nprint(nulls)\n# {\"age\": False, \"email\": True, \"phone\": True}\n\n# Use for data quality checks\nif any(nulls.values()):\n    print(\"Warning: DataFrame contains null values\")\n</code></pre>"},{"location":"guides/dataframe/#sorting-and-analytics","title":"Sorting and Analytics","text":""},{"location":"guides/dataframe/#sorting","title":"Sorting","text":"<pre><code># Sort by column (ascending)\ndf_sorted = df.sort(\"age\")\n\n# Sort descending\ndf_sorted = df.sort(\"score\", ascending=False)\n\n# None values always sort to the end\ndf_sorted = df.sort(\"nullable_column\")\n</code></pre>"},{"location":"guides/dataframe/#unique-values","title":"Unique Values","text":"<pre><code># Get unique values from a column\ncategories = df.unique(\"category\")\n# ['A', 'B', 'C'] - preserves order of first appearance\n\n# Count unique values\nnum_unique = len(df.unique(\"category\"))\n</code></pre>"},{"location":"guides/dataframe/#value-counts","title":"Value Counts","text":"<pre><code># Count occurrences of each value\ncounts = df.value_counts(\"category\")\n# {'A': 3, 'B': 2, 'C': 1}\n\n# Find most common value\nmost_common = max(counts, key=counts.get)\n\n# Get distribution\ntotal = len(df)\ndistribution = {k: v/total for k, v in counts.items()}\n</code></pre>"},{"location":"guides/dataframe/#json-support","title":"JSON Support","text":""},{"location":"guides/dataframe/#creating-from-json","title":"Creating from JSON","text":"<pre><code># From JSON array of objects\njson_data = '[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]'\ndf = DataFrame.from_json(json_data)\n\n# From API response\nimport requests\nresponse = requests.get(\"https://api.example.com/data\")\ndf = DataFrame.from_json(response.text)\n</code></pre>"},{"location":"guides/dataframe/#exporting-to-json","title":"Exporting to JSON","text":"<pre><code># As array of objects (records format)\njson_str = df.to_json(orient=\"records\")\n# '[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]'\n\n# As object with arrays (columns format)\njson_str = df.to_json(orient=\"columns\")\n# '{\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]}'\n\n# For API responses\nfrom fastapi import Response\n\n@app.get(\"/data\")\nasync def get_data():\n    df = get_dataframe()\n    return Response(content=df.to_json(), media_type=\"application/json\")\n</code></pre>"},{"location":"guides/dataframe/#row-filtering-and-transformation","title":"Row Filtering and Transformation","text":""},{"location":"guides/dataframe/#filtering-rows","title":"Filtering Rows","text":"<pre><code># Filter with predicate function\nadults = df.filter(lambda row: row['age'] &gt;= 18)\n\n# Multiple conditions\nactive_adults = df.filter(lambda row: row['age'] &gt;= 18 and row['active'])\n\n# Complex filtering\nhigh_scorers = df.filter(lambda row: row['score'] &gt; 90 or (row['score'] &gt; 80 and row['bonus_eligible']))\n</code></pre>"},{"location":"guides/dataframe/#applying-transformations","title":"Applying Transformations","text":"<pre><code># Transform column values\ndf_upper = df.apply(str.upper, 'name')\n\n# Apply custom function\ndf_doubled = df.apply(lambda x: x * 2, 'price')\n\n# Apply method\ndf_rounded = df.apply(round, 'price')\n</code></pre>"},{"location":"guides/dataframe/#adding-columns","title":"Adding Columns","text":"<pre><code># Add new column\ntotal = [x + y for x, y in zip(df['price'], df['tax'])]\ndf_with_total = df.add_column('total', total)\n\n# Chain column additions\ndf_enhanced = (\n    df.add_column('total', totals)\n      .add_column('formatted', formatted_values)\n)\n</code></pre>"},{"location":"guides/dataframe/#row-operations","title":"Row Operations","text":""},{"location":"guides/dataframe/#dropping-rows","title":"Dropping Rows","text":"<pre><code># Drop rows by index\ndf_cleaned = df.drop_rows([0, 5, 10])\n\n# Drop first row\ndf_no_header = df.drop_rows([0])\n\n# Drop multiple rows\ninvalid_indices = [i for i, row in enumerate(df) if row['status'] == 'invalid']\ndf_valid = df.drop_rows(invalid_indices)\n</code></pre>"},{"location":"guides/dataframe/#removing-duplicates","title":"Removing Duplicates","text":"<pre><code># Remove duplicate rows (all columns)\ndf_unique = df.drop_duplicates()\n\n# Remove duplicates by specific columns\ndf_unique_users = df.drop_duplicates(subset=['user_id'])\n\n# Remove duplicates considering multiple columns\ndf_unique_pairs = df.drop_duplicates(subset=['category', 'product'])\n</code></pre>"},{"location":"guides/dataframe/#filling-missing-values","title":"Filling Missing Values","text":"<pre><code># Fill all None with single value\ndf_filled = df.fillna(0)\n\n# Column-specific fill values\ndf_filled = df.fillna({\n    'age': 0,\n    'name': 'Unknown',\n    'score': -1\n})\n\n# Partial filling (only specified columns)\ndf_partial = df.fillna({'age': 0})  # Other columns keep None\n</code></pre>"},{"location":"guides/dataframe/#concatenating-dataframes","title":"Concatenating DataFrames","text":"<pre><code># Stack DataFrames vertically\ndf1 = DataFrame.from_dict({'name': ['Alice'], 'age': [25]})\ndf2 = DataFrame.from_dict({'name': ['Bob'], 'age': [30]})\ncombined = df1.concat(df2)\n\n# Combine multiple DataFrames\ndfs = [df1, df2, df3]\nresult = dfs[0]\nfor df in dfs[1:]:\n    result = result.concat(df)\n</code></pre>"},{"location":"guides/dataframe/#reshaping-operations","title":"Reshaping Operations","text":"<p>The <code>melt()</code> method transforms DataFrames from wide format (many columns) to long format (fewer columns, more rows). This is essential for preparing data for analysis, visualization, or API interchange.</p>"},{"location":"guides/dataframe/#understanding-wide-vs-long-format","title":"Understanding Wide vs Long Format","text":"<p>Wide Format: Multiple measurement columns <pre><code># Example: Student grades across subjects\ndf_wide = DataFrame.from_dict({\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'math': [90, 78, 95],\n    'science': [85, 92, 89],\n    'history': [88, 81, 93]\n})\n# name    | math | science | history\n# Alice   | 90   | 85      | 88\n# Bob     | 78   | 92      | 81\n# Charlie | 95   | 89      | 93\n</code></pre></p> <p>Long Format: Single measurement column with category identifier <pre><code># Melt to long format\ndf_long = df_wide.melt(\n    id_vars=['name'],\n    value_vars=['math', 'science', 'history'],\n    var_name='subject',\n    value_name='score'\n)\n# name    | subject | score\n# Alice   | math    | 90\n# Alice   | science | 85\n# Alice   | history | 88\n# Bob     | math    | 78\n# ...\n</code></pre></p>"},{"location":"guides/dataframe/#basic-melt-usage","title":"Basic melt() Usage","text":"<pre><code>from servicekit.data import DataFrame\n\n# Create wide format data\ndf = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget'],\n    'q1_sales': [1000, 800],\n    'q2_sales': [1100, 850],\n    'q3_sales': [1200, 900]\n})\n\n# Melt to long format\nmelted = df.melt(\n    id_vars=['product'],           # Columns to keep as identifiers\n    value_vars=['q1_sales', 'q2_sales', 'q3_sales'],  # Columns to unpivot\n    var_name='quarter',            # Name for variable column\n    value_name='sales'             # Name for value column\n)\n\n# Result:\n# product | quarter   | sales\n# Widget  | q1_sales  | 1000\n# Widget  | q2_sales  | 1100\n# Widget  | q3_sales  | 1200\n# Gadget  | q1_sales  | 800\n# Gadget  | q2_sales  | 850\n# Gadget  | q3_sales  | 900\n</code></pre>"},{"location":"guides/dataframe/#melt-parameters","title":"melt() Parameters","text":"Parameter Type Default Description <code>id_vars</code> <code>list[str] \\| None</code> <code>None</code> Columns to keep as identifiers (not melted) <code>value_vars</code> <code>list[str] \\| None</code> <code>None</code> Columns to unpivot (if None, uses all non-id columns) <code>var_name</code> <code>str</code> <code>\"variable\"</code> Name for the column containing former column names <code>value_name</code> <code>str</code> <code>\"value\"</code> Name for the column containing values"},{"location":"guides/dataframe/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/dataframe/#surveyquestionnaire-data","title":"Survey/Questionnaire Data","text":"<pre><code># Wide format: each question is a column\nsurvey = DataFrame.from_dict({\n    'respondent_id': [1, 2, 3],\n    'age': [25, 30, 35],\n    'q1_rating': [5, 4, 5],\n    'q2_rating': [4, 4, 5],\n    'q3_rating': [5, 3, 5]\n})\n\n# Melt to long format for analysis\nresponses = survey.melt(\n    id_vars=['respondent_id', 'age'],\n    value_vars=['q1_rating', 'q2_rating', 'q3_rating'],\n    var_name='question',\n    value_name='rating'\n)\n\n# Now easy to analyze: average rating per question\navg_by_question = responses.groupby('question').mean('rating')\n</code></pre>"},{"location":"guides/dataframe/#time-series-data","title":"Time Series Data","text":"<pre><code># Wide format: each month is a column\nsales = DataFrame.from_dict({\n    'region': ['North', 'South', 'East'],\n    'product': ['Widget', 'Widget', 'Widget'],\n    'jan': [1000, 1200, 900],\n    'feb': [1100, 1300, 950],\n    'mar': [1200, 1400, 1000]\n})\n\n# Melt to time series format\ntime_series = sales.melt(\n    id_vars=['region', 'product'],\n    value_vars=['jan', 'feb', 'mar'],\n    var_name='month',\n    value_name='sales'\n)\n\n# Now can analyze trends over time\ntotal_by_month = time_series.groupby('month').sum('sales')\n</code></pre>"},{"location":"guides/dataframe/#api-data-standardization","title":"API Data Standardization","text":"<pre><code># API returns different metrics as columns\nsensor_data = DataFrame.from_dict({\n    'sensor_id': ['s1', 's2'],\n    'location': ['room_a', 'room_b'],\n    'temp_c': [22.5, 23.1],\n    'humidity_pct': [45, 48],\n    'pressure_kpa': [101.3, 101.2]\n})\n\n# Standardize to key-value format\nmetrics = sensor_data.melt(\n    id_vars=['sensor_id', 'location'],\n    value_vars=['temp_c', 'humidity_pct', 'pressure_kpa'],\n    var_name='metric_type',\n    value_name='metric_value'\n)\n\n# Easier to store/process uniform metric records\n</code></pre>"},{"location":"guides/dataframe/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/dataframe/#combining-melt-with-groupby","title":"Combining melt() with groupby()","text":"<pre><code># Wide format sales data\ndf = DataFrame.from_dict({\n    'region': ['North', 'North', 'South', 'South'],\n    'product': ['Widget', 'Gadget', 'Widget', 'Gadget'],\n    'q1': [1000, 800, 1200, 900],\n    'q2': [1100, 850, 1300, 950]\n})\n\n# Melt then aggregate\nmelted = df.melt(\n    id_vars=['region', 'product'],\n    value_vars=['q1', 'q2'],\n    var_name='quarter',\n    value_name='sales'\n)\n\n# Total sales by region\nregion_totals = melted.groupby('region').sum('sales')\n\n# Average sales by product\nproduct_avg = melted.groupby('product').mean('sales')\n</code></pre>"},{"location":"guides/dataframe/#filtering-after-melt","title":"Filtering After melt()","text":"<pre><code># Melt then filter for specific conditions\nmelted = df.melt(id_vars=['product'], value_vars=['q1', 'q2', 'q3', 'q4'])\n\n# Only keep quarters with sales &gt; 1000\nhigh_sales = melted.filter(lambda row: row['value'] &gt; 1000)\n\n# Group filtered results\nsummary = high_sales.groupby('product').count()\n</code></pre>"},{"location":"guides/dataframe/#melt-design-notes","title":"melt() Design Notes","text":"<ul> <li>Stdlib only: No external dependencies, pure Python implementation</li> <li>Immutable: Returns new DataFrame, original unchanged</li> <li>None values: Preserved during transformation</li> <li>Column order: Results maintain row order and value_vars order</li> <li>Validation: Raises <code>KeyError</code> for non-existent columns, <code>ValueError</code> for name conflicts</li> </ul>"},{"location":"guides/dataframe/#pivot-long-to-wide-format","title":"pivot() - Long to Wide Format","text":"<p>The <code>pivot()</code> method is the inverse of <code>melt()</code> - it transforms data from long format to wide format by spreading row values into columns.</p>"},{"location":"guides/dataframe/#basic-pivot-usage","title":"Basic pivot() Usage","text":"<pre><code>from servicekit.data import DataFrame\n\n# Long format data\ndf_long = DataFrame.from_dict({\n    'date': ['2024-01', '2024-01', '2024-02', '2024-02'],\n    'metric': ['sales', 'profit', 'sales', 'profit'],\n    'value': [1000, 200, 1100, 220]\n})\n\n# Pivot to wide format\ndf_wide = df_long.pivot(index='date', columns='metric', values='value')\n\n# Result:\n# date    | profit | sales\n# 2024-01 | 200    | 1000\n# 2024-02 | 220    | 1100\n</code></pre>"},{"location":"guides/dataframe/#pivot-parameters","title":"pivot() Parameters","text":"Parameter Type Description <code>index</code> <code>str</code> Column to use as row index in result <code>columns</code> <code>str</code> Column whose unique values become new columns <code>values</code> <code>str</code> Column containing values to fill pivoted table"},{"location":"guides/dataframe/#pivot-use-cases","title":"Pivot Use Cases","text":"<p>Report Generation: <pre><code># Student grades in long format\ndf_long = DataFrame.from_dict({\n    'student': ['Alice', 'Alice', 'Alice', 'Bob', 'Bob', 'Bob'],\n    'subject': ['math', 'science', 'history', 'math', 'science', 'history'],\n    'score': [90, 85, 88, 78, 92, 81]\n})\n\n# Pivot for report card\nreport = df_long.pivot(index='student', columns='subject', values='score')\n# student | history | math | science\n# Alice   | 88      | 90   | 85\n# Bob     | 81      | 78   | 92\n</code></pre></p> <p>Time Series Restructuring: <pre><code># API returns time series in long format\ntime_series = DataFrame.from_dict({\n    'week': [1, 1, 2, 2],\n    'day': ['mon', 'tue', 'mon', 'tue'],\n    'hours': [8, 7, 9, 8]\n})\n\n# Pivot for weekly view\nweekly = time_series.pivot(index='week', columns='day', values='hours')\n# week | mon | tue\n# 1    | 8   | 7\n# 2    | 9   | 8\n</code></pre></p>"},{"location":"guides/dataframe/#combining-melt-and-pivot","title":"Combining melt() and pivot()","text":"<p>These operations are inverses - you can round-trip data:</p> <pre><code># Start with wide format\ndf_wide = DataFrame.from_dict({\n    'id': [1, 2],\n    'a': [10, 20],\n    'b': [30, 40]\n})\n\n# Melt to long format\ndf_long = df_wide.melt(id_vars=['id'], value_vars=['a', 'b'])\n# id | variable | value\n# 1  | a        | 10\n# 1  | b        | 30\n# 2  | a        | 20\n# 2  | b        | 40\n\n# Pivot back to wide format\ndf_restored = df_long.pivot(index='id', columns='variable', values='value')\n# id | a  | b\n# 1  | 10 | 30\n# 2  | 20 | 40\n</code></pre>"},{"location":"guides/dataframe/#pivot-design-notes","title":"pivot() Design Notes","text":"<ul> <li>Duplicate detection: Raises <code>ValueError</code> if index/column combinations are duplicated</li> <li>Sparse data: Missing combinations filled with <code>None</code></li> <li>Column ordering: Result columns are sorted alphabetically</li> <li>Validation: Raises <code>KeyError</code> for non-existent column names</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#transpose-swap-rows-and-columns","title":"transpose() - Swap Rows and Columns","text":"<p>The <code>transpose()</code> method swaps rows and columns (matrix transpose). The first column becomes column headers, and column names become the first column values.</p>"},{"location":"guides/dataframe/#basic-transpose-usage","title":"Basic transpose() Usage","text":"<pre><code>from servicekit.data import DataFrame\n\n# Metrics as rows\ndf = DataFrame.from_dict({\n    'metric': ['revenue', 'profit', 'growth'],\n    '2023': [1000, 200, 0.10],\n    '2024': [1200, 250, 0.20]\n})\n\n# Transpose to have metrics as columns\ndf_t = df.transpose()\n\n# Result:\n# index | revenue | profit | growth\n# 2023  | 1000    | 200    | 0.10\n# 2024  | 1200    | 250    | 0.20\n</code></pre>"},{"location":"guides/dataframe/#transpose-use-cases","title":"Transpose Use Cases","text":"<p>Report Formatting: <pre><code># API returns quarterly metrics by region\ndata = DataFrame.from_dict({\n    'region': ['North', 'South', 'East', 'West'],\n    'Q1': [100, 200, 150, 180],\n    'Q2': [110, 210, 160, 190],\n    'Q3': [120, 220, 170, 200],\n    'Q4': [130, 230, 180, 210]\n})\n\n# Transpose for quarterly view\nquarterly = data.transpose()\n# index | North | South | East | West\n# Q1    | 100   | 200   | 150  | 180\n# Q2    | 110   | 210   | 160  | 190\n# ...\n</code></pre></p> <p>Rotating Time Series: <pre><code># Monthly sales by product (wide format)\nmonthly = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget', 'Tool'],\n    'jan': [100, 80, 60],\n    'feb': [110, 85, 65],\n    'mar': [120, 90, 70]\n})\n\n# Transpose to have products as columns\nby_month = monthly.transpose()\n# index | Widget | Gadget | Tool\n# jan   | 100    | 80     | 60\n# feb   | 110    | 85     | 65\n# mar   | 120    | 90     | 70\n</code></pre></p> <p>Preparing for Visualization: <pre><code># Data with entities as rows\nentities = DataFrame.from_dict({\n    'entity': ['Team A', 'Team B', 'Team C'],\n    'score': [85, 92, 78],\n    'rank': [2, 1, 3]\n})\n\n# Transpose for chart libraries expecting columns as series\nchart_data = entities.transpose()\n# index | Team A | Team B | Team C\n# score | 85     | 92     | 78\n# rank  | 2      | 1      | 3\n</code></pre></p>"},{"location":"guides/dataframe/#combining-with-other-operations","title":"Combining with Other Operations","text":"<p>Transpose + Filter + Transpose: <pre><code># Start with wide format\ndf = DataFrame.from_dict({\n    'id': [1, 2, 3],\n    'metric_a': [100, 200, 300],\n    'metric_b': [10, 20, 30],\n    'metric_c': [5, 15, 25]\n})\n\n# Transpose to work with metrics as rows\ndf_t = df.transpose()\n\n# Filter for specific IDs (now columns)\n# Then transpose back\nfiltered = df_t.filter(lambda row: row['1'] &gt; 50)\nresult = filtered.transpose()\n</code></pre></p> <p>Transpose for Aggregation: <pre><code># Quarterly data by product\ndf = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget'],\n    'Q1': [100, 80],\n    'Q2': [110, 85],\n    'Q3': [120, 90],\n    'Q4': [130, 95]\n})\n\n# Transpose and melt for time series analysis\ndf_t = df.transpose()\n# Now can use melt() or groupby() on transposed data\n</code></pre></p>"},{"location":"guides/dataframe/#transpose-design-notes","title":"transpose() Design Notes","text":"<ul> <li>First column as index: First column values become new column names (converted to strings)</li> <li>Column names as data: Original column names (except first) become first column values</li> <li>Immutable: Returns new DataFrame, original unchanged</li> <li>None values: Preserved during transformation</li> <li>Empty handling: Empty DataFrames return empty result</li> <li>Round-trip: Transposing twice restores original structure (with potential column name changes)</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#combining-dataframes","title":"Combining DataFrames","text":""},{"location":"guides/dataframe/#merge-database-style-joins","title":"merge() - Database-Style Joins","text":"<p>The <code>merge()</code> method combines two DataFrames using SQL-like join operations (inner, left, right, outer).</p>"},{"location":"guides/dataframe/#basic-merge-usage","title":"Basic merge() Usage","text":"<pre><code>from servicekit.data import DataFrame\n\n# Users data\nusers = DataFrame.from_dict({\n    'user_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\n# Orders data\norders = DataFrame.from_dict({\n    'user_id': [1, 1, 2],\n    'amount': [100, 150, 200]\n})\n\n# Join to get user names with orders\nresult = orders.merge(users, on='user_id', how='left')\n# user_id | amount | name\n# 1       | 100    | Alice\n# 1       | 150    | Alice\n# 2       | 200    | Bob\n</code></pre>"},{"location":"guides/dataframe/#merge-parameters","title":"merge() Parameters","text":"Parameter Type Default Description <code>other</code> <code>DataFrame</code> - DataFrame to merge with <code>on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) to join on (must exist in both) <code>how</code> <code>\"inner\" \\| \"left\" \\| \"right\" \\| \"outer\"</code> <code>\"inner\"</code> Type of join to perform <code>left_on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) from left DataFrame to join on <code>right_on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) from right DataFrame to join on <code>suffixes</code> <code>tuple[str, str]</code> <code>(\"_x\", \"_y\")</code> Suffixes for overlapping column names"},{"location":"guides/dataframe/#join-types","title":"Join Types","text":"<p>Inner Join (default): Only rows with matching keys in both DataFrames <pre><code>left = DataFrame.from_dict({'key': [1, 2, 3], 'left_val': ['a', 'b', 'c']})\nright = DataFrame.from_dict({'key': [1, 2, 4], 'right_val': ['x', 'y', 'z']})\n\nresult = left.merge(right, on='key', how='inner')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n</code></pre></p> <p>Left Join: All rows from left, matched rows from right (None if no match) <pre><code>result = left.merge(right, on='key', how='left')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 3   | c        | None\n</code></pre></p> <p>Right Join: All rows from right, matched rows from left (None if no match) <pre><code>result = left.merge(right, on='key', how='right')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 4   | None     | z\n</code></pre></p> <p>Outer Join: All rows from both DataFrames <pre><code>result = left.merge(right, on='key', how='outer')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 3   | c        | None\n# 4   | None     | z\n</code></pre></p>"},{"location":"guides/dataframe/#common-merge-patterns","title":"Common Merge Patterns","text":"<p>Enriching Data from Another Service: <pre><code># Orders from one service\norders = DataFrame.from_dict({\n    'order_id': [101, 102, 103],\n    'user_id': [1, 2, 1],\n    'amount': [50, 75, 100]\n})\n\n# User details from another service\nusers = DataFrame.from_dict({\n    'user_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'tier': ['gold', 'silver', 'gold']\n})\n\n# Enrich orders with user data\nenriched = orders.merge(users, on='user_id', how='left')\n# order_id | user_id | amount | name  | tier\n# 101      | 1       | 50     | Alice | gold\n# 102      | 2       | 75     | Bob   | silver\n# 103      | 1       | 100    | Alice | gold\n</code></pre></p> <p>Different Column Names: <pre><code>products = DataFrame.from_dict({\n    'product_id': [1, 2, 3],\n    'product_name': ['Widget', 'Gadget', 'Tool']\n})\n\nsales = DataFrame.from_dict({\n    'item_id': [1, 2, 1],\n    'quantity': [10, 5, 8]\n})\n\n# Join on different column names\nresult = sales.merge(\n    products,\n    left_on='item_id',\n    right_on='product_id',\n    how='left'\n)\n</code></pre></p> <p>Multiple Join Keys: <pre><code># Join on multiple columns for composite keys\nresult = df1.merge(df2, on=['region', 'product'], how='inner')\n</code></pre></p> <p>Handling Column Conflicts: <pre><code># Both DataFrames have 'value' column\nleft = DataFrame.from_dict({'key': [1, 2], 'value': [10, 20]})\nright = DataFrame.from_dict({'key': [1, 2], 'value': [100, 200]})\n\n# Use suffixes to distinguish\nresult = left.merge(right, on='key', suffixes=('_old', '_new'))\n# key | value_old | value_new\n# 1   | 10        | 100\n# 2   | 20        | 200\n</code></pre></p>"},{"location":"guides/dataframe/#merge-design-notes","title":"merge() Design Notes","text":"<ul> <li>One-to-many joins: Cartesian product of matching rows</li> <li>None handling: None values in keys can match None</li> <li>Validation: Raises <code>KeyError</code> for non-existent columns, <code>ValueError</code> for invalid parameters</li> <li>Performance: Uses dict-based lookup for efficient joins</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#missing-data-operations","title":"Missing Data Operations","text":""},{"location":"guides/dataframe/#detecting-missing-values","title":"Detecting Missing Values","text":"<pre><code># Check for None values (returns DataFrame of booleans)\nis_null = df.isna()\nprint(is_null.data)  # [[False, True], [True, False], ...]\n\n# Check for non-None values\nnot_null = df.notna()\n\n# Check if columns have None values\nnulls = df.has_nulls()\nprint(nulls)  # {'age': False, 'email': True}\n</code></pre>"},{"location":"guides/dataframe/#removing-missing-values","title":"Removing Missing Values","text":"<pre><code># Drop rows with any None values (default)\nclean_df = df.dropna()\n\n# Drop rows only if all values are None\ndf.dropna(axis=0, how='all')\n\n# Drop columns with any None values\ndf.dropna(axis=1, how='any')\n\n# Drop columns only if all values are None\ndf.dropna(axis=1, how='all')\n</code></pre>"},{"location":"guides/dataframe/#filling-missing-values_1","title":"Filling Missing Values","text":"<pre><code># Fill all None with a single value\ndf.fillna(0)\n\n# Fill with column-specific values\ndf.fillna({\n    'age': 0,\n    'name': 'Unknown',\n    'score': -1\n})\n</code></pre>"},{"location":"guides/dataframe/#complete-data-cleaning-pipeline","title":"Complete Data Cleaning Pipeline","text":"<pre><code># Clean data by removing bad rows and filling missing values\nclean_df = (\n    df.dropna(axis=1, how='all')     # Remove empty columns\n    .fillna({'age': 0, 'name': ''})  # Fill remaining None\n    .filter(lambda row: row['age'] &gt;= 0)  # Remove invalid rows\n)\n</code></pre>"},{"location":"guides/dataframe/#dataframe-utilities","title":"DataFrame Utilities","text":""},{"location":"guides/dataframe/#comparing-dataframes","title":"Comparing DataFrames","text":"<pre><code># Check if two DataFrames are identical\ndf1 = DataFrame.from_dict({'a': [1, 2], 'b': [3, 4]})\ndf2 = DataFrame.from_dict({'a': [1, 2], 'b': [3, 4]})\n\nassert df1.equals(df2)  # True\n\n# Order matters\ndf3 = DataFrame.from_dict({'a': [2, 1], 'b': [4, 3]})\nassert not df1.equals(df3)  # False\n</code></pre>"},{"location":"guides/dataframe/#copying-dataframes","title":"Copying DataFrames","text":"<pre><code># Create independent copy\ndf_copy = df.deepcopy()\n\n# Modifications to copy don't affect original\ndf_copy.data[0][0] = 'modified'\nassert df.data[0][0] != 'modified'\n</code></pre>"},{"location":"guides/dataframe/#counting-unique-values","title":"Counting Unique Values","text":"<pre><code># Count unique values in a column\nunique_count = df.nunique('category')\nprint(f\"Found {unique_count} unique categories\")\n\n# Get the actual unique values\nunique_values = df.unique('category')\nprint(f\"Categories: {unique_values}\")\n\n# Count occurrences of each value\nvalue_counts = df.value_counts('status')\nprint(value_counts)  # {'active': 10, 'inactive': 5}\n</code></pre>"},{"location":"guides/dataframe/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"guides/dataframe/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Generate statistical summary\nstats = df.describe()\n\n# Results include: count, mean, std, min, 25%, 50%, 75%, max\n# Non-numeric columns show None for statistics\nprint(stats.get_column('age'))  # [5, 32.5, 4.2, 25, 28, 31, 36, 45]\nprint(stats.get_column('stat'))  # ['count', 'mean', 'std', ...]\n</code></pre>"},{"location":"guides/dataframe/#group-by-operations","title":"Group By Operations","text":"<p>The <code>groupby()</code> method provides SQL-like aggregation capabilities. It returns a <code>GroupBy</code> helper object that builds groups internally and provides aggregation methods.</p> <p>How it works: - Groups rows by unique values in the specified column - Filters out rows where the grouping column is None - Provides aggregation methods that return new DataFrames - Uses eager evaluation (groups are built immediately) - Uses only Python stdlib (statistics module for mean)</p> <p>Available aggregation methods:</p> Method Description Returns <code>count()</code> Count rows per group DataFrame with <code>[group_col, 'count']</code> <code>sum(col)</code> Sum numeric column per group DataFrame with <code>[group_col, 'col_sum']</code> <code>mean(col)</code> Average numeric column per group DataFrame with <code>[group_col, 'col_mean']</code> <code>min(col)</code> Minimum value per group DataFrame with <code>[group_col, 'col_min']</code> <code>max(col)</code> Maximum value per group DataFrame with <code>[group_col, 'col_max']</code> <p>Basic usage:</p> <pre><code>from servicekit.data import DataFrame\n\n# Sample sales data\ndf = DataFrame(\n    columns=['region', 'product', 'sales', 'quantity'],\n    data=[\n        ['North', 'Widget', 1000, 10],\n        ['North', 'Gadget', 1500, 15],\n        ['South', 'Widget', 800, 8],\n        ['South', 'Gadget', 1200, 12],\n        ['North', 'Widget', 1100, 11],\n    ]\n)\n\n# Count rows per group\nregion_counts = df.groupby('region').count()\n# Returns: DataFrame({'region': ['North', 'South'], 'count': [3, 2]})\n\n# Sum sales by region\ntotal_sales = df.groupby('region').sum('sales')\n# Returns: DataFrame({'region': ['North', 'South'], 'sales_sum': [3600, 2000]})\n\n# Average quantity by product\navg_qty = df.groupby('product').mean('quantity')\n# Returns: DataFrame({'product': ['Widget', 'Gadget'], 'quantity_mean': [9.67, 13.5]})\n\n# Find min/max sales by region\nmin_sales = df.groupby('region').min('sales')\nmax_sales = df.groupby('region').max('sales')\n</code></pre> <p>Advanced patterns:</p> <pre><code># Multiple aggregations on same grouping\ngrouped = df.groupby('region')\nsummary = DataFrame(\n    columns=['region', 'count', 'total_sales', 'avg_sales'],\n    data=[\n        [\n            group,\n            grouped.count().filter(lambda r: r['region'] == group)[0]['count'],\n            grouped.sum('sales').filter(lambda r: r['region'] == group)[0]['sales_sum'],\n            grouped.mean('sales').filter(lambda r: r['region'] == group)[0]['sales_mean'],\n        ]\n        for group in df.unique('region')\n    ]\n)\n\n# Combine with filtering\nhigh_sales = df.filter(lambda r: r['sales'] &gt; 1000)\nsummary = high_sales.groupby('region').count()\n\n# Chain with other operations\ndf.groupby('product').sum('sales').sort('sales_sum', reverse=True).head(5)\n</code></pre> <p>Design notes: - Groups are built eagerly when <code>groupby()</code> is called - Aggregation methods filter out None values automatically - All methods return new DataFrame instances (immutable pattern) - Uses stdlib only (no pandas/numpy dependencies) - Raises KeyError if column not found</p>"},{"location":"guides/dataframe/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/dataframe/#data-pipeline","title":"Data Pipeline","text":"<pre><code># Read, process, write\ndf = (\n    DataFrame.from_csv(\"input.csv\")\n    .select([\"name\", \"age\", \"score\"])\n    .rename({\"score\": \"grade\"})\n    .filter(lambda row: row['age'] &gt;= 18)\n    .drop_duplicates(subset=['name'])\n    .fillna({'grade': 0})\n    .head(1000)\n)\ndf.to_csv(\"output.csv\")\n\n# Advanced pipeline with transformations\ndf = (\n    DataFrame.from_csv(\"sales.csv\")\n    .drop(['internal_id', 'debug_flag'])\n    .rename({'product_name': 'product', 'sale_amount': 'amount'})\n    .filter(lambda row: row['amount'] &gt; 0)\n    .apply(str.upper, 'product')\n    .drop_duplicates(subset=['order_id'])\n    .sort('amount', ascending=False)\n)\n</code></pre>"},{"location":"guides/dataframe/#api-data-validation","title":"API Data Validation","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom servicekit.data import DataFrame\n\n@app.post(\"/data/$validate\")\nasync def validate_data(file: UploadFile):\n    \"\"\"Validate uploaded CSV data.\"\"\"\n    content = await file.read()\n    df = DataFrame.from_csv(csv_string=content.decode())\n\n    # Validate structure\n    try:\n        df.validate_structure()\n    except ValueError as e:\n        raise HTTPException(400, f\"Invalid structure: {e}\")\n\n    # Check required columns\n    required = [\"user_id\", \"timestamp\", \"value\"]\n    missing = set(required) - set(df.columns)\n    if missing:\n        raise HTTPException(400, f\"Missing columns: {missing}\")\n\n    # Check for nulls\n    nulls = df.has_nulls()\n    if any(nulls.get(col, False) for col in required):\n        raise HTTPException(400, \"Required columns contain null values\")\n\n    # Return metadata\n    return {\n        \"rows\": df.shape[0],\n        \"columns\": df.columns,\n        \"types\": df.infer_types(),\n        \"sample\": df.head(5).to_dict(orient=\"records\")\n    }\n</code></pre>"},{"location":"guides/dataframe/#data-transformation","title":"Data Transformation","text":"<pre><code>def clean_dataframe(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Clean and standardize DataFrame.\"\"\"\n    # Remove unnecessary columns\n    df = df.drop([\"temp\", \"debug\"])\n\n    # Rename for consistency\n    df = df.rename({\n        \"user_name\": \"name\",\n        \"user_age\": \"age\"\n    })\n\n    # Validate\n    df.validate_structure()\n\n    return df\n</code></pre>"},{"location":"guides/dataframe/#format-conversion","title":"Format Conversion","text":"<pre><code># CSV to Pandas\ndf = DataFrame.from_csv(\"data.csv\")\npandas_df = df.to_pandas()\n\n# Pandas to CSV\ndf = DataFrame.from_pandas(pandas_df)\ndf.to_csv(\"output.csv\")\n\n# Cross-library conversion\npolars_df = DataFrame.from_pandas(pandas_df).to_polars()\n</code></pre>"},{"location":"guides/dataframe/#api-response-formats","title":"API Response Formats","text":""},{"location":"guides/dataframe/#json-response","title":"JSON Response","text":"<pre><code>from fastapi import FastAPI\nfrom servicekit.data import DataFrame\n\n@app.get(\"/data\")\nasync def get_data():\n    \"\"\"Return data as JSON.\"\"\"\n    df = get_dataframe()\n    return df.to_dict(orient=\"records\")\n</code></pre>"},{"location":"guides/dataframe/#csv-download","title":"CSV Download","text":"<pre><code>from fastapi.responses import Response\n\n@app.get(\"/data/export.csv\")\nasync def download_csv():\n    \"\"\"Download data as CSV.\"\"\"\n    df = get_dataframe()\n    csv_data = df.to_csv()\n\n    return Response(\n        content=csv_data,\n        media_type=\"text/csv\",\n        headers={\n            \"Content-Disposition\": \"attachment; filename=data.csv\"\n        }\n    )\n</code></pre>"},{"location":"guides/dataframe/#paginated-response","title":"Paginated Response","text":"<pre><code>from pydantic import BaseModel\n\nclass PaginatedData(BaseModel):\n    \"\"\"Paginated DataFrame response.\"\"\"\n    total: int\n    page: int\n    page_size: int\n    data: list[dict]\n\n@app.get(\"/data\", response_model=PaginatedData)\nasync def get_paginated_data(page: int = 1, page_size: int = 100):\n    \"\"\"Return paginated data.\"\"\"\n    df = get_dataframe()\n\n    # Calculate pagination\n    total = df.shape[0]\n    start = (page - 1) * page_size\n\n    # Get page using head/tail\n    if start + page_size &lt; total:\n        page_df = df.tail(-start).head(page_size)\n    else:\n        page_df = df.tail(-start)\n\n    return PaginatedData(\n        total=total,\n        page=page,\n        page_size=page_size,\n        data=page_df.to_dict(orient=\"records\")\n    )\n</code></pre>"},{"location":"guides/dataframe/#error-handling","title":"Error Handling","text":""},{"location":"guides/dataframe/#import-errors","title":"Import Errors","text":"<p>DataFrames with optional dependencies raise clear errors:</p> <pre><code>try:\n    df.to_pandas()\nexcept ImportError as e:\n    print(e)\n    # \"pandas is required for to_pandas(). Install with: uv add pandas\"\n</code></pre>"},{"location":"guides/dataframe/#validation-errors","title":"Validation Errors","text":"<pre><code># Column not found\ntry:\n    df.select([\"nonexistent\"])\nexcept KeyError as e:\n    print(e)  # \"Column 'nonexistent' not found in DataFrame\"\n\n# Invalid structure\ntry:\n    df = DataFrame(columns=[\"a\", \"b\"], data=[[1, 2, 3]])\n    df.validate_structure()\nexcept ValueError as e:\n    print(e)  # \"Row 0 has 3 values, expected 2\"\n</code></pre>"},{"location":"guides/dataframe/#csv-errors","title":"CSV Errors","text":"<pre><code># File not found\ntry:\n    df = DataFrame.from_csv(\"missing.csv\")\nexcept FileNotFoundError as e:\n    print(e)  # \"File not found: missing.csv\"\n\n# Invalid parameters\ntry:\n    df = DataFrame.from_csv()  # Neither path nor csv_string\nexcept ValueError as e:\n    print(e)  # \"Either path or csv_string must be provided\"\n</code></pre>"},{"location":"guides/dataframe/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/dataframe/#when-to-use-dataframe","title":"When to Use DataFrame","text":"<p>Good use cases: - API data interchange (JSON \u2194 DataFrame \u2194 CSV) - Small to medium datasets (&lt;100k rows) - Format conversion between libraries - Data validation and inspection - Prototyping and development</p> <p>Not recommended for: - Large datasets (&gt;1M rows) - use pandas/polars directly - Heavy data transformations - use specialized libraries - Production analytics - use pandas/polars/DuckDB - High-performance computing - use NumPy/pandas</p>"},{"location":"guides/dataframe/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code># DataFrame stores data as list of lists (row-oriented)\n# For large datasets, convert to columnar format:\npandas_df = df.to_pandas()  # More efficient for operations\n\n# For very large files, consider streaming:\n# - Read in chunks with pandas\n# - Process incrementally\n# - Use DataFrame for API boundaries only\n</code></pre>"},{"location":"guides/dataframe/#testing-with-dataframe","title":"Testing with DataFrame","text":""},{"location":"guides/dataframe/#test-data-creation","title":"Test Data Creation","text":"<pre><code>import pytest\nfrom servicekit.data import DataFrame\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Create sample DataFrame for testing.\"\"\"\n    return DataFrame.from_dict({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"score\": [95, 87, 92]\n    })\n\ndef test_data_processing(sample_data):\n    \"\"\"Test data processing pipeline.\"\"\"\n    result = sample_data.select([\"name\", \"score\"])\n    assert result.shape == (3, 2)\n    assert \"id\" not in result.columns\n</code></pre>"},{"location":"guides/dataframe/#csv-round-trip-testing","title":"CSV Round-Trip Testing","text":"<pre><code>def test_csv_roundtrip(tmp_path):\n    \"\"\"Test CSV export and import.\"\"\"\n    # Create test data\n    original = DataFrame.from_dict({\n        \"name\": [\"Alice\", \"Bob\"],\n        \"age\": [25, 30]\n    })\n\n    # Write to file\n    csv_file = tmp_path / \"test.csv\"\n    original.to_csv(csv_file)\n\n    # Read back\n    restored = DataFrame.from_csv(csv_file)\n\n    # Verify (note: CSV makes all values strings)\n    assert restored.columns == original.columns\n    assert restored.shape == original.shape\n</code></pre>"},{"location":"guides/dataframe/#best-practices","title":"Best Practices","text":""},{"location":"guides/dataframe/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Validate early: Call <code>validate_structure()</code> after data ingestion</li> <li>Check types: Use <code>infer_types()</code> to understand your data</li> <li>Handle nulls: Use <code>has_nulls()</code> to detect data quality issues</li> <li>Immutable pattern: Chain operations without modifying originals</li> <li>Small data: Use DataFrame for API boundaries, not heavy processing</li> <li>Clear errors: Let ImportError guide users to install dependencies</li> <li>CSV for interchange: Use CSV for human-readable data exchange</li> </ul>"},{"location":"guides/dataframe/#avoid","title":"Avoid","text":"<ul> <li>Large datasets: Don't use for &gt;100k rows (use pandas/polars instead)</li> <li>Heavy operations: Don't use for joins, aggregations, complex queries</li> <li>In-place modification: Don't try to modify DataFrames (they're immutable)</li> <li>Type assumptions: CSV imports make all values strings</li> <li>Missing validation: Always validate data from external sources</li> </ul>"},{"location":"guides/dataframe/#dependencies","title":"Dependencies","text":""},{"location":"guides/dataframe/#core-required","title":"Core (Required)","text":"<ul> <li>pydantic: For data validation and serialization</li> </ul>"},{"location":"guides/dataframe/#optional-no-dependencies","title":"Optional (No Dependencies)","text":"<ul> <li>CSV support: Uses Python stdlib <code>csv</code> module</li> <li>Data inspection: Uses Python stdlib <code>random</code> module</li> <li>Column operations: Pure Python (no dependencies)</li> <li>Validation: Pure Python (no dependencies)</li> </ul>"},{"location":"guides/dataframe/#optional-with-dependencies","title":"Optional (With Dependencies)","text":"<p>Install as needed:</p> <pre><code># For pandas support\nuv add pandas\n\n# For polars support\nuv add polars\n\n# For xarray support\nuv add xarray\n\n# For PyArrow/Parquet support (future)\nuv add 'servicekit[arrow]'\n</code></pre>"},{"location":"guides/dataframe/#examples","title":"Examples","text":""},{"location":"guides/dataframe/#example-files","title":"Example Files","text":"<p>See <code>examples/</code> directory: - Basic DataFrame operations - API integration patterns - CSV upload/download - Data validation workflows</p>"},{"location":"guides/dataframe/#interactive-session","title":"Interactive Session","text":"<pre><code>from servicekit.data import DataFrame\n\n# Create sample data\ndf = DataFrame.from_dict({\n    \"product\": [\"Apple\", \"Banana\", \"Cherry\", \"Date\"],\n    \"price\": [1.2, 0.5, 3.0, 2.5],\n    \"stock\": [100, 150, 80, 60]\n})\n\n# Explore\nprint(f\"Shape: {df.shape}\")\nprint(f\"Columns: {df.columns}\")\nprint(df.head(2))\n\n# Analyze\nprint(f\"Types: {df.infer_types()}\")\nprint(f\"Has nulls: {df.has_nulls()}\")\n\n# Transform\nexpensive = df.select([\"product\", \"price\"])\nprint(expensive.to_dict(orient=\"records\"))\n\n# Export\nexpensive.to_csv(\"expensive_items.csv\")\n</code></pre>"},{"location":"guides/dataframe/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dataframe/#importerror-for-optional-libraries","title":"ImportError for Optional Libraries","text":"<p>Problem: Getting ImportError when using pandas/polars methods.</p> <p>Solution: <pre><code># Install required library\nuv add pandas  # or polars, xarray\n</code></pre></p>"},{"location":"guides/dataframe/#csv-values-all-strings","title":"CSV Values All Strings","text":"<p>Problem: After <code>from_csv()</code>, all values are strings.</p> <p>Solution: CSV format doesn't preserve types. Either: 1. Convert manually after import 2. Use <code>infer_types()</code> to detect types 3. Use Parquet for type preservation (future feature)</p>"},{"location":"guides/dataframe/#column-not-found-errors","title":"Column Not Found Errors","text":"<p>Problem: <code>KeyError: Column 'x' not found</code>.</p> <p>Solution: <pre><code># Check available columns\nprint(df.columns)\n\n# Case-sensitive matching\ndf.select([\"Name\"])  # Error if column is \"name\"\n</code></pre></p>"},{"location":"guides/dataframe/#memory-issues-with-large-data","title":"Memory Issues with Large Data","text":"<p>Problem: Running out of memory with large DataFrames.</p> <p>Solution: DataFrame is designed for small-medium data. For large datasets: <pre><code># Use pandas directly for large data\nimport pandas as pd\npandas_df = pd.read_csv(\"large_file.csv\", chunksize=10000)\n\n# Or use polars for better performance\nimport polars as pl\npolars_df = pl.read_csv(\"large_file.csv\")\n</code></pre></p>"},{"location":"guides/dataframe/#next-steps","title":"Next Steps","text":"<ul> <li>Learn More: See other guides for integrating DataFrame with APIs</li> <li>Contribute: Submit PRs for new format support (Parquet, Arrow, etc.)</li> <li>Examples: Check <code>examples/</code> directory for real-world usage</li> </ul> <p>For related features, see: - Servicekit Repository - Building services with servicekit - Authentication Guide - Securing data endpoints - Job Scheduler Guide - Processing DataFrame data in background</p>"},{"location":"guides/health-checks/","title":"Health Checks and Monitoring","text":"<p>Servicekit provides comprehensive health check capabilities for service monitoring, including one-time health checks and continuous streaming for real-time monitoring.</p>"},{"location":"guides/health-checks/#quick-start","title":"Quick Start","text":"<p>Enable health checks in your service:</p> <pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()  # Enables /health endpoint\n    .build()\n)\n</code></pre> <p>Your service now exposes health endpoints at <code>/health</code> and <code>/health/$stream</code>.</p>"},{"location":"guides/health-checks/#endpoints","title":"Endpoints","text":""},{"location":"guides/health-checks/#one-time-health-check","title":"One-Time Health Check","text":"<p>Endpoint: <code>GET /health</code></p> <p>Returns current health status in a single response:</p> <pre><code>curl http://localhost:8000/health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\"\n}\n</code></pre></p> <p>Use Cases: - Kubernetes liveness/readiness probes - Load balancer health checks - Manual health verification - CI/CD deployment validation</p>"},{"location":"guides/health-checks/#continuous-health-monitoring-sse","title":"Continuous Health Monitoring (SSE)","text":"<p>Endpoint: <code>GET /health/$stream</code></p> <p>Streams health status updates continuously using Server-Sent Events (SSE):</p> <pre><code># Stream with default 1.0s interval\ncurl -N http://localhost:8000/health/\\$stream\n\n# Stream with custom 2.0s interval\ncurl -N \"http://localhost:8000/health/\\$stream?poll_interval=2.0\"\n</code></pre> <p>Response Format (text/event-stream): <pre><code>data: {\"status\":\"healthy\"}\n\ndata: {\"status\":\"healthy\"}\n\ndata: {\"status\":\"healthy\"}\n</code></pre></p> <p>Query Parameters: - <code>poll_interval</code> (float): Seconds between health checks. Default: 1.0</p> <p>Use Cases: - Real-time dashboard monitoring - Continuous integration tests - Service health visualization - Alert detection systems - Development/debugging</p> <p>Note: Stream continues indefinitely until client disconnects. Use Ctrl+C to stop.</p>"},{"location":"guides/health-checks/#custom-health-checks","title":"Custom Health Checks","text":"<p>Add custom health checks to monitor specific subsystems:</p> <pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\nfrom servicekit.api.routers.health import HealthState\n\nasync def check_database() -&gt; tuple[HealthState, str | None]:\n    \"\"\"Check database connectivity.\"\"\"\n    try:\n        # Test database connection\n        async with get_session() as session:\n            await session.execute(\"SELECT 1\")\n        return (HealthState.HEALTHY, None)\n    except Exception as e:\n        return (HealthState.UNHEALTHY, f\"Database error: {str(e)}\")\n\nasync def check_redis() -&gt; tuple[HealthState, str | None]:\n    \"\"\"Check Redis connectivity.\"\"\"\n    try:\n        # Test Redis connection\n        await redis_client.ping()\n        return (HealthState.HEALTHY, None)\n    except Exception as e:\n        return (HealthState.DEGRADED, f\"Redis unavailable: {str(e)}\")\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health(checks={\n        \"database\": check_database,\n        \"redis\": check_redis,\n    })\n    .build()\n)\n</code></pre> <p>Response with Custom Checks: <pre><code>{\n  \"status\": \"degraded\",\n  \"checks\": {\n    \"database\": {\n      \"state\": \"healthy\"\n    },\n    \"redis\": {\n      \"state\": \"degraded\",\n      \"message\": \"Redis unavailable: Connection refused\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"guides/health-checks/#health-states","title":"Health States","text":"<ul> <li><code>healthy</code>: All checks passed, service fully operational</li> <li><code>degraded</code>: Some non-critical checks failed, service partially operational</li> <li><code>unhealthy</code>: Critical checks failed, service not operational</li> </ul> <p>Aggregation Logic: - Overall status = worst state among all checks - <code>unhealthy</code> &gt; <code>degraded</code> &gt; <code>healthy</code> - Exception in check = <code>unhealthy</code> with error message</p>"},{"location":"guides/health-checks/#kubernetes-integration","title":"Kubernetes Integration","text":""},{"location":"guides/health-checks/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"<p>Use health checks for Kubernetes pod lifecycle management:</p> <p>deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: servicekit-service\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: your-servicekit-app\n        ports:\n        - containerPort: 8000\n\n        # Liveness probe - restart if unhealthy\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 3\n\n        # Readiness probe - remove from service if not ready\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 2\n</code></pre></p> <p>Best Practices: - Liveness: Checks if app is stuck/deadlocked (longer intervals, higher threshold) - Readiness: Checks if app can serve traffic (shorter intervals, lower threshold) - Use <code>/health</code> for both probes (not <code>/health/$stream</code>) - Set appropriate timeouts (3-5 seconds recommended)</p>"},{"location":"guides/health-checks/#service-mesh-integration","title":"Service Mesh Integration","text":"<p>For service meshes like Istio or Linkerd, health checks are used for traffic routing:</p> <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: servicekit-service\nspec:\n  host: servicekit-service\n  trafficPolicy:\n    outlierDetection:\n      consecutiveErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n      minHealthPercent: 40\n    connectionPool:\n      http:\n        http1MaxPendingRequests: 100\n        http2MaxRequests: 100\n</code></pre>"},{"location":"guides/health-checks/#python-client-examples","title":"Python Client Examples","text":""},{"location":"guides/health-checks/#one-time-health-check_1","title":"One-Time Health Check","text":"<pre><code>import httpx\n\nasync def check_service_health(base_url: str) -&gt; bool:\n    \"\"\"Check if service is healthy.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"{base_url}/health\")\n        data = response.json()\n        return data[\"status\"] == \"healthy\"\n</code></pre>"},{"location":"guides/health-checks/#continuous-monitoring-with-sse","title":"Continuous Monitoring with SSE","text":"<pre><code>import httpx\nimport json\n\nasync def monitor_service_health(base_url: str, poll_interval: float = 1.0):\n    \"\"\"Monitor service health via SSE stream.\"\"\"\n    url = f\"{base_url}/health/$stream?poll_interval={poll_interval}\"\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\"GET\", url) as response:\n            async for line in response.aiter_lines():\n                if line.startswith(\"data: \"):\n                    data = json.loads(line[6:])\n                    status = data[\"status\"]\n                    print(f\"Health: {status}\")\n\n                    if status != \"healthy\":\n                        # Alert or take action\n                        await send_alert(f\"Service unhealthy: {data}\")\n</code></pre>"},{"location":"guides/health-checks/#health-check-with-timeout","title":"Health Check with Timeout","text":"<pre><code>import httpx\nimport asyncio\n\nasync def health_check_with_timeout(base_url: str, timeout: float = 3.0) -&gt; str:\n    \"\"\"Check health with timeout.\"\"\"\n    try:\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.get(f\"{base_url}/health\")\n            response.raise_for_status()\n            data = response.json()\n            return data[\"status\"]\n    except httpx.TimeoutException:\n        return \"timeout\"\n    except Exception as e:\n        return f\"error: {str(e)}\"\n</code></pre>"},{"location":"guides/health-checks/#load-balancer-integration","title":"Load Balancer Integration","text":""},{"location":"guides/health-checks/#haproxy-configuration","title":"HAProxy Configuration","text":"<pre><code>backend servicekit_servers\n    balance roundrobin\n    option httpchk GET /health\n    http-check expect status 200\n    http-check expect string healthy\n\n    server app1 10.0.1.10:8000 check inter 5s rise 2 fall 3\n    server app2 10.0.1.11:8000 check inter 5s rise 2 fall 3\n    server app3 10.0.1.12:8000 check inter 5s rise 2 fall 3\n</code></pre>"},{"location":"guides/health-checks/#nginx-configuration","title":"NGINX Configuration","text":"<pre><code>upstream servicekit_backend {\n    server 10.0.1.10:8000 max_fails=3 fail_timeout=30s;\n    server 10.0.1.11:8000 max_fails=3 fail_timeout=30s;\n    server 10.0.1.12:8000 max_fails=3 fail_timeout=30s;\n}\n\nserver {\n    listen 80;\n\n    location /health {\n        proxy_pass http://servicekit_backend;\n        proxy_connect_timeout 2s;\n        proxy_read_timeout 5s;\n    }\n\n    location / {\n        proxy_pass http://servicekit_backend;\n        # Health check performed separately\n    }\n}\n</code></pre>"},{"location":"guides/health-checks/#monitoring-dashboards","title":"Monitoring Dashboards","text":""},{"location":"guides/health-checks/#grafana-dashboard-with-sse","title":"Grafana Dashboard with SSE","text":"<p>Create a custom panel using SSE streaming:</p> <pre><code>// Grafana panel plugin for SSE health monitoring\nconst eventSource = new EventSource('http://localhost:8000/health/$stream');\n\neventSource.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n  updateHealthStatus(data.status);\n\n  if (data.checks) {\n    updateChecksTable(data.checks);\n  }\n};\n</code></pre>"},{"location":"guides/health-checks/#simple-html-dashboard","title":"Simple HTML Dashboard","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Service Health Monitor&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Service Health&lt;/h1&gt;\n    &lt;div id=\"status\"&gt;Connecting...&lt;/div&gt;\n    &lt;pre id=\"checks\"&gt;&lt;/pre&gt;\n\n    &lt;script&gt;\n        const eventSource = new EventSource('http://localhost:8000/health/$stream?poll_interval=2.0');\n\n        eventSource.onmessage = (event) =&gt; {\n            const data = JSON.parse(event.data);\n            const statusEl = document.getElementById('status');\n            const checksEl = document.getElementById('checks');\n\n            // Update status with color coding\n            statusEl.textContent = `Status: ${data.status}`;\n            statusEl.style.color = data.status === 'healthy' ? 'green' :\n                                   data.status === 'degraded' ? 'orange' : 'red';\n\n            // Show detailed checks\n            if (data.checks) {\n                checksEl.textContent = JSON.stringify(data.checks, null, 2);\n            }\n        };\n\n        eventSource.onerror = () =&gt; {\n            document.getElementById('status').textContent = 'Connection lost';\n            document.getElementById('status').style.color = 'red';\n        };\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"guides/health-checks/#best-practices","title":"Best Practices","text":""},{"location":"guides/health-checks/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Enable in all services: Health checks are essential for production reliability</li> <li>Use custom checks: Monitor critical dependencies (database, cache, external APIs)</li> <li>Keep checks fast: Health checks should complete in &lt;1 second</li> <li>Avoid expensive operations: Don't run migrations, heavy queries, or external calls</li> <li>Use appropriate states: <code>degraded</code> for non-critical, <code>unhealthy</code> for critical failures</li> <li>SSE for dashboards: Use <code>/health/$stream</code> for real-time monitoring UIs</li> <li>One-time for probes: Use <code>/health</code> for Kubernetes and load balancers</li> <li>Unauthenticated: Keep health endpoints public for infrastructure access</li> </ul>"},{"location":"guides/health-checks/#avoid","title":"Avoid","text":"<ul> <li>Expensive checks: Heavy database queries, full table scans, complex computations</li> <li>External dependencies in liveness: Don't make liveness depend on external services</li> <li>High frequency polling: Don't poll <code>/health</code> more than once per second</li> <li>Authenticated health: Health endpoints should be unauthenticated for infrastructure</li> <li>Incomplete aggregation: Always include all critical subsystems in checks</li> </ul>"},{"location":"guides/health-checks/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"guides/health-checks/#with-authentication","title":"With Authentication","text":"<p>Health endpoints should remain unauthenticated:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_health()\n    .with_auth(\n        unauthenticated_paths=[\n            \"/health\",        # Health check\n            \"/health/$stream\" # Health monitoring\n        ]\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/health-checks/#with-monitoring","title":"With Monitoring","text":"<p>Combine health checks with Prometheus metrics:</p> <pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_health(checks={\"database\": check_database})\n    .with_monitoring()  # Prometheus metrics at /metrics\n    .build()\n)\n</code></pre> <p>Operational Endpoints: - <code>/health</code> - Kubernetes liveness/readiness (one-time) - <code>/health/$stream</code> - Real-time monitoring (continuous) - <code>/metrics</code> - Prometheus metrics (scraping)</p> <p>All operational endpoints use root-level paths for easy discovery.</p>"},{"location":"guides/health-checks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/health-checks/#health-check-times-out","title":"Health Check Times Out","text":"<p>Problem: Health check takes too long or times out.</p> <p>Solution: 1. Review custom health check functions 2. Ensure checks complete in &lt;1 second 3. Remove expensive operations (heavy queries, external calls) 4. Use connection pooling for database checks</p>"},{"location":"guides/health-checks/#health-returns-unhealthy","title":"Health Returns Unhealthy","text":"<p>Problem: Service reports unhealthy but seems functional.</p> <p>Solution: 1. Check logs for health check errors 2. Review custom check implementations 3. Test dependencies manually (database, cache, APIs) 4. Verify network connectivity to dependencies</p>"},{"location":"guides/health-checks/#sse-stream-disconnects","title":"SSE Stream Disconnects","text":"<p>Problem: <code>/health/$stream</code> disconnects frequently.</p> <p>Solution: 1. Check nginx/proxy timeout settings 2. Increase client timeout 3. Verify network stability 4. Check for reverse proxy buffering (should be disabled)</p>"},{"location":"guides/health-checks/#kubernetes-pod-restarts","title":"Kubernetes Pod Restarts","text":"<p>Problem: Pods restart frequently due to liveness probe failures.</p> <p>Solution: 1. Increase <code>initialDelaySeconds</code> for slower startup 2. Increase <code>failureThreshold</code> to allow temporary failures 3. Increase <code>timeoutSeconds</code> for slower responses 4. Review health check performance</p>"},{"location":"guides/health-checks/#examples","title":"Examples","text":"<ul> <li><code>examples/monitoring_api.py</code> - Service with health checks and monitoring</li> <li><code>examples/docs/monitoring_api.postman_collection.json</code> - Postman collection with health endpoints</li> </ul>"},{"location":"guides/health-checks/#next-steps","title":"Next Steps","text":"<ul> <li>Metrics: Add Prometheus monitoring with <code>.with_monitoring()</code></li> <li>Alerting: Set up alerts based on health status</li> <li>Dashboards: Create real-time monitoring dashboards with SSE</li> <li>Custom Checks: Implement checks for your specific dependencies</li> </ul> <p>For related features, see: - Monitoring Guide - Prometheus metrics and OpenTelemetry - Job Scheduler Guide - Background job health monitoring</p>"},{"location":"guides/job-scheduler/","title":"Job Scheduler","text":"<p>Servicekit provides an async job scheduler for managing long-running tasks with real-time status monitoring via Server-Sent Events (SSE).</p>"},{"location":"guides/job-scheduler/#quick-start","title":"Quick Start","text":""},{"location":"guides/job-scheduler/#submit-a-job","title":"Submit a Job","text":"<pre><code># Start the example service\nfastapi dev examples/job_scheduler_sse_api.py\n\n# Submit a 30-second computation job and capture job ID\nJOB_ID=$(curl -s -X POST http://localhost:8000/api/v1/slow-compute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"steps\": 30}' | jq -r '.job_id')\n\necho \"Job ID: $JOB_ID\"\n</code></pre> <p>Response: <pre><code>{\n  \"job_id\": \"01JQRS7X...\",\n  \"message\": \"Job submitted with 30 steps. Stream real-time status...\",\n  \"stream_url\": \"/api/v1/jobs/01JQRS7X.../$stream\"\n}\n</code></pre></p>"},{"location":"guides/job-scheduler/#monitor-job-status-real-time-sse","title":"Monitor Job Status (Real-Time SSE)","text":"<pre><code># Stream status updates in real-time\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n</code></pre> <p>Output (streaming): <pre><code>data: {\"id\":\"01JQRS7X...\",\"status\":\"pending\",\"submitted_at\":\"2025-10-12T...\"}\n\ndata: {\"id\":\"01JQRS7X...\",\"status\":\"running\",\"started_at\":\"2025-10-12T...\"}\n\ndata: {\"id\":\"01JQRS7X...\",\"status\":\"completed\",\"finished_at\":\"2025-10-12T...\",\"artifact_id\":null}\n</code></pre></p> <p>Note: Use <code>-N</code> flag to disable cURL buffering for real-time streaming.</p>"},{"location":"guides/job-scheduler/#job-lifecycle","title":"Job Lifecycle","text":"<p>Jobs progress through these states:</p> <ol> <li>pending - Job submitted, waiting to start</li> <li>running - Job is currently executing</li> <li>completed - Job finished successfully</li> <li>failed - Job encountered an error</li> <li>canceled - Job was canceled by user</li> </ol>"},{"location":"guides/job-scheduler/#terminal-states","title":"Terminal States","text":"<p>These states indicate the job is finished: - <code>completed</code> - Success - <code>failed</code> - Error occurred - <code>canceled</code> - User canceled</p> <p>SSE streams automatically close when a terminal state is reached.</p>"},{"location":"guides/job-scheduler/#polling-vs-streaming","title":"Polling vs Streaming","text":""},{"location":"guides/job-scheduler/#traditional-polling","title":"Traditional Polling","text":"<pre><code># Client must repeatedly poll every second\nwhile true; do\n  curl http://localhost:8000/api/v1/jobs/01JQRS7X...\n  sleep 1\ndone\n</code></pre> <p>Problems: - Wastes bandwidth (repeated full HTTP requests) - Polling interval trade-off (fast = expensive, slow = delayed updates) - Client-side polling logic needed</p>"},{"location":"guides/job-scheduler/#sse-streaming","title":"SSE Streaming","text":"<pre><code># Server pushes updates automatically\ncurl -N http://localhost:8000/api/v1/jobs/01JQRS7X.../\\$stream\n</code></pre> <p>Benefits: - Efficient (single HTTP connection, server pushes updates) - Real-time (updates sent immediately when status changes) - Standard (W3C EventSource API built into browsers) - Simple (no client-side polling logic)</p>"},{"location":"guides/job-scheduler/#real-time-streaming-with-sse","title":"Real-Time Streaming with SSE","text":"<p>Server-Sent Events (SSE) provide efficient real-time updates over a single HTTP connection.</p>"},{"location":"guides/job-scheduler/#browser-javascript-eventsource-api","title":"Browser JavaScript (EventSource API)","text":"<pre><code>const jobId = \"01JQRS7X...\";\nconst eventSource = new EventSource(`/api/v1/jobs/${jobId}/$stream`);\n\neventSource.onmessage = (event) =&gt; {\n  const job = JSON.parse(event.data);\n  console.log(`Status: ${job.status}`);\n\n  // Update UI\n  document.getElementById('status').textContent = job.status;\n\n  // Close connection when done\n  if (['completed', 'failed', 'canceled'].includes(job.status)) {\n    console.log('Job finished');\n    eventSource.close();\n  }\n};\n\neventSource.onerror = (error) =&gt; {\n  console.error('SSE connection error:', error);\n  eventSource.close();\n};\n</code></pre>"},{"location":"guides/job-scheduler/#curl-command-line","title":"cURL (Command Line)","text":"<pre><code># Stream job status updates\ncurl -N http://localhost:8000/api/v1/jobs/01JQRS7X.../\\$stream\n\n# Custom poll interval (default: 0.5 seconds)\ncurl -N \"http://localhost:8000/api/v1/jobs/01JQRS7X.../\\$stream?poll_interval=1.0\"\n</code></pre> <p>Important: Use <code>-N</code> / <code>--no-buffer</code> flag to disable buffering and see real-time updates.</p>"},{"location":"guides/job-scheduler/#python-httpx","title":"Python (httpx)","text":"<pre><code>import httpx\nimport json\n\njob_id = \"01JQRS7X...\"\nurl = f\"http://localhost:8000/api/v1/jobs/{job_id}/$stream\"\n\nwith httpx.stream(\"GET\", url) as response:\n    for line in response.iter_lines():\n        if line.startswith(\"data: \"):\n            data = line[6:]  # Remove \"data: \" prefix\n            job = json.loads(data)\n            print(f\"Status: {job['status']}\")\n\n            # Stop when terminal state reached\n            if job['status'] in ['completed', 'failed', 'canceled']:\n                break\n</code></pre>"},{"location":"guides/job-scheduler/#python-requests-not-recommended","title":"Python (requests - Not Recommended)","text":"<p>The <code>requests</code> library buffers responses by default, making it unsuitable for SSE. Use <code>httpx</code> instead.</p>"},{"location":"guides/job-scheduler/#configuration","title":"Configuration","text":""},{"location":"guides/job-scheduler/#servicebuilder-setup","title":"ServiceBuilder Setup","text":"<pre><code>from servicekit.api import ServiceBuilder, ServiceInfo\n\napp = (\n    ServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_jobs(max_concurrency=5)  # Limit concurrent jobs\n    .build()\n)\n</code></pre> <p>Parameters: - <code>max_concurrency</code> (<code>int | None</code>): Maximum concurrent jobs. <code>None</code> = unlimited.</p>"},{"location":"guides/job-scheduler/#sse-poll-interval","title":"SSE Poll Interval","text":"<p>The SSE endpoint polls the scheduler internally at configurable intervals:</p> <pre><code># Default: 0.5 seconds\ncurl -N http://localhost:8000/api/v1/jobs/01JQRS.../\\$stream\n\n# Custom: 1.0 second\ncurl -N \"http://localhost:8000/api/v1/jobs/01JQRS.../\\$stream?poll_interval=1.0\"\n</code></pre> <p>Recommendations: - Development: 0.5s (default) - good balance - Production: 1.0s - reduces server load - High-frequency: 0.1s - near real-time (use sparingly)</p>"},{"location":"guides/job-scheduler/#api-reference","title":"API Reference","text":""},{"location":"guides/job-scheduler/#post-apiv1jobs","title":"POST /api/v1/jobs","text":"<p>Not exposed directly. Submit jobs via custom endpoints (e.g., <code>/api/v1/slow-compute</code>).</p>"},{"location":"guides/job-scheduler/#get-apiv1jobs","title":"GET /api/v1/jobs","text":"<p>List all jobs with optional status filtering.</p> <pre><code># List all jobs\ncurl http://localhost:8000/api/v1/jobs\n\n# Filter by status\ncurl http://localhost:8000/api/v1/jobs?status_filter=completed\n</code></pre>"},{"location":"guides/job-scheduler/#get-apiv1jobsjob_id","title":"GET /api/v1/jobs/{job_id}","text":"<p>Get job status and details (single request).</p> <pre><code>curl http://localhost:8000/api/v1/jobs/01JQRS7X...\n</code></pre> <p>Response: <pre><code>{\n  \"id\": \"01JQRS7X...\",\n  \"status\": \"running\",\n  \"submitted_at\": \"2025-10-12T15:30:00Z\",\n  \"started_at\": \"2025-10-12T15:30:01Z\",\n  \"finished_at\": null,\n  \"error\": null,\n  \"artifact_id\": null\n}\n</code></pre></p>"},{"location":"guides/job-scheduler/#get-apiv1jobsjob_idstream","title":"GET /api/v1/jobs/{job_id}/$stream","text":"<p>Stream real-time job status updates via Server-Sent Events.</p> <p>Query Parameters: - <code>poll_interval</code> (float, default: 0.5): Seconds between status checks</p> <p>Response Format: <pre><code>Content-Type: text/event-stream\nCache-Control: no-cache\nConnection: keep-alive\n\ndata: {\"id\":\"...\",\"status\":\"pending\",...}\n\ndata: {\"id\":\"...\",\"status\":\"running\",...}\n\ndata: {\"id\":\"...\",\"status\":\"completed\",...}\n</code></pre></p> <p>Connection closes automatically when job reaches terminal state.</p>"},{"location":"guides/job-scheduler/#delete-apiv1jobsjob_id","title":"DELETE /api/v1/jobs/{job_id}","text":"<p>Cancel and delete a job.</p> <pre><code>curl -X DELETE http://localhost:8000/api/v1/jobs/01JQRS7X...\n</code></pre> <p>Returns <code>204 No Content</code> on success.</p>"},{"location":"guides/job-scheduler/#error-handling","title":"Error Handling","text":""},{"location":"guides/job-scheduler/#invalid-job-id-400","title":"Invalid Job ID (400)","text":"<pre><code>{\n  \"detail\": \"Invalid job ID format\"\n}\n</code></pre>"},{"location":"guides/job-scheduler/#job-not-found-404","title":"Job Not Found (404)","text":"<pre><code>{\n  \"detail\": \"Job not found\"\n}\n</code></pre>"},{"location":"guides/job-scheduler/#failed-jobs","title":"Failed Jobs","text":"<p>When a job fails, the <code>error</code> field contains the error message:</p> <pre><code>{\n  \"id\": \"01JQRS7X...\",\n  \"status\": \"failed\",\n  \"error\": \"ValueError: Invalid input\",\n  \"error_traceback\": \"Traceback (most recent call last):\\n...\"\n}\n</code></pre>"},{"location":"guides/job-scheduler/#job-deletion-during-streaming","title":"Job Deletion During Streaming","text":"<p>If a job is deleted while being streamed, the SSE connection sends a final event and closes:</p> <pre><code>data: {\"status\": \"deleted\"}\n</code></pre>"},{"location":"guides/job-scheduler/#testing","title":"Testing","text":""},{"location":"guides/job-scheduler/#manual-testing","title":"Manual Testing","text":"<p>Terminal 1: Start service <pre><code>fastapi dev examples/job_scheduler_sse_api.py\n</code></pre></p> <p>Terminal 2: Submit job and stream status <pre><code># Submit job and capture job ID\nJOB_ID=$(curl -s -X POST http://localhost:8000/api/v1/slow-compute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"steps\": 30}' | jq -r '.job_id')\n\necho \"Job ID: $JOB_ID\"\n\n# Stream status updates in real-time\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n</code></pre></p>"},{"location":"guides/job-scheduler/#browser-testing","title":"Browser Testing","text":"<ol> <li>Submit job via Swagger UI: http://localhost:8000/docs</li> <li>Open browser console (F12)</li> <li>Run JavaScript:</li> </ol> <pre><code>const jobId = \"01JQRS7X...\";  // From Swagger response\nconst es = new EventSource(`/api/v1/jobs/${jobId}/$stream`);\nes.onmessage = (e) =&gt; console.log(JSON.parse(e.data));\n</code></pre>"},{"location":"guides/job-scheduler/#production-deployment","title":"Production Deployment","text":""},{"location":"guides/job-scheduler/#concurrency-limits","title":"Concurrency Limits","text":"<p>Set <code>max_concurrency</code> to prevent resource exhaustion:</p> <pre><code>.with_jobs(max_concurrency=10)  # Max 10 concurrent jobs\n</code></pre> <p>Recommendations: - CPU-bound jobs: Set to number of CPU cores - I/O-bound jobs: Higher limits OK (10-50) - Memory-intensive: Lower limits to prevent OOM</p>"},{"location":"guides/job-scheduler/#load-balancers-and-proxies","title":"Load Balancers and Proxies","text":"<p>SSE requires special configuration for long-lived connections.</p> <p>nginx: <pre><code>location /api/v1/jobs {\n    proxy_pass http://backend;\n    proxy_buffering off;  # Required for SSE\n    proxy_read_timeout 600s;  # Allow long connections\n    proxy_http_version 1.1;\n}\n</code></pre></p> <p>Apache: <pre><code>&lt;Location /api/v1/jobs&gt;\n    ProxyPass http://backend\n    ProxyPassReverse http://backend\n    ProxyPreserveHost On\n    SetEnv proxy-nokeepalive 1\n&lt;/Location&gt;\n</code></pre></p> <p>AWS ALB: - Enable HTTP/2 (supports SSE) - Set idle timeout \u2265 60 seconds</p>"},{"location":"guides/job-scheduler/#monitoring","title":"Monitoring","text":"<p>Track job metrics:</p> <pre><code># Example: Prometheus metrics\nfrom prometheus_client import Counter, Histogram\n\njob_submissions = Counter('jobs_submitted_total', 'Total jobs submitted')\njob_duration = Histogram('job_duration_seconds', 'Job execution time')\n</code></pre>"},{"location":"guides/job-scheduler/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/job-scheduler/#stream-closes-immediately","title":"Stream Closes Immediately","text":"<p>Problem: SSE connection closes right after opening.</p> <p>Causes: 1. Job already in terminal state 2. Invalid job ID</p> <p>Solution: <pre><code># Check job status first\ncurl http://localhost:8000/api/v1/jobs/01JQRS7X...\n\n# If completed/failed/canceled, stream will close immediately\n</code></pre></p>"},{"location":"guides/job-scheduler/#no-updates-appearing","title":"No Updates Appearing","text":"<p>Problem: Connected but no events streaming.</p> <p>Causes: 1. cURL buffering enabled 2. Proxy buffering responses</p> <p>Solution: <pre><code># Use -N flag with cURL\ncurl -N http://localhost:8000/api/v1/jobs/01JQRS7X.../\\$stream\n\n# Check proxy configuration (see Production Deployment)\n</code></pre></p>"},{"location":"guides/job-scheduler/#eventsource-not-working-in-browser","title":"EventSource Not Working in Browser","text":"<p>Problem: JavaScript EventSource API fails or doesn't connect.</p> <p>Causes: 1. CORS issues 2. HTTPS mixed content (HTTPS page, HTTP EventSource) 3. Ad blockers</p> <p>Solution: <pre><code>// Check for errors\nconst es = new EventSource('/api/v1/jobs/01JQRS.../$stream');\nes.onerror = (e) =&gt; {\n  console.error('EventSource error:', e);\n  console.log('ReadyState:', es.readyState);  // 0=connecting, 1=open, 2=closed\n};\n\n// CORS: Ensure same origin or proper CORS headers\n// HTTPS: Use HTTPS for both page and EventSource\n// Ad blockers: Disable and test\n</code></pre></p>"},{"location":"guides/job-scheduler/#high-cpu-usage","title":"High CPU Usage","text":"<p>Problem: Scheduler consuming excessive CPU.</p> <p>Causes: 1. Too many concurrent jobs 2. Short poll_interval with many streams</p> <p>Solution: <pre><code># Limit concurrent jobs\n.with_jobs(max_concurrency=10)\n</code></pre></p> <pre><code># Increase poll interval\ncurl -N \"http://localhost:8000/api/v1/jobs/01JQRS.../\\$stream?poll_interval=1.0\"\n</code></pre>"},{"location":"guides/job-scheduler/#examples","title":"Examples","text":""},{"location":"guides/job-scheduler/#complete-workflow","title":"Complete Workflow","text":"<pre><code># 1. Submit job and extract job_id\nJOB_ID=$(curl -s -X POST http://localhost:8000/api/v1/slow-compute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"steps\": 30}' | jq -r '.job_id')\n\necho \"Job ID: $JOB_ID\"\n\n# 2. Stream status updates in real-time\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n</code></pre>"},{"location":"guides/job-scheduler/#react-component","title":"React Component","text":"<pre><code>import { useEffect, useState } from 'react';\n\nfunction JobStatus({ jobId }) {\n  const [job, setJob] = useState(null);\n\n  useEffect(() =&gt; {\n    const eventSource = new EventSource(`/api/v1/jobs/${jobId}/$stream`);\n\n    eventSource.onmessage = (event) =&gt; {\n      const jobData = JSON.parse(event.data);\n      setJob(jobData);\n\n      // Close when finished\n      if (['completed', 'failed', 'canceled'].includes(jobData.status)) {\n        eventSource.close();\n      }\n    };\n\n    eventSource.onerror = () =&gt; {\n      eventSource.close();\n    };\n\n    return () =&gt; eventSource.close();\n  }, [jobId]);\n\n  return (\n    &lt;div&gt;\n      &lt;h3&gt;Job {jobId}&lt;/h3&gt;\n      &lt;p&gt;Status: {job?.status || 'connecting...'}&lt;/p&gt;\n      {job?.error &amp;&amp; &lt;p className=\"error\"&gt;{job.error}&lt;/p&gt;}\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"guides/job-scheduler/#vue-component","title":"Vue Component","text":"<pre><code>&lt;template&gt;\n  &lt;div&gt;\n    &lt;h3&gt;Job {{ jobId }}&lt;/h3&gt;\n    &lt;p&gt;Status: {{ job?.status || 'connecting...' }}&lt;/p&gt;\n    &lt;p v-if=\"job?.error\" class=\"error\"&gt;{{ job.error }}&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;script setup&gt;\nimport { ref, onMounted, onUnmounted } from 'vue';\n\nconst props = defineProps(['jobId']);\nconst job = ref(null);\nlet eventSource;\n\nonMounted(() =&gt; {\n  eventSource = new EventSource(`/api/v1/jobs/${props.jobId}/$stream`);\n\n  eventSource.onmessage = (event) =&gt; {\n    const jobData = JSON.parse(event.data);\n    job.value = jobData;\n\n    if (['completed', 'failed', 'canceled'].includes(jobData.status)) {\n      eventSource.close();\n    }\n  };\n});\n\nonUnmounted(() =&gt; {\n  eventSource?.close();\n});\n&lt;/script&gt;\n</code></pre>"},{"location":"guides/job-scheduler/#next-steps","title":"Next Steps","text":"<ul> <li>ML Workflows: Combine with <code>.with_ml()</code> for training jobs</li> <li>Task Execution: Use with <code>.with_tasks()</code> for script execution</li> <li>Artifact Storage: Jobs can return ULIDs to link results</li> </ul> <p>For more examples: - <code>examples/job_scheduler_api.py</code> - Basic job scheduler - <code>examples/job_scheduler_sse_api.py</code> - SSE streaming (30s job) - <code>examples/task_execution_api.py</code> - Task execution with jobs</p>"},{"location":"guides/monitoring/","title":"Monitoring with OpenTelemetry and Prometheus","text":"<p>Servicekit provides built-in monitoring through OpenTelemetry instrumentation with automatic Prometheus metrics export.</p>"},{"location":"guides/monitoring/#quick-start","title":"Quick Start","text":"<p>Enable monitoring in your service with a single method call:</p> <pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_monitoring()  # Enables OpenTelemetry + Prometheus endpoint\n    .with_database()\n    .with_health()\n    .build()\n)\n</code></pre> <p>Your service now exposes Prometheus metrics at <code>/metrics</code>.</p>"},{"location":"guides/monitoring/#features","title":"Features","text":""},{"location":"guides/monitoring/#automatic-instrumentation","title":"Automatic Instrumentation","text":"<ul> <li>FastAPI: HTTP request metrics (duration, status codes, paths)</li> <li>SQLAlchemy: Database query metrics (connection pool, query duration)</li> <li>Python Runtime: Garbage collection, memory usage, CPU time</li> </ul>"},{"location":"guides/monitoring/#metrics-endpoint","title":"Metrics Endpoint","text":"<ul> <li>Path: <code>/metrics</code> (operational endpoint, root level)</li> <li>Format: Prometheus text format</li> <li>Content-Type: <code>text/plain; version=0.0.4; charset=utf-8</code></li> </ul>"},{"location":"guides/monitoring/#zero-configuration","title":"Zero Configuration","text":"<p>No manual instrumentation needed - Servicekit automatically:</p> <ul> <li>Instruments all FastAPI routes</li> <li>Tracks SQLAlchemy database operations</li> <li>Exposes Python runtime metrics</li> <li>Handles OpenTelemetry lifecycle</li> </ul>"},{"location":"guides/monitoring/#configuration","title":"Configuration","text":""},{"location":"guides/monitoring/#basic-configuration","title":"Basic Configuration","text":"<pre><code>.with_monitoring()  # Uses defaults\n</code></pre> <p>Defaults: - Metrics endpoint: <code>/metrics</code> - Service name: From <code>ServiceInfo.display_name</code> - Tags: <code>[\"monitoring\"]</code></p>"},{"location":"guides/monitoring/#custom-configuration","title":"Custom Configuration","text":"<pre><code>.with_monitoring(\n    prefix=\"/custom/metrics\",           # Custom endpoint path\n    tags=[\"Observability\", \"Telemetry\"], # Custom OpenAPI tags\n    service_name=\"production-api\",       # Override service name\n)\n</code></pre>"},{"location":"guides/monitoring/#parameters","title":"Parameters","text":"<ul> <li>prefix (<code>str</code>): Metrics endpoint path. Default: <code>/metrics</code></li> <li>tags (<code>List[str]</code>): OpenAPI tags for metrics endpoint. Default: <code>[\"monitoring\"]</code></li> <li>service_name (<code>str | None</code>): Service name in metrics labels. Default: from <code>ServiceInfo</code></li> </ul>"},{"location":"guides/monitoring/#metrics-endpoint_1","title":"Metrics Endpoint","text":""},{"location":"guides/monitoring/#testing-the-endpoint","title":"Testing the Endpoint","text":"<pre><code># Get metrics\ncurl http://localhost:8000/metrics\n\n# Filter specific metrics\ncurl http://localhost:8000/metrics | grep http_request\n\n# Monitor continuously\nwatch -n 1 'curl -s http://localhost:8000/metrics | grep http_request_duration'\n</code></pre>"},{"location":"guides/monitoring/#expected-output","title":"Expected Output","text":"<pre><code># HELP python_gc_objects_collected_total Objects collected during gc\n# TYPE python_gc_objects_collected_total counter\npython_gc_objects_collected_total{generation=\"0\"} 234.0\n\n# HELP http_server_request_duration_seconds HTTP request duration\n# TYPE http_server_request_duration_seconds histogram\nhttp_server_request_duration_seconds_bucket{http_method=\"GET\",http_status_code=\"200\",le=\"0.005\"} 45.0\n\n# HELP db_client_connections_usage Number of connections that are currently in use\n# TYPE db_client_connections_usage gauge\ndb_client_connections_usage{pool_name=\"default\",state=\"used\"} 2.0\n</code></pre>"},{"location":"guides/monitoring/#kubernetes-integration","title":"Kubernetes Integration","text":""},{"location":"guides/monitoring/#deployment-with-service-monitor","title":"Deployment with Service Monitor","text":"<p>deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: servicekit-service\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: servicekit-service\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"8000\"\n        prometheus.io/path: \"/metrics\"\n    spec:\n      containers:\n      - name: app\n        image: your-servicekit-app\n        ports:\n        - containerPort: 8000\n          name: http\n</code></pre></p> <p>service.yaml: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: servicekit-service\n  labels:\n    app: servicekit-service\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    name: http\n  selector:\n    app: servicekit-service\n</code></pre></p> <p>servicemonitor.yaml (Prometheus Operator): <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: servicekit-service\n  labels:\n    app: servicekit-service\nspec:\n  selector:\n    matchLabels:\n      app: servicekit-service\n  endpoints:\n  - port: http\n    path: /metrics\n    interval: 30s\n</code></pre></p>"},{"location":"guides/monitoring/#prometheus-configuration","title":"Prometheus Configuration","text":""},{"location":"guides/monitoring/#scrape-configuration","title":"Scrape Configuration","text":"<p>Add to <code>prometheus.yml</code>:</p> <pre><code>scrape_configs:\n  - job_name: 'servicekit-services'\n    scrape_interval: 15s\n    static_configs:\n      - targets: ['localhost:8000']\n        labels:\n          service: 'servicekit-api'\n          environment: 'production'\n</code></pre>"},{"location":"guides/monitoring/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  servicekit-app:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - LOG_LEVEL=INFO\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus-data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana-data:/var/lib/grafana\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n      - GF_AUTH_ANONYMOUS_ENABLED=true\n\nvolumes:\n  prometheus-data:\n  grafana-data:\n</code></pre></p>"},{"location":"guides/monitoring/#grafana-dashboards","title":"Grafana Dashboards","text":""},{"location":"guides/monitoring/#adding-prometheus-data-source","title":"Adding Prometheus Data Source","text":"<ol> <li>Navigate to Configuration \u2192 Data Sources</li> <li>Click Add data source</li> <li>Select Prometheus</li> <li>Set URL: <code>http://prometheus:9090</code> (Docker) or <code>http://localhost:9090</code> (local)</li> <li>Click Save &amp; Test</li> </ol>"},{"location":"guides/monitoring/#example-queries","title":"Example Queries","text":"<p>HTTP Request Rate: <pre><code>rate(http_server_requests_total{job=\"servicekit-services\"}[5m])\n</code></pre></p> <p>Request Duration (p95): <pre><code>histogram_quantile(0.95,\n  rate(http_server_request_duration_seconds_bucket[5m])\n)\n</code></pre></p> <p>Database Connection Pool Usage: <pre><code>db_client_connections_usage{state=\"used\"} /\ndb_client_connections_limit\n</code></pre></p> <p>Error Rate: <pre><code>rate(http_server_requests_total{http_status_code=~\"5..\"}[5m])\n</code></pre></p> <p>ML Training Job Rate: <pre><code>rate(ml_train_jobs_total{job=\"servicekit-services\"}[5m])\n</code></pre></p> <p>ML Prediction Job Rate: <pre><code>rate(ml_predict_jobs_total{job=\"servicekit-services\"}[5m])\n</code></pre></p> <p>Total ML Jobs (Train + Predict): <pre><code>sum(rate(ml_train_jobs_total{job=\"servicekit-services\"}[5m])) +\nsum(rate(ml_predict_jobs_total{job=\"servicekit-services\"}[5m]))\n</code></pre></p>"},{"location":"guides/monitoring/#available-metrics","title":"Available Metrics","text":""},{"location":"guides/monitoring/#http-metrics-fastapi","title":"HTTP Metrics (FastAPI)","text":"<ul> <li><code>http_server_request_duration_seconds</code> - Request duration histogram</li> <li><code>http_server_requests_total</code> - Total requests counter</li> <li><code>http_server_active_requests</code> - Active requests gauge</li> </ul> <p>Labels: <code>http_method</code>, <code>http_status_code</code>, <code>http_route</code></p>"},{"location":"guides/monitoring/#database-metrics-sqlalchemy","title":"Database Metrics (SQLAlchemy)","text":"<ul> <li><code>db_client_connections_usage</code> - Connection pool usage</li> <li><code>db_client_connections_limit</code> - Connection pool limit</li> <li><code>db_client_operation_duration_seconds</code> - Query duration</li> </ul> <p>Labels: <code>pool_name</code>, <code>state</code>, <code>operation</code></p>"},{"location":"guides/monitoring/#python-runtime-metrics","title":"Python Runtime Metrics","text":"<ul> <li><code>python_gc_objects_collected_total</code> - GC collections</li> <li><code>python_gc_collections_total</code> - GC runs</li> <li><code>python_info</code> - Python version info</li> <li><code>process_cpu_seconds_total</code> - CPU time</li> <li><code>process_resident_memory_bytes</code> - Memory usage</li> </ul>"},{"location":"guides/monitoring/#ml-metrics-when-using-with_ml","title":"ML Metrics (when using <code>.with_ml()</code>)","text":"<ul> <li><code>ml_train_jobs_total</code> - Total number of ML training jobs submitted</li> <li><code>ml_predict_jobs_total</code> - Total number of ML prediction jobs submitted</li> </ul> <p>Labels: <code>service_name</code></p>"},{"location":"guides/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"guides/monitoring/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Enable monitoring in production for observability</li> <li>Set meaningful service names to identify services in multi-service setups</li> <li>Monitor key metrics: request rate, error rate, duration (RED method)</li> <li>Set up alerts for error rates, high latency, and resource exhaustion</li> <li>Use service labels to tag metrics with environment, version, region</li> <li>Keep <code>/metrics</code> unauthenticated for Prometheus access (use network policies)</li> </ul>"},{"location":"guides/monitoring/#avoid","title":"Avoid","text":"<ul> <li>Exposing metrics publicly (use internal network or auth proxy)</li> <li>Scraping too frequently (15-30s interval is usually sufficient)</li> <li>Ignoring high cardinality (avoid unbounded label values)</li> <li>Skipping resource limits (monitor and limit Prometheus storage growth)</li> </ul>"},{"location":"guides/monitoring/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"guides/monitoring/#with-authentication","title":"With Authentication","text":"<pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_monitoring()\n    .with_auth(\n        unauthenticated_paths=[\n            \"/health\",      # Health check\n            \"/metrics\",     # Prometheus scraping\n            \"/docs\"         # API docs\n        ]\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/monitoring/#with-health-checks","title":"With Health Checks","text":"<pre><code>app = (\n    BaseServiceBuilder(info=info)\n    .with_health()         # /health - Health check endpoint\n    .with_system()         # /api/v1/system - System metadata\n    .with_monitoring()     # /metrics - Prometheus metrics\n    .build()\n)\n</code></pre> <p>Operational monitoring endpoints (<code>/health</code>, <code>/health/$stream</code>, <code>/metrics</code>) use root-level paths for easy discovery by Kubernetes, monitoring dashboards, and Prometheus. Service metadata endpoints (<code>/api/v1/system</code>, <code>/api/v1/info</code>) use versioned API paths.</p> <p>For detailed health check configuration and usage, see the Health Checks Guide.</p>"},{"location":"guides/monitoring/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/monitoring/#metrics-endpoint-returns-404","title":"Metrics Endpoint Returns 404","text":"<p>Problem: <code>/metrics</code> endpoint not found.</p> <p>Solution: Ensure you called <code>.with_monitoring()</code> in your BaseServiceBuilder chain.</p>"},{"location":"guides/monitoring/#no-metrics-appear","title":"No Metrics Appear","text":"<p>Problem: Endpoint returns empty or minimal metrics.</p> <p>Solution: 1. Make some requests to your API endpoints 2. Verify FastAPI instrumentation with: <code>curl http://localhost:8000/api/v1/configs</code> 3. Check metrics again: <code>curl http://localhost:8000/metrics | grep http_request</code></p>"},{"location":"guides/monitoring/#prometheus-cannot-scrape","title":"Prometheus Cannot Scrape","text":"<p>Problem: Prometheus shows targets as \"DOWN\".</p> <p>Solution: 1. Verify service is running: <code>curl http://localhost:8000/health</code> 2. Check network connectivity 3. Verify scrape config matches service port and path 4. Check for firewall/network policies blocking access</p>"},{"location":"guides/monitoring/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Prometheus uses too much memory.</p> <p>Solution: 1. Reduce retention time: <code>--storage.tsdb.retention.time=15d</code> 2. Increase scrape interval: <code>scrape_interval: 30s</code> 3. Limit metric cardinality (check for unbounded labels)</p>"},{"location":"guides/monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>Health Checks: Add health monitoring with <code>.with_health()</code> - see Health Checks Guide</li> <li>Alerting: Set up Prometheus Alertmanager for notifications</li> <li>Distributed Tracing: Future support for OpenTelemetry traces (see ROADMAP.md)</li> <li>Custom Metrics: Use <code>get_meter()</code> for application-specific metrics</li> <li>SLOs: Define Service Level Objectives based on metrics</li> </ul>"},{"location":"guides/monitoring/#examples","title":"Examples","text":"<ul> <li><code>examples/monitoring_api.py</code> - Complete monitoring example</li> <li><code>examples/docs/monitoring_api.postman_collection.json</code> - Postman collection</li> </ul> <p>For more details, see: - Health Checks Guide - Health check configuration - OpenTelemetry Documentation - Prometheus Documentation - Grafana Documentation</p>"},{"location":"guides/registration/","title":"Service Registration","text":"<p>Servicekit provides automatic service registration with an orchestrator for service discovery in Docker Compose and Kubernetes environments.</p>"},{"location":"guides/registration/#quick-start","title":"Quick Start","text":""},{"location":"guides/registration/#basic-registration","title":"Basic Registration","text":"<p>The simplest approach with auto-detected hostname:</p> <pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_registration()  # Reads SERVICEKIT_ORCHESTRATOR_URL from environment\n    .build()\n)\n</code></pre> <p>Set the environment variable:</p> <pre><code>export SERVICEKIT_ORCHESTRATOR_URL=http://orchestrator:9000/services/$register\nfastapi run your_file.py\n</code></pre>"},{"location":"guides/registration/#docker-compose","title":"Docker Compose","text":"<p>The recommended approach for multi-service deployments:</p> <pre><code>services:\n  orchestrator:\n    image: your-orchestrator:latest\n    ports:\n      - \"9000:9000\"\n\n  my-service:\n    image: your-service:latest\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n      # Hostname auto-detected from container name\n    depends_on:\n      - orchestrator\n</code></pre>"},{"location":"guides/registration/#custom-serviceinfo","title":"Custom ServiceInfo","text":"<p>For services with additional metadata:</p> <pre><code>from servicekit.api import ServiceInfo\n\nclass CustomServiceInfo(ServiceInfo):\n    \"\"\"Extended service info with custom fields.\"\"\"\n    deployment_env: str = \"production\"\n    team: str = \"platform\"\n    capabilities: list[str] = []\n\napp = (\n    BaseServiceBuilder(\n        info=CustomServiceInfo(\n            display_name=\"My Service\",\n            version=\"1.0.0\",\n            deployment_env=\"staging\",\n            team=\"data-science\",\n            capabilities=[\"ml-inference\", \"analytics\"],\n        )\n    )\n    .with_registration()\n    .build()\n)\n</code></pre>"},{"location":"guides/registration/#configuration-options","title":"Configuration Options","text":"<p>The <code>.with_registration()</code> method accepts these parameters:</p> <pre><code>.with_registration(\n    orchestrator_url=None,              # Direct value or env var\n    host=None,                          # Direct value, auto-detect, or env var\n    port=None,                          # Direct value, env var, or 8000\n    orchestrator_url_env=\"SERVICEKIT_ORCHESTRATOR_URL\",\n    host_env=\"SERVICEKIT_HOST\",\n    port_env=\"SERVICEKIT_PORT\",\n    max_retries=5,                      # Number of registration attempts\n    retry_delay=2.0,                    # Seconds between retries\n    fail_on_error=False,                # Abort startup on failure\n    timeout=10.0,                       # HTTP request timeout\n    enable_keepalive=True,              # Enable periodic ping to keep service alive\n    keepalive_interval=10.0,            # Seconds between keepalive pings\n    auto_deregister=True,               # Automatically deregister on shutdown\n)\n</code></pre>"},{"location":"guides/registration/#parameters","title":"Parameters","text":"<ul> <li>orchestrator_url (<code>str | None</code>): Orchestrator registration endpoint URL. If None, reads from environment variable.</li> <li>host (<code>str | None</code>): Service hostname. If None, auto-detects via <code>socket.gethostname()</code> or reads from environment variable.</li> <li>port (<code>int | None</code>): Service port. If None, reads from environment variable or defaults to 8000.</li> <li>orchestrator_url_env (<code>str</code>): Environment variable name for orchestrator URL. Default: <code>SERVICEKIT_ORCHESTRATOR_URL</code>.</li> <li>host_env (<code>str</code>): Environment variable name for hostname override. Default: <code>SERVICEKIT_HOST</code>.</li> <li>port_env (<code>str</code>): Environment variable name for port override. Default: <code>SERVICEKIT_PORT</code>.</li> <li>max_retries (<code>int</code>): Maximum number of registration attempts. Default: 5.</li> <li>retry_delay (<code>float</code>): Delay in seconds between retry attempts. Default: 2.0.</li> <li>fail_on_error (<code>bool</code>): If True, raise exception and abort startup on registration failure. If False, log warning and continue. Default: False.</li> <li>timeout (<code>float</code>): HTTP request timeout in seconds. Default: 10.0.</li> <li>enable_keepalive (<code>bool</code>): Enable periodic pings to keep service registered. Default: True.</li> <li>keepalive_interval (<code>float</code>): Seconds between keepalive pings. Default: 10.0.</li> <li>auto_deregister (<code>bool</code>): Automatically deregister service on shutdown. Default: True.</li> </ul>"},{"location":"guides/registration/#how-it-works","title":"How It Works","text":""},{"location":"guides/registration/#registration-flow","title":"Registration Flow","text":"<ol> <li>Service Starts: FastAPI application initializes during lifespan startup</li> <li>Hostname Resolution: Determines service hostname (see resolution order below)</li> <li>Port Resolution: Determines service port (see resolution order below)</li> <li>URL Construction: Builds service URL as <code>http://&lt;hostname&gt;:&lt;port&gt;</code></li> <li>Payload Creation: Serializes ServiceInfo to JSON (supports custom subclasses)</li> <li>Registration Request: Sends POST to orchestrator endpoint</li> <li>Retry on Failure: Retries with delay if request fails</li> <li>Keepalive Started: If enabled, background task starts pinging orchestrator</li> <li>Service Runs: Service handles requests while staying alive via pings</li> <li>Shutdown: On graceful shutdown, stops keepalive and optionally deregisters</li> <li>Logging: Logs all registration, ping, and deregistration events</li> </ol>"},{"location":"guides/registration/#keepalive-and-ttl","title":"Keepalive and TTL","text":"<p>Services can be configured to send periodic \"ping\" requests to the orchestrator to indicate they're still alive. The orchestrator tracks a Time-To-Live (TTL) for each service and automatically removes services that haven't pinged within the TTL window.</p> <p>How it works:</p> <ol> <li>Initial Registration: Service registers and receives response with:</li> <li><code>id</code>: Unique ULID identifier for this service</li> <li><code>ttl_seconds</code>: How long until service expires (default: 30 seconds)</li> <li> <p><code>ping_url</code>: Endpoint to send keepalive pings (automatically provided by orchestrator)</p> </li> <li> <p>Keepalive Loop: Background task automatically sends PUT requests to <code>ping_url</code> every N seconds:</p> </li> <li>Default interval: 10 seconds (configurable via <code>keepalive_interval</code>)</li> <li>Each ping resets the service's expiration time</li> <li> <p>Failures are logged but don't crash the service</p> </li> <li> <p>TTL Expiration: Orchestrator runs cleanup every 5 seconds:</p> </li> <li>Removes services that haven't pinged within TTL window</li> <li> <p>Logs expired services for monitoring</p> </li> <li> <p>Graceful Shutdown: On service shutdown:</p> </li> <li>Keepalive task stops (no more pings)</li> <li>Service explicitly deregisters (if <code>auto_deregister=True</code>)</li> <li>Immediate removal from registry</li> </ol> <p>Configuration examples:</p> <pre><code># Default: keepalive enabled, auto-deregister on shutdown\n.with_registration()\n\n# Disable keepalive (rely on manual health checks)\n.with_registration(enable_keepalive=False)\n\n# Custom keepalive interval (faster pings)\n.with_registration(keepalive_interval=5.0)\n\n# Don't deregister on shutdown (let TTL expire naturally)\n.with_registration(auto_deregister=False)\n</code></pre>"},{"location":"guides/registration/#registration-payload","title":"Registration Payload","text":"<p>The service sends this payload to the orchestrator:</p> <pre><code>{\n  \"url\": \"http://my-service:8000\",\n  \"info\": {\n    \"display_name\": \"My Service\",\n    \"version\": \"1.0.0\",\n    \"summary\": \"Service description\",\n    ...\n  }\n}\n</code></pre> <p>For custom ServiceInfo subclasses:</p> <pre><code>{\n  \"url\": \"http://ml-service:8000\",\n  \"info\": {\n    \"display_name\": \"ML Service\",\n    \"version\": \"2.0.0\",\n    \"deployment_env\": \"production\",\n    \"team\": \"data-science\",\n    \"capabilities\": [\"ml-inference\", \"feature-extraction\"],\n    \"priority\": 5\n  }\n}\n</code></pre>"},{"location":"guides/registration/#registration-response","title":"Registration Response","text":"<p>The orchestrator responds with registration details, including the ping endpoint:</p> <pre><code>{\n  \"id\": \"01K83B5V85PQZ1HTH4DQ7NC9JM\",\n  \"status\": \"registered\",\n  \"service_url\": \"http://my-service:8000\",\n  \"message\": \"Service registered successfully\",\n  \"ttl_seconds\": 30,\n  \"ping_url\": \"http://orchestrator:9000/services/01K83B5V85PQZ1HTH4DQ7NC9JM/$ping\"\n}\n</code></pre> <p>Key fields: - <code>id</code>: Unique ULID identifier assigned by orchestrator - <code>ttl_seconds</code>: Time-to-live in seconds (service must ping within this window) - <code>ping_url</code>: Endpoint for keepalive pings (automatically used by the service)</p> <p>Important: The <code>ping_url</code> is provided by the orchestrator - services don't need to configure it. The service automatically uses this URL for keepalive pings when <code>enable_keepalive=True</code>.</p>"},{"location":"guides/registration/#hostname-resolution","title":"Hostname Resolution","text":"<p>Priority order:</p> <ol> <li>Direct Parameter: <code>host=\"my-service\"</code> in <code>.with_registration()</code></li> <li>Auto-Detection: <code>socket.gethostname()</code> (returns Docker container name or hostname)</li> <li>Environment Variable: Value of <code>SERVICEKIT_HOST</code> (or custom env var)</li> <li>Error: Raises exception if <code>fail_on_error=True</code>, otherwise logs warning</li> </ol> <p>Docker Behavior: In Docker Compose, <code>socket.gethostname()</code> returns the service name or container ID, making auto-detection work seamlessly.</p>"},{"location":"guides/registration/#port-resolution","title":"Port Resolution","text":"<p>Priority order:</p> <ol> <li>Direct Parameter: <code>port=8080</code> in <code>.with_registration()</code></li> <li>Environment Variable: Value of <code>SERVICEKIT_PORT</code> (or custom env var)</li> <li>Default: 8000</li> </ol> <p>Important: <code>SERVICEKIT_PORT</code> should match the container's internal port, not the host-mapped port.</p>"},{"location":"guides/registration/#examples","title":"Examples","text":""},{"location":"guides/registration/#environment-variables-production","title":"Environment Variables (Production)","text":"<pre><code>from servicekit.api import BaseServiceBuilder, ServiceInfo\n\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Production Service\"))\n    .with_logging()\n    .with_health()\n    .with_registration()  # Reads from environment\n    .build()\n)\n</code></pre> <p>docker-compose.yml: <pre><code>services:\n  my-service:\n    image: my-service:latest\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n      # SERVICEKIT_HOST auto-detected\n      # SERVICEKIT_PORT defaults to 8000\n</code></pre></p>"},{"location":"guides/registration/#direct-configuration-testing","title":"Direct Configuration (Testing)","text":"<pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Test Service\"))\n    .with_registration(\n        orchestrator_url=\"http://localhost:9000/services/$register\",\n        host=\"test-service\",\n        port=8080,\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/registration/#custom-environment-variable-names","title":"Custom Environment Variable Names","text":"<pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_registration(\n        orchestrator_url_env=\"MY_APP_ORCHESTRATOR_URL\",\n        host_env=\"MY_APP_HOST\",\n        port_env=\"MY_APP_PORT\",\n    )\n    .build()\n)\n</code></pre> <p>Environment: <pre><code>export MY_APP_ORCHESTRATOR_URL=http://orchestrator:9000/services/$register\nexport MY_APP_HOST=my-service\nexport MY_APP_PORT=8000\n</code></pre></p>"},{"location":"guides/registration/#fail-fast-mode","title":"Fail-Fast Mode","text":"<p>For critical services that must register:</p> <pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Critical Service\"))\n    .with_registration(\n        fail_on_error=True,  # Abort startup if registration fails\n        max_retries=10,\n        retry_delay=1.0,\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/registration/#custom-retry-strategy","title":"Custom Retry Strategy","text":"<pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_registration(\n        max_retries=10,      # More attempts\n        retry_delay=1.0,     # Faster retries\n        timeout=5.0,         # Shorter timeout\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/registration/#docker-compose_1","title":"Docker Compose","text":""},{"location":"guides/registration/#basic-setup","title":"Basic Setup","text":"<pre><code>services:\n  orchestrator:\n    image: orchestrator:latest\n    ports:\n      - \"9000:9000\"\n\n  service-a:\n    image: my-service:latest\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n    depends_on:\n      - orchestrator\n</code></pre>"},{"location":"guides/registration/#with-health-checks","title":"With Health Checks","text":"<p>Wait for orchestrator to be healthy before starting services:</p> <pre><code>services:\n  orchestrator:\n    image: orchestrator:latest\n    ports:\n      - \"9000:9000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/health\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  service-a:\n    image: my-service:latest\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n    depends_on:\n      orchestrator:\n        condition: service_healthy  # Wait for healthy status\n</code></pre>"},{"location":"guides/registration/#custom-port-mapping","title":"Custom Port Mapping","text":"<p>When container port differs from host port:</p> <pre><code>services:\n  service-a:\n    image: my-service:latest\n    ports:\n      - \"8001:8000\"  # Host:Container\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n      SERVICEKIT_PORT: \"8000\"  # Use container port, not host port\n</code></pre> <p>Why: Other services connect using the internal Docker network, so they use the container port (8000), not the host-mapped port (8001).</p>"},{"location":"guides/registration/#multiple-services","title":"Multiple Services","text":"<pre><code>services:\n  orchestrator:\n    image: orchestrator:latest\n    ports:\n      - \"9000:9000\"\n\n  service-a:\n    image: my-service:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n\n  service-b:\n    image: my-service:latest\n    ports:\n      - \"8001:8000\"\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n</code></pre> <p>Both services register with different hostnames (service-a, service-b) but same internal port (8000).</p>"},{"location":"guides/registration/#kubernetes","title":"Kubernetes","text":""},{"location":"guides/registration/#configmap-for-orchestrator-url","title":"ConfigMap for Orchestrator URL","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: registration-config\ndata:\n  orchestrator-url: \"http://orchestrator-service:9000/services/$register\"\n</code></pre>"},{"location":"guides/registration/#deployment-with-registration","title":"Deployment with Registration","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-service\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: my-service\n    spec:\n      containers:\n        - name: my-service\n          image: my-service:latest\n          env:\n            - name: SERVICEKIT_ORCHESTRATOR_URL\n              valueFrom:\n                configMapKeyRef:\n                  name: registration-config\n                  key: orchestrator-url\n            - name: SERVICEKIT_HOST\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name  # Pod name\n</code></pre>"},{"location":"guides/registration/#service-discovery","title":"Service Discovery","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-service\n  ports:\n    - port: 8000\n      targetPort: 8000\n</code></pre>"},{"location":"guides/registration/#error-handling","title":"Error Handling","text":""},{"location":"guides/registration/#retry-behavior","title":"Retry Behavior","text":"<p>By default, registration retries 5 times with 2-second delays:</p> <pre><code>.with_registration(\n    max_retries=5,      # Total attempts\n    retry_delay=2.0,    # Seconds between attempts\n)\n</code></pre> <p>Example timeline: - Attempt 1: Immediate - Attempt 2: After 2 seconds - Attempt 3: After 4 seconds (2s + 2s) - Attempt 4: After 6 seconds - Attempt 5: After 8 seconds</p>"},{"location":"guides/registration/#fail-on-error","title":"Fail on Error","text":"<p>Default (fail_on_error=False): Service starts even if registration fails</p> <pre><code>.with_registration(fail_on_error=False)  # Log warning, continue\n</code></pre> <p>Fail-Fast (fail_on_error=True): Service aborts startup if registration fails</p> <pre><code>.with_registration(fail_on_error=True)  # Raise exception, abort\n</code></pre> <p>When to use fail-fast: - Critical services that require orchestrator awareness - Production environments with strict registration requirements - Services that cannot function without being registered</p>"},{"location":"guides/registration/#structured-logging","title":"Structured Logging","text":"<p>All registration events are logged with structured data:</p> <pre><code>{\n  \"event\": \"registration.starting\",\n  \"orchestrator_url\": \"http://orchestrator:9000/services/$register\",\n  \"service_url\": \"http://my-service:8000\",\n  \"host_source\": \"auto-detected\",\n  \"port_source\": \"default\",\n  \"max_retries\": 5\n}\n</code></pre> <p>Success: <pre><code>{\n  \"event\": \"registration.success\",\n  \"service_url\": \"http://my-service:8000\",\n  \"attempt\": 1,\n  \"status_code\": 200\n}\n</code></pre></p> <p>Failure: <pre><code>{\n  \"event\": \"registration.attempt_failed\",\n  \"service_url\": \"http://my-service:8000\",\n  \"attempt\": 1,\n  \"max_retries\": 5,\n  \"error\": \"Connection refused\",\n  \"error_type\": \"ConnectError\"\n}\n</code></pre></p>"},{"location":"guides/registration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/registration/#registration-fails","title":"Registration Fails","text":"<p>Check orchestrator is reachable:</p> <pre><code># From service container\ndocker compose exec my-service curl http://orchestrator:9000/health\n</code></pre> <p>Check environment variables:</p> <pre><code>docker compose exec my-service env | grep SERVICEKIT\n</code></pre> <p>View registration logs:</p> <pre><code>docker compose logs my-service | grep registration\n</code></pre>"},{"location":"guides/registration/#hostname-auto-detection-issues","title":"Hostname Auto-Detection Issues","text":"<p>Problem: Auto-detection fails or returns unexpected value</p> <p>Solution: Override with environment variable</p> <pre><code>environment:\n  SERVICEKIT_HOST: my-custom-hostname\n</code></pre> <p>Debug hostname detection:</p> <pre><code>docker compose exec my-service hostname\ndocker compose exec my-service python -c \"import socket; print(socket.gethostname())\"\n</code></pre>"},{"location":"guides/registration/#port-mismatch","title":"Port Mismatch","text":"<p>Problem: Orchestrator cannot reach service at registered URL</p> <p>Common mistake: Using host-mapped port instead of container port</p> <pre><code># WRONG\nports:\n  - \"8001:8000\"\nenvironment:\n  SERVICEKIT_PORT: \"8001\"  # \u274c Host port\n\n# CORRECT\nports:\n  - \"8001:8000\"\nenvironment:\n  SERVICEKIT_PORT: \"8000\"  # \u2705 Container port\n</code></pre> <p>Why: Services communicate via Docker's internal network using container ports, not host-mapped ports.</p>"},{"location":"guides/registration/#orchestrator-url-missing","title":"Orchestrator URL Missing","text":"<p>Problem: No orchestrator URL configured</p> <p>Error: <code>registration.missing_orchestrator_url</code></p> <p>Solution: Set environment variable</p> <pre><code>export SERVICEKIT_ORCHESTRATOR_URL=http://orchestrator:9000/services/$register\n</code></pre> <p>Or use direct configuration:</p> <pre><code>.with_registration(orchestrator_url=\"http://orchestrator:9000/services/$register\")\n</code></pre>"},{"location":"guides/registration/#service-not-appearing-in-registry","title":"Service Not Appearing in Registry","text":"<p>Check orchestrator logs:</p> <pre><code>docker compose logs orchestrator\n</code></pre> <p>Check service startup logs:</p> <pre><code>docker compose logs my-service | grep registration\n</code></pre> <p>Verify orchestrator endpoint:</p> <pre><code>curl http://localhost:9000/services\n</code></pre>"},{"location":"guides/registration/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/registration/#high-availability","title":"High Availability","text":"<p>Use multiple orchestrator replicas:</p> <pre><code>services:\n  orchestrator:\n    image: orchestrator:latest\n    deploy:\n      replicas: 3\n\n  service-a:\n    environment:\n      SERVICEKIT_ORCHESTRATOR_URL: http://orchestrator:9000/services/$register\n</code></pre>"},{"location":"guides/registration/#retry-strategy","title":"Retry Strategy","text":"<p>Adjust retries for production reliability:</p> <pre><code>.with_registration(\n    max_retries=10,       # More attempts\n    retry_delay=1.0,      # Faster retries\n    timeout=30.0,         # Longer timeout\n    fail_on_error=True,   # Fail fast in production\n)\n</code></pre>"},{"location":"guides/registration/#security","title":"Security","text":"<p>Authentication: Add API keys or tokens to registration requests (requires custom implementation)</p> <p>TLS: Use HTTPS for orchestrator communication</p> <pre><code>.with_registration(\n    orchestrator_url=\"https://orchestrator:9443/services/$register\"\n)\n</code></pre>"},{"location":"guides/registration/#monitoring","title":"Monitoring","text":"<p>Monitor registration health: - Track registration success/failure rates - Alert on repeated registration failures - Monitor orchestrator availability - Log all registration attempts for audit</p>"},{"location":"guides/registration/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/registration/</code> - Complete registration demo with orchestrator</li> <li><code>core_api/</code> - Basic CRUD service</li> <li><code>monitoring/</code> - Prometheus metrics</li> <li><code>auth_envvar/</code> - Environment-based authentication</li> </ul>"},{"location":"guides/registration/#see-also","title":"See Also","text":"<ul> <li>Health Checks - Configure health endpoints</li> <li>Monitoring - Prometheus metrics</li> <li>Authentication - API key authentication</li> </ul>"}]}